<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>An efficient probabilistic hardware architecture for diffusion-like models</title>
<!--Generated on Tue Oct 28 01:04:10 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on October 28, 2025.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2510.23972v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span>The Challenge with EBMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span>Denoising thermodynamic models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span>Denoising Thermodynamic Computers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S4" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span>Training DTMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span>Conclusion: Scaling Thermodynamic Machine Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Denoising Diffusion Models</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS1" title="In Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Forward Processes</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS1.SSS1" title="In A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.1 </span>Continuous Variables</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS1.SSS2" title="In A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.2 </span>Discrete Variables</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS2" title="In Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Reverse Processes</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS2.SSS1" title="In A.2 Reverse Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.1 </span>Continuous variables</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS2.SSS2" title="In A.2 Reverse Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.2 </span>Discrete variables</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS3" title="In Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>The Diffusion Loss</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS3.SSS1" title="In A.3 The Diffusion Loss ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.1 </span>A Monte-Carlo gradient estimator</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS4" title="In Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Simplification of the Energy Landscape</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS5" title="In Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Conditional Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS6" title="In Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6 </span>Learning the marginal</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A2" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Hardware accelerators for EBMs</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A2.SS1" title="In Appendix B Hardware accelerators for EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Quadratic EBMs</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A2.SS1.SSS1" title="In B.1 Quadratic EBMs ‣ Appendix B Hardware accelerators for EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1.1 </span>Potts models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A2.SS1.SSS2" title="In B.1 Quadratic EBMs ‣ Appendix B Hardware accelerators for EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1.2 </span>Gaussian-Bernoulli EBMs</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>A hardware architecture for denoising</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3.SS1" title="In Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Implementation of the forward process energy function</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3.SS2" title="In Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Implementation of the marginal energy function</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Energetic analysis of the hardware architecture</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS1" title="In Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.1 </span>Biasing circuit</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS2" title="In Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.2 </span>Local communication</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS3" title="In Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.3 </span>Global communication</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS3.SSS1" title="In D.3 Global communication ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.3.1 </span>Clocking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS3.SSS2" title="In D.3 Global communication ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.3.2 </span>Initialization and readout</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS4" title="In Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.4 </span>Analysis of a complete sampling program</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS5" title="In Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.5 </span>Level of realism</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS5.SSS1" title="In D.5 Level of realism ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.5.1 </span>Programming the weights and biases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS5.SSS2" title="In D.5 Level of realism ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.5.2 </span>Off-chip communication</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS5.SSS3" title="In D.5 Level of realism ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.5.3 </span>Supporting circuitry</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A5" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Energy analysis of GPUs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A6" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Total correlation penalty</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A6.SS1" title="In Appendix F Total correlation penalty ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F.1 </span>Gradients of the total correlation penalty</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A6.SS2" title="In Appendix F Total correlation penalty ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F.2 </span>Low dimensional embedding of the data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A6.SS3" title="In Appendix F Total correlation penalty ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F.3 </span>Control of the penalty strength</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A7" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Embedding integers into Boltzmann machines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A8" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H </span>The autocorrelation function and mixing time</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A9" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span>Deterministic embeddings for DTMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A10" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">J </span>Some details on our RNG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A11" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">K </span>MEBM experiments</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>These authors contributed equally to this work.</span></span></span>
<h1 class="ltx_title ltx_title_document">An efficient probabilistic hardware architecture for diffusion-like models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andraž Jelinčič
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Owen Lockwood
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Akhil Garlapati
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guillaume Verdon
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Trevor McCourt<math alttext="\,{}^{*,\,}" class="ltx_Math" display="inline" id="m1" intent=":literal"><semantics><msup><mi></mi><mrow><mo rspace="0em">∗</mo><mo>,</mo></mrow></msup><annotation encoding="application/x-tex">\,{}^{*,\,}</annotation></semantics></math>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:trevor@extropic.ai">trevor@extropic.ai</a>
</span>
<span class="ltx_contact ltx_role_affiliation">Extropic Corporation
</span></span></span>
</div>
<div class="ltx_dates">(October 28, 2025)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">The proliferation of probabilistic AI has promoted proposals for specialized stochastic computers. Despite promising efficiency gains, these proposals have failed to gain traction because they rely on fundamentally limited modeling techniques and exotic, unscalable hardware. In this work, we address these shortcomings by proposing an all-transistor probabilistic computer that implements powerful denoising models at the hardware level. A system-level analysis indicates that devices based on our architecture could achieve performance parity with GPUs on a simple image benchmark using approximately <span class="ltx_text ltx_number">10,000</span> times less energy.</p>
</div>
<div class="ltx_para" id="p1">
<p class="ltx_p">The unprecedented recent investment in large-scale AI systems will soon put a strain on the world’s energy infrastructure. Every year, U.S. firms spend an amount larger than the inflation-adjusted cost of the Apollo program on AI-focused data centers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib32" title="GenAI: Giga$$$, TeraWatt-Hours, and GigaTons of CO2">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib33" title="The manhattan project, the apollo program, and federal energy technology r&amp;d programs: a comparative analysis">65</a>]</cite>. By 2030, these data centers could consume 10% of all of the energy produced in the U.S. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib95" title="Powering Intelligence: Analyzing Artificial Intelligence and Data Center Energy Consumption">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p">Despite this enormous bet on scaling today’s AI systems, they may be far from optimal in terms of energy efficiency. Existing AI systems based on autoregressive large language models (LLMs) are valuable tools in white-collar fields <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib19" title="Competition-level code generation with AlphaCode">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib24" title="GPT-4 passes the bar exam">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib25" title="Capabilities of gpt-4 on medical challenge problems">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib29" title="Experimental evidence on the productivity effects of generative artificial intelligence">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib30" title="Generative AI at work">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib31" title="The impact of ai on developer productivity: Evidence from github copilot">52</a>]</cite>, and are being adopted by consumers faster than the internet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib28" title="The rapid adoption of generative ai">8</a>]</cite>. However, LLMs were architected specifically for GPUs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib35" title="Attention is all you need">68</a>]</cite>, hardware originally intended for graphics, whose suitability for machine learning was discovered accidentally decades later <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib37" title="Deep learning with cots hpc systems">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib38" title="High Performance Convolutional Neural Networks for Document Processing">15</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="647" id="S0.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold">Leveraging CMOS probabilistic hardware in ultra-efficient AI systems.</span> The central result of this article: an all‑transistor probabilistic computer running a denoising thermodynamic model (DTM) could match GPU performance on a simple modeling benchmark while using about <math alttext="\numprint{10000}\times" class="ltx_math_unparsed" display="inline" id="S0.F1.m2" intent=":literal"><semantics><mrow><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">\numprint{10000}\times</annotation></semantics></math> less energy. All models are trained on binarized Fashion-MNIST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib118" title="Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms">72</a>]</cite> and evaluated with Fréchet Inception Distance (FID) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib120" title="GANs trained by a two time-scale update rule converge to a local nash equilibrium">32</a>]</cite>. DTM variants are of increasing depth, chaining 2–8 sequential Energy-Based Models (EBMs). GPU baselines cover single‑step VAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib20" title="Auto-Encoding Variational Bayes">42</a>]</cite> and GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib18" title="Generative adversarial nets">31</a>]</cite>, plus DDPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib68" title="Deep Unsupervised Learning using Nonequilibrium Thermodynamics">62</a>]</cite> at varying numbers of steps. We also compare DTM to a monolithic EBM across multiple mixing‑time limits. The horizontal axis shows the energy needed for generating a single new image using the trained model (inference).</figcaption>
</figure>
<div class="ltx_para" id="p3">
<p class="ltx_p">Had a different style of hardware been popular in the last few decades, AI algorithms would have evolved in a completely different direction, and possibly a more energy-efficient one. This interplay between algorithm research and hardware availability is known as the "Hardware Lottery" <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib39" title="The hardware lottery">37</a>]</cite>, and it entrenches hardware-algorithm pairings that may be far from optimal.</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p">Therefore, prudent planning calls for systematic exploration of other types of AI systems in search of energy-efficient alternatives. Active efforts include mixed‑signal compute‑in‑memory accelerators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib44" title="An analog-AI chip for energy-efficient speech recognition and transcription">5</a>]</cite>, photonic neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib43" title="Single-chip photonic deep neural network with forward-only training">7</a>]</cite>, and neuromorphic processors that emulate biological spiking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib41" title="SpiNNaker2: A large-scale neuromorphic system for event-based and asynchronous machine learning">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib40" title="Efficient Video and Audio Processing with Loihi 2">59</a>]</cite>.</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p">The development of more efficient computers for AI is challenging because it requires innovation not only at the component level but also at the system level. It is insufficient to invent a new technology that performs a mathematical operation efficiently in isolation; one must also know how to combine multiple components to run a practical algorithm. In addition to these integration challenges, GPU performance per joule is doubling every few years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib45" title="Summarizing CPU and GPU design trends with product data">66</a>]</cite>, making it very difficult for cutting-edge computing schemes to gain mainstream adoption.</p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p">Probabilistic computing is an attractive approach because it can connect directly to AI at the system level via Energy-Based Models (EBMs). EBMs are a well-established model class in contemporary deep learning and have been competitive with the state of the art in tasks like image generation and robotic path planning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib53" title="Generative modeling by estimating gradients of the data distribution">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib54" title="Planning with Diffusion for Flexible Behavior Synthesis">39</a>]</cite>.</p>
</div>
<div class="ltx_para" id="p7">
<p class="ltx_p">Hardware implementations of EBMs work with special model families that adhere to physical constraints such as locality, sparsity, and connection density. Thanks to these constraints, probabilistic computers can utilize specialized stochastic circuitry to efficiently and quickly produce samples from a Boltzmann distribution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib42" title="CMOS plus stochastic nanomagnets enabling heterogeneous computers for probabilistic inference and learning">61</a>]</cite>. Depending on the precise kind of hardware being used, this sampling may occur as part of the natural dynamics of the device <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib60" title="Dynamical Computing on the Nanoscale: Superconducting Circuits for Thermodynamically-Efficient Classical Information Processing">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib61" title="Harnessing fluctuations in thermodynamic computing via time-reversal symmetries">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib62" title="Application of Quantum Annealing to Training of Deep Neural Networks">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib63" title="Intrinsic optimization using stochastic nanomagnets">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib66" title="Low-Barrier Nanomagnets as p-Bits for Spin Logic">25</a>]</cite> or may be orchestrated using an algorithm like Gibbs sampling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib67" title="Training deep Boltzmann networks with sparse Ising machines">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib64" title="Integer factorization using stochastic magnetic tunnel junctions">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib65" title="CMOS plus stochastic nanomagnets enabling heterogeneous computers for probabilistic inference and learning">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib15" title="Scalable connectivity for ising machines: dense to sparse">57</a>]</cite>. Using probabilistic hardware to accelerate EBMs falls under the broad umbrella of <span class="ltx_text ltx_font_italic">thermodynamic computing</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib51" title="Thermodynamic computing">19</a>]</cite>.</p>
</div>
<div class="ltx_para" id="p8">
<p class="ltx_p">Past attempts at EBM accelerators have suffered from issues at both the architectural and hardware levels. All previous proposals used EBMs as monolithic models of data distributions, which is known to be challenging to scale <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib52" title="Implicit generation and modeling with energy based models">23</a>]</cite>. Additionally, existing devices have relied on exotic components such as magnetic tunnel junctions as sources of intense thermal noise for random-number generation (RNG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib90" title="Correlation free large-scale probabilistic computing using a true-random chaotic oscillator p-bit">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib93" title="Stochastic logic in biased coupled photonic probabilistic bits">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib65" title="CMOS plus stochastic nanomagnets enabling heterogeneous computers for probabilistic inference and learning">60</a>]</cite>. These exotic components have not yet been tightly integrated with transistors in commercial CMOS processes and do not currently constitute a scalable solution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib96" title="Low Energy Barrier Nanomagnet Design for Binary Stochastic Neurons: Design Challenges for Real Nanomagnets With Fabrication Defects">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib97" title="Sensitivity of the Power Spectra of Thermal Magnetization Fluctuations in Low Barrier Nanomagnets Proposed for Stochastic Computing to In-Plane Barrier Height Variations and Structural Defects">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib98" title="Reliability and Scalability of p-Bits Implemented With Low Energy Barrier Nanomagnets">22</a>]</cite>.</p>
</div>
<div class="ltx_para" id="p9">
<p class="ltx_p">In this work, we address these issues and propose a commercially viable probabilistic computing system. Our contributions extend from broad architectural choices down to designing and fabricating novel mixed-signal RNG circuitry.</p>
</div>
<div class="ltx_para" id="p10">
<p class="ltx_p">At the top level, we introduce a new probabilistic computer architecture that runs Denoising Thermodynamic Models (DTMs) instead of monolithic EBMs. As their name suggests, rather than using the hardware’s EBM to model data distributions directly, DTMs sequentially compose many hardware EBMs to model a process that <span class="ltx_text ltx_font_italic">denoises</span> the data gradually. Diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib68" title="Deep Unsupervised Learning using Nonequilibrium Thermodynamics">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib70" title="Denoising Diffusion Probabilistic Models">35</a>]</cite> also follow this denoising procedure and are much more capable than EBMs. This key architectural change addresses a fundamental issue with previous approaches and represents the first scalable method for applying probabilistic hardware to machine learning.</p>
</div>
<div class="ltx_para" id="p11">
<p class="ltx_p">Additionally, we show that our new architecture can be implemented at scale using present-day CMOS processes by experimentally demonstrating an all-transistor RNG that is fast, energy efficient, and small. By using transistors as the only building blocks of our RNG circuits, we avoid the significant and ambiguous communication overheads that can occur at interfaces between technologies. Furthermore, the absence of such communication overhead allows for the principled forecasting of device performance that we present in this work. Our RNG leverages the stochastic dynamics of subthreshold transistor networks, which we have recently studied in detail in Ref. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib1" title="Taming non-equilibrium thermal fluctuations in subthreshold CMOS circuits">26</a>]</cite>.</p>
</div>
<div class="ltx_para" id="p12">
<p class="ltx_p">Our system-level analysis indicates that combining our new architecture with our all-transistor probabilistic computing circuitry could achieve unprecedented energy efficiency in probabilistic modeling. Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="Figure 1 ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a> compares the predicted performance and energy consumption of such a system to several standard deep-learning algorithms running on GPUs and a traditional EBM-based probabilistic computer. The DTM-based probabilistic computer system achieves performance parity with the most efficient GPU-based algorithm while using around four orders of magnitude less energy.</p>
</div>
<div class="ltx_para" id="p13">
<p class="ltx_p">The remainder of this article will substantiate the results presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="Figure 1 ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a>, which are based on a combination of measurements from real circuits, physical models, and simulations. To begin, we introduce a fundamental compromise inherent in using EBMs as standalone models of data, which we refer to as the <span class="ltx_text ltx_font_italic">mixing-expressivity tradeoff</span>. We then discuss how this compromise can be avoided by wielding EBMs as part of a denoising process rather than monolithically. Next, we outline how to build a hardware system using DTMs to implement this denoising process at a very low level. Then, we study simulations of this hardware system, further justifying the results shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="Figure 1 ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a> and highlighting some of the practical merits of DTMs compared to existing approaches. Finally, we conclude by discussing how the capabilities of probabilistic accelerators for machine learning may be scaled by merging them with traditional neural networks.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span>The Challenge with EBMs</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">The fundamental problem of machine learning is inferring the probability distribution that underlies some data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib107" title="Machine learning: Trends, perspectives, and prospects">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib108" title="Probabilistic machine learning and artificial intelligence">28</a>]</cite>. An early approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib22" title="Boltzmann Machines: Constraint Satisfaction Networks that Learn">33</a>]</cite> to this was to use a monolithic EBM (MEBM) to fit a data distribution directly by shaping a parameterized energy function <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S1.p1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation></semantics></math>:</p>
</div>
<div class="ltx_para" id="S1.p2">
<table class="ltx_equation ltx_eqn_table" id="S1.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P(x;\theta)\propto e^{-\mathcal{E}(x,\theta)}," class="ltx_Math" display="block" id="S1.E1.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><msup><mi>e</mi><mrow><mo>−</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">P(x;\theta)\propto e^{-\mathcal{E}(x,\theta)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="x" class="ltx_Math" display="inline" id="S1.p2.m1" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> is a random variable representing the data and <math alttext="\theta" class="ltx_Math" display="inline" id="S1.p2.m2" intent=":literal"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> represents the parameters of the EBM.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">Fitting an MEBM corresponds to assigning low energies to values of <math alttext="x" class="ltx_Math" display="inline" id="S1.p3.m1" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> where data are abundant and high energies to values of <math alttext="x" class="ltx_Math" display="inline" id="S1.p3.m2" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> that are far from data. Real-world data are often clustered into distinct modes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib73" title="Maximum likelihood from incomplete data via the EM algorithm">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib74" title="Mixture density networks">9</a>]</cite>, meaning that a MEBM that fits data well will have a complex, rugged energy landscape with many deep valleys surrounded by tall mountains. This complexity is illustrated by the cartoon in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.F2" title="Figure 2 ‣ I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">2</span></a> (a).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p">Unlike the systems we propose, existing probabilistic computers based on MEBMs struggle with the multimodality of real-world data, which hinders their efficiency. Namely, the amount of energy the computer must expend to draw a sample from the MEBM’s distribution can be tremendous if its energy landscape is very rough.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p">Specifically, sampling algorithms that operate in high dimensions (such as Gibbs sampling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib6" title="Probabilistic Machine Learning: Advanced Topics">48</a>]</cite>) are locally-informed iterative procedures, meaning that they sample a landscape by randomly making small movements in the space based on low-dimensional information. When using such a procedure to sample from Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.E1" title="Equation 1 ‣ I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a>), the probability that the iteration will move up in energy to some state <math alttext="X[k+1]" class="ltx_Math" display="inline" id="S1.p5.m1" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">X[k+1]</annotation></semantics></math> is exponentially small in the energy increase compared to the current state <math alttext="X[k]" class="ltx_Math" display="inline" id="S1.p5.m2" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">X[k]</annotation></semantics></math>, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{P}(X[k+1]=x^{\prime}|X[k]=x)\propto e^{-\left(\mathcal{E}(x^{\prime})-\mathcal{E}(x)\right)}." class="ltx_Math" display="block" id="S1.E2.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi mathvariant="normal">ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>x</mi><mo>′</mo></msup><mo fence="false">|</mo><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>=</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><msup><mi>e</mi><mrow><mo>−</mo><mrow><mo>(</mo><mrow><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></msup></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbb{P}(X[k+1]=x^{\prime}|X[k]=x)\propto e^{-\left(\mathcal{E}(x^{\prime})-\mathcal{E}(x)\right)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">For large differences in energy, like those encountered when trying to move between two valleys separated by a significant barrier, this probability can be very close to zero. These barriers grind the iterative sampler to a halt.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p">The mixing-expressivity tradeoff (MET) summarizes this issue with existing probabilistic computer architectures, reflecting the fact that modeling performance and sampling hardness are coupled for MEBMs. Specifically, as the expressivity (modeling performance) of an MEBM increases, its <span class="ltx_text ltx_font_italic">mixing time</span> (the amount of computational effort needed to draw independent samples from the MEBM’s distribution) becomes progressively longer, resulting in expensive inference and unstable training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib75" title="Efficient training of energy-based models using Jarzynski equality">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib76" title="Parallel tempering for training of restricted Boltzmann machines">21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p">The empirical effect of the MET on the efficiency of MEBM-based probabilistic computing systems is illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.F2" title="Figure 2 ‣ I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">2</span></a> (b). Mixing time increases very rapidly with performance, inflating the amount of energy required to sample from the model. The effect of this increased mixing time is reflected in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="Figure 1 ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a>: despite the MEBM-based solution using the same EBMs and underlying hardware as the DTM-based solution, its energy consumption is several orders of magnitude larger due to the glacially slow mixing.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="943" id="S1.F2.g1" src="x2.png" width="623"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold">The mixing-expressivity tradeoff.</span> <span class="ltx_text ltx_font_bold">(a)</span> A cartoon illustrating the mixing-expressivity tradeoff in EBMs. It shows a projection of an energy landscape fit to a simple dataset. The "airplane" mode is well separated from the "dog" mode, with very little data in between. Progressively better fits of the EBM to the data tend to feature larger energy barriers <math alttext="\Delta E" class="ltx_Math" display="inline" id="S1.F2.m2" intent=":literal"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">\Delta E</annotation></semantics></math> between the modes, making the EBM increasingly difficult to sample from. <span class="ltx_text ltx_font_bold">(b)</span> An example of the effect of the mixing-expressivity tradeoff on model performance as measured using the Fashion-MNIST dataset. The blue curve in the plot shows the results of experiments on MEBMs with limited allowed mixing time. Performance and mixing time are strongly correlated. Mixing times were computed by fitting an exponential function to the large-lag behavior of the autocorrelation function; see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A11" title="Appendix K MEBM experiments ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">K</span></a>. In contrast, a DTM (orange cross) has higher performance despite substantially lower sampling requirements.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span>Denoising thermodynamic models</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p">The MET makes it clear that MEBMs have a flaw that makes them challenging and energetically costly to scale. However, this flaw is avoidable, and many types of probabilistic machine learning models have been developed to solve the distribution modeling problem while circumventing the MET.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p">Denoising diffusion models were explicitly designed to sidestep the MET by gradually building complexity through a series of simple, easy-to-sample probabilistic transformations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib68" title="Deep Unsupervised Learning using Nonequilibrium Thermodynamics">62</a>]</cite>. By doing so, they allowed for much more complex distributions to be expressed given a fixed compute budget and substantially expanded the capabilities of generative models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib109" title="Cascaded diffusion models for high fidelity image generation">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib110" title="High-resolution image synthesis with latent diffusion models">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib111" title="Photorealistic text-to-image diffusion models with deep language understanding">56</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p">DTMs merge EBMs with diffusion models, offering an alternative path for probabilistic computing that assuages the MET. DTMs are a slight generalization of recent work from deep learning practitioners that has pushed the frontier of EBM performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib69" title="Learning Energy-Based Models by Diffusion Recovery Likelihood">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib80" title="Latent Diffusion Energy-Based Model for Interpretable Text Modelling">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib81" title="Energy-based diffusion language models for text generation">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib82" title="Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood">75</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p">Instead of trying to use a single EBM to model the data, DTMs chain many EBMs to gradually build up to the complexity of the data distribution. This gradual buildup of complexity allows the landscape of each EBM in the chain to remain relatively simple (and easy to sample) without limiting the complexity of the distribution modeled by the chain as a whole; see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.F2" title="Figure 2 ‣ I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">2</span></a> (b).</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p">Denoising models attempt to reverse a process that gradually transforms the data distribution <math alttext="Q(x^{0})" class="ltx_Math" display="inline" id="S2.p5.m1" intent=":literal"><semantics><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mn>0</mn></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q(x^{0})</annotation></semantics></math> into simple noise. This forward process is given by the Markov chain</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q(x^{0},\dots,x^{T})=Q(x^{0})\prod_{t=1}^{T}Q(x^{t}|x^{t-1})." class="ltx_Math" display="block" id="S2.E3.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mn>0</mn></msup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>x</mi><mi>T</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mn>0</mn></msup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mi>t</mi></msup><mo fence="false">|</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">Q(x^{0},\dots,x^{T})=Q(x^{0})\prod_{t=1}^{T}Q(x^{t}|x^{t-1}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The forward process is typically chosen such that it has a unique stationary distribution <math alttext="Q(x^{T})" class="ltx_Math" display="inline" id="S2.p5.m2" intent=":literal"><semantics><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>T</mi></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q(x^{T})</annotation></semantics></math>, which takes a simple form (e.g., Gaussian or uniform).</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p">Reversal of the forward process is achieved by learning a set of distributions <math alttext="P_{\theta}(x^{t-1}|x^{t})" class="ltx_Math" display="inline" id="S2.p6.m1" intent=":literal"><semantics><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{\theta}(x^{t-1}|x^{t})</annotation></semantics></math> that approximate the reversal of each conditional in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.E3" title="Equation 3 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">3</span></a>). In doing so, we learn a map from simple noise to the data distribution, which can then be used to generate new data.</p>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p">In traditional diffusion models, the forward process is made to be sufficiently fine-grained (using a large number of steps <math alttext="T" class="ltx_Math" display="inline" id="S2.p7.m1" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>) such that the conditional distribution of each step in the reverse process takes some simple form (such as Gaussian or categorical). This simple distribution is parameterized by a neural network, which is then trained to minimize the Kullback-Leibler (KL) divergence between the joint distributions <math alttext="Q" class="ltx_Math" display="inline" id="S2.p7.m2" intent=":literal"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> and <math alttext="P_{\theta}" class="ltx_Math" display="inline" id="S2.p7.m3" intent=":literal"><semantics><msub><mi>P</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">P_{\theta}</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{DN}(\theta)=D\left(Q(x^{0},\dots,x^{T})\middle\|P_{\theta}(x^{0},\dots,x^{T})\right)," class="ltx_math_unparsed" display="block" id="S2.E4.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mi>D</mi><mrow><mo>(</mo><mi>Q</mi><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mn>0</mn></msup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>x</mi><mi>T</mi></msup><mo stretchy="false">)</mo></mrow><mo fence="true" lspace="0em" rspace="0em" stretchy="true">∥</mo><msub><mi>P</mi><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mn>0</mn></msup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>x</mi><mi>T</mi></msup><mo stretchy="false">)</mo></mrow><mo>)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{DN}(\theta)=D\left(Q(x^{0},\dots,x^{T})\middle\|P_{\theta}(x^{0},\dots,x^{T})\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the joint distribution of the model is the product of the learned conditionals:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P_{\theta}(x^{0},\dots,x^{T})=Q(x^{T})\prod_{t=1}^{T}P_{\theta}(x^{t-1}|x^{t})." class="ltx_Math" display="block" id="S2.E5.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mn>0</mn></msup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>x</mi><mi>T</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>T</mi></msup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">P_{\theta}(x^{0},\dots,x^{T})=Q(x^{T})\prod_{t=1}^{T}P_{\theta}(x^{t-1}|x^{t}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS2" title="A.2 Reverse Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">A.A.2</span></a> for more details.</p>
</div>
<div class="ltx_para" id="S2.p8">
<p class="ltx_p">EBM-based denoising models approach the problem from a different angle <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib69" title="Learning Energy-Based Models by Diffusion Recovery Likelihood">27</a>]</cite>. In many cases, it is straightforward to re-cast the forward process in an exponential form,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q(x^{t}|x^{t-1})\propto e^{-\mathcal{E}^{f}_{t-1}\left(x^{t-1},x^{t}\right)}," class="ltx_Math" display="block" id="S2.E6.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mi>t</mi></msup><mo fence="false">|</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><msup><mi>e</mi><mrow><mo>−</mo><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>f</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">Q(x^{t}|x^{t-1})\propto e^{-\mathcal{E}^{f}_{t-1}\left(x^{t-1},x^{t}\right)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathcal{E}^{f}_{t-1}" class="ltx_Math" display="inline" id="S2.p8.m1" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>f</mi></msubsup><annotation encoding="application/x-tex">\mathcal{E}^{f}_{t-1}</annotation></semantics></math> is the energy function associated with the forward process step that adds noise to <math alttext="x^{t-1}" class="ltx_Math" display="inline" id="S2.p8.m2" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">x^{t-1}</annotation></semantics></math>. We then use an EBM with a particular energy function to model the conditional, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P_{\theta}(x^{t-1}|x^{t})\propto e^{-\left(\mathcal{E}^{f}_{t-1}\left(x^{t-1},x^{t}\right)+\mathcal{E}^{\theta}_{t-1}\left(x^{t-1},\theta\right)\right)}." class="ltx_Math" display="block" id="S2.E7.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><msup><mi>e</mi><mrow><mo>−</mo><mrow><mo>(</mo><mrow><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>f</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></msup></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">P_{\theta}(x^{t-1}|x^{t})\propto e^{-\left(\mathcal{E}^{f}_{t-1}\left(x^{t-1},x^{t}\right)+\mathcal{E}^{\theta}_{t-1}\left(x^{t-1},\theta\right)\right)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.p9">
<p class="ltx_p">Equation (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.E7" title="Equation 7 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">7</span></a>) allows for a compromise between the number of steps in the approximation to the reverse process and the difficulty of sampling at each step. As the number of steps in the forward process is increased, the effect of each noising step becomes smaller, meaning that <math alttext="\mathcal{E}^{f}_{t-1}" class="ltx_Math" display="inline" id="S2.p9.m1" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>f</mi></msubsup><annotation encoding="application/x-tex">\mathcal{E}^{f}_{t-1}</annotation></semantics></math> more tightly binds <math alttext="x^{t}" class="ltx_Math" display="inline" id="S2.p9.m2" intent=":literal"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding="application/x-tex">x^{t}</annotation></semantics></math> to <math alttext="x^{t-1}" class="ltx_Math" display="inline" id="S2.p9.m3" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">x^{t-1}</annotation></semantics></math>. This binding can simplify the distribution given in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.E7" title="Equation 7 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">7</span></a>) by imposing an energy penalty that prevents it from being strongly multimodal; see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS4" title="A.4 Simplification of the Energy Landscape ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">A.A.4</span></a> for further discussion.</p>
</div>
<div class="ltx_para" id="S2.p10">
<p class="ltx_p">As illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.F3" title="Figure 3 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">3</span></a> (a), models of the form given in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.E7" title="Equation 7 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">7</span></a>) reshape simple noise into an approximation of the data distribution. Increasing <math alttext="T" class="ltx_Math" display="inline" id="S2.p10.m1" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> while holding the EBM architecture constant simultaneously increases the expressive power of the chain and makes each step easier to sample from, entirely bypassing the MET.</p>
</div>
<div class="ltx_para" id="S2.p11">
<p class="ltx_p">To maximally leverage probabilistic hardware for EBM sampling, DTMs generalize Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.E7" title="Equation 7 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">7</span></a>) by introducing latent variables <math alttext="\{z^{t}\}" class="ltx_Math" display="inline" id="S2.p11.m1" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msup><mi>z</mi><mi>t</mi></msup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{z^{t}\}</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P_{\theta}(x^{t-1}|x^{t})\propto\sum_{z^{t-1}}e^{-\left(\mathcal{E}^{f}_{t-1}\left(x^{t-1},x^{t}\right)+\mathcal{E}^{\theta}_{t-1}\left(x^{t-1},z^{t-1},\theta\right)\right)}." class="ltx_Math" display="block" id="S2.E8.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">∝</mo><mrow><munder><mo movablelimits="false">∑</mo><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></munder><msup><mi>e</mi><mrow><mo>−</mo><mrow><mo>(</mo><mrow><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>f</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></msup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">P_{\theta}(x^{t-1}|x^{t})\propto\sum_{z^{t-1}}e^{-\left(\mathcal{E}^{f}_{t-1}\left(x^{t-1},x^{t}\right)+\mathcal{E}^{\theta}_{t-1}\left(x^{t-1},z^{t-1},\theta\right)\right)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Introducing latent variables allows the size and complexity of the probabilistic model to be increased independently of the data
dimension.</p>
</div>
<div class="ltx_para" id="S2.p12">
<p class="ltx_p">A convenient property of DTMs is that if the approximation to the reverse-process conditional is exact (<math alttext="P_{\theta}(x^{t-1}|x^{t})\to Q(x^{t-1}|x^{t})" class="ltx_Math" display="inline" id="S2.p12.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">→</mo><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">P_{\theta}(x^{t-1}|x^{t})\to Q(x^{t-1}|x^{t})</annotation></semantics></math>), one also learns the marginal distribution at <math alttext="t-1" class="ltx_Math" display="inline" id="S2.p12.m2" intent=":literal"><semantics><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t-1</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q(x^{t-1})\propto\sum_{z^{t-1}}e^{-\mathcal{E}^{\theta}_{t-1}\left(x^{t-1},z^{t-1},\theta\right)}." class="ltx_Math" display="block" id="S2.E9.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">∝</mo><mrow><munder><mo movablelimits="false">∑</mo><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></munder><msup><mi>e</mi><mrow><mo>−</mo><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow></msup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">Q(x^{t-1})\propto\sum_{z^{t-1}}e^{-\mathcal{E}^{\theta}_{t-1}\left(x^{t-1},z^{t-1},\theta\right)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS6" title="A.6 Learning the marginal ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">A.A.6</span></a> for further details. Note that this property relies on the normalizing constant associated with the distribution in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.E6" title="Equation 6 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">6</span></a>) being independent of <math alttext="x^{t-1}" class="ltx_Math" display="inline" id="S2.p12.m3" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">x^{t-1}</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="892" id="S2.F3.g1" src="x3.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold">The denoising thermodynamic computer architecture.</span> <span class="ltx_text ltx_font_bold">(a)</span> Traditional diffusion models have simple conditionals and must take small steps when approximating the reverse process. Since EBMs can express more complex distributions, DTMs can take potentially much larger steps. <span class="ltx_text ltx_font_bold">(b)</span> A sketch of how a chip based on the DTCA chains hardware EBMs to approximate the reverse process. Each EBM is implemented by distinct circuitry, parts of which are dedicated to receiving the inputs and conditionally sampling the outputs and latents. <span class="ltx_text ltx_font_bold">(c)</span> An abstract diagram of a hardware EBM. The state variables <math alttext="x^{t}" class="ltx_Math" display="inline" id="S2.F3.m7" intent=":literal"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding="application/x-tex">x^{t}</annotation></semantics></math> and <math alttext="x^{t-1}" class="ltx_Math" display="inline" id="S2.F3.m8" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">x^{t-1}</annotation></semantics></math> map onto distinct physical degrees of freedom represented by the blue and green nodes, respectively. The coupling between these two sets of nodes implements the forward process energy function <math alttext="\mathcal{E}_{t}^{f}\left(x^{t-1},x^{t}\right)" class="ltx_Math" display="inline" id="S2.F3.m9" intent=":literal"><semantics><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mi>t</mi><mi>f</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}_{t}^{f}\left(x^{t-1},x^{t}\right)</annotation></semantics></math>. The set of orange nodes represents a set of latent variables <math alttext="z^{t-1}" class="ltx_Math" display="inline" id="S2.F3.m10" intent=":literal"><semantics><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">z^{t-1}</annotation></semantics></math>. The couplings between these nodes and to the <math alttext="x^{t-1}" class="ltx_Math" display="inline" id="S2.F3.m11" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">x^{t-1}</annotation></semantics></math> nodes implements <math alttext="\mathcal{E}_{t-1}^{\theta}\left(z^{t-1},x^{t-1}\right)" class="ltx_Math" display="inline" id="S2.F3.m12" intent=":literal"><semantics><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}_{t-1}^{\theta}\left(z^{t-1},x^{t-1}\right)</annotation></semantics></math>.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span>Denoising Thermodynamic Computers</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">The Denoising Thermodynamic Computer Architecture (DTCA) tightly integrates DTMs into probabilistic hardware, allowing for the highly efficient implementation of EBM-aided diffusion models.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p">Practical implementations of the DTCA utilize natural-to-implement EBMs that exhibit sparse and local connectivity, as is typical in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib67" title="Training deep Boltzmann networks with sparse Ising machines">49</a>]</cite>. This constraint allows sampling of the EBM to be performed by massively parallel arrays of primitive circuitry that implement Gibbs sampling. Refer to Appendices <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A2" title="Appendix B Hardware accelerators for EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">B</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3" title="Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">C</span></a> for a further theoretical discussion of the hardware architecture.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p">A key feature of the DTCA is that <math alttext="\mathcal{E}^{f}_{t-1}" class="ltx_Math" display="inline" id="S3.p3.m1" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>f</mi></msubsup><annotation encoding="application/x-tex">\mathcal{E}^{f}_{t-1}</annotation></semantics></math> can be implemented efficiently using our constrained EBMs. Specifically, for both continuous and discrete diffusion, <math alttext="\mathcal{E}^{f}_{t-1}" class="ltx_Math" display="inline" id="S3.p3.m2" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>f</mi></msubsup><annotation encoding="application/x-tex">\mathcal{E}^{f}_{t-1}</annotation></semantics></math> can be implemented using a single pairwise interaction between corresponding variables in <math alttext="x^{t}" class="ltx_Math" display="inline" id="S3.p3.m3" intent=":literal"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding="application/x-tex">x^{t}</annotation></semantics></math> and <math alttext="x^{t-1}" class="ltx_Math" display="inline" id="S3.p3.m4" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">x^{t-1}</annotation></semantics></math>; see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS1" title="A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">A.A.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3.SS1" title="C.1 Implementation of the forward process energy function ‣ Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">C.C.1</span></a> for details. This structure can be reflected in how the chip is laid out to implement these interactions without violating locality constraints.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p">Critically, Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.E8" title="Equation 8 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">8</span></a>) places no constraints on the form of <math alttext="\mathcal{E}_{t-1}^{\theta}" class="ltx_Math" display="inline" id="S3.p4.m1" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><annotation encoding="application/x-tex">\mathcal{E}_{t-1}^{\theta}</annotation></semantics></math>. Therefore, we are free to use EBMs that our hardware implements especially efficiently. At the lowest level, this corresponds to high-dimensional, regularly structured latent variable EBM. If more powerful models are desired, these hardware latent-variable EBMs can be arbitrarily scaled by combining them into software-defined graphical models.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p">The modular nature of DTMs enables various hardware implementations. For example, each EBM in the chain can be implemented using distinct physical circuitry on the same chip, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.F3" title="Figure 3 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">3</span></a> (b). Alternatively, the various EBMs may be split across several communicating chips or implemented by the same hardware, reprogrammed with distinct sets of weights at different times. For any given EBM in the chain, both the data variables <math alttext="x^{t}" class="ltx_Math" display="inline" id="S3.p5.m1" intent=":literal"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding="application/x-tex">x^{t}</annotation></semantics></math>, <math alttext="x^{t-1}" class="ltx_Math" display="inline" id="S3.p5.m2" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">x^{t-1}</annotation></semantics></math> and the latent variables <math alttext="z^{t-1}" class="ltx_Math" display="inline" id="S3.p5.m3" intent=":literal"><semantics><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">z^{t-1}</annotation></semantics></math> are physically embodied in sampling circuits that are connected in a simple way that reflects the structure of Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.E7" title="Equation 7 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">7</span></a>). This variable structure is shown schematically in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.F3" title="Figure 3 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">3</span></a> (c).</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p">To understand the performance of a future hardware device, we developed a GPU simulator of the DTCA and used it to train a DTM on the Fashion-MNIST dataset. We measure the performance of the DTM using FID and utilize a physical model to estimate the energy required to generate new images. These numbers can be compared to conventional algorithm/hardware pairings, such as a VAE running on a GPU; these results are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="Figure 1 ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p">The DTM that produced the results shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="Figure 1 ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a> used Boltzmann machine EBMs. Boltzmann machines, also known as Ising models in physics, use binary random variables and are the simplest type of discrete-variable EBM.</p>
</div>
<div class="ltx_para" id="S3.p8">
<p class="ltx_p">Boltzmann machines are hardware efficient because the Gibbs sampling update rule required to sample from them is simple. Boltzmann machines implement energy functions of the form</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{E}(x)=-\beta\Big(\sum_{i\neq j}x_{i}J_{ij}x_{j}+\sum_{i=1}h_{i}x_{i}\Big)," class="ltx_Math" display="block" id="S3.E10.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi class="ltx_font_mathcaligraphic">ℰ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.600em" minsize="1.600em">(</mo><mrow><mrow><munder><mo lspace="0em" movablelimits="false">∑</mo><mrow><mi>i</mi><mo>≠</mo><mi>j</mi></mrow></munder><mrow><msub><mi>x</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>J</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></mrow><mo rspace="0.055em">+</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></munder><mrow><msub><mi>h</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow></mrow><mo maxsize="1.600em" minsize="1.600em">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{E}(x)=-\beta\Big(\sum_{i\neq j}x_{i}J_{ij}x_{j}+\sum_{i=1}h_{i}x_{i}\Big),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where each <math alttext="x_{i}\in\{-1,1\}" class="ltx_Math" display="inline" id="S3.p8.m1" intent=":literal"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mrow><mo stretchy="false">{</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">x_{i}\in\{-1,1\}</annotation></semantics></math>.
The Gibbs sampling update rule for sampling from the corresponding EBM is</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{P}\left(X_{i}[k+1]=+1\mid X[k]=x\right)=\sigma\!\bigg(2\beta\Big(\sum_{j\neq i}J_{ij}\,x_{j}+h_{i}\Big)\bigg)," class="ltx_Math" display="block" id="S3.E11.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi mathvariant="normal">ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><msub><mi>X</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mo>+</mo><mrow><mn>1</mn><mo>∣</mo><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo>=</mo><mi>x</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mpadded style="width:0.437em;" width="0.437em"><mi>σ</mi></mpadded><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="2.100em" minsize="2.100em">(</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.600em" minsize="1.600em">(</mo><mrow><mrow><munder><mo lspace="0em" movablelimits="false">∑</mo><mrow><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></munder><mrow><msub><mi>J</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></mrow><mo>+</mo><msub><mi>h</mi><mi>i</mi></msub></mrow><mo maxsize="1.600em" minsize="1.600em">)</mo></mrow></mrow><mo maxsize="2.100em" minsize="2.100em">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbb{P}\left(X_{i}[k+1]=+1\mid X[k]=x\right)=\sigma\!\bigg(2\beta\Big(\sum_{j\neq i}J_{ij}\,x_{j}+h_{i}\Big)\bigg),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which can be evaluated simply using an appropriately biased source of random bits.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="240" id="S3.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold">A programmable source of random bits. (a)</span> A laboratory measurement of the operating characteristic of our RNG. The probability of the output voltage signal being in the high state (<math alttext="x=1" class="ltx_Math" display="inline" id="S3.F4.m6" intent=":literal"><semantics><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x=1</annotation></semantics></math>) can be programmed by varying an input voltage. The relationship between <math alttext="\mathbb{P}(x=1)" class="ltx_Math" display="inline" id="S3.F4.m7" intent=":literal"><semantics><mrow><mi mathvariant="normal">ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{P}(x=1)</annotation></semantics></math> and the input voltage is well-approximated by a sigmoid function. The inset shows the output voltage signal as a function of time for different input voltages. <span class="ltx_text ltx_font_bold">(b)</span> The autocorrelation function of the RNG at the unbiased point (<math alttext="\mathbb{P}(x=1)=0.5" class="ltx_Math" display="inline" id="S3.F4.m8" intent=":literal"><semantics><mrow><mrow><mi mathvariant="normal">ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\mathbb{P}(x=1)=0.5</annotation></semantics></math>). The decay is approximately exponential with the rate <math alttext="\tau_{0}\approx 100\text{ns}" class="ltx_Math" display="inline" id="S3.F4.m9" intent=":literal"><semantics><mrow><msub><mi>τ</mi><mn>0</mn></msub><mo>≈</mo><mrow><mn>100</mn><mo lspace="0em" rspace="0em">​</mo><mtext>ns</mtext></mrow></mrow><annotation encoding="application/x-tex">\tau_{0}\approx 100\text{ns}</annotation></semantics></math>. <span class="ltx_text ltx_font_bold">(c)</span> Estimating the effect of manufacturing variation on RNG performance. Each point in the plot represents the results of a simulation of an RNG circuit with transistor parameters sampled according to a procedure defined by the manufacturer’s PDK. Each color represents a different process corner, each for which <math alttext="\sim 200" class="ltx_Math" display="inline" id="S3.F4.m10" intent=":literal"><semantics><mrow><mi></mi><mo>∼</mo><mn>200</mn></mrow><annotation encoding="application/x-tex">\sim 200</annotation></semantics></math> realizations of the RNG were simulated. The "typical" corner represents a balanced case, whereas the other two are asymmetric corners where the two types of transistors (NMOS and PMOS) are skewed in opposite directions. The slow NMOS and fast PMOS case is worst performing for us due to an asymmetry in our design. </figcaption>
</figure>
<div class="ltx_para" id="S3.p9">
<p class="ltx_p">Implementing our proposed hardware architecture using Boltzmann machines is particularly simple. A device will consist of a regular grid of Bernoulli sampling circuits, where each sampling circuit implements the Gibbs sampling update for a single variable <math alttext="x_{i}" class="ltx_Math" display="inline" id="S3.p9.m1" intent=":literal"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math>. The bias of the sampling circuits (probability that it produces 1 as opposed to <math alttext="-1" class="ltx_Math" display="inline" id="S3.p9.m2" intent=":literal"><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">-1</annotation></semantics></math>) is constrained to be a sigmoidal function of an input voltage, allowing the conditional update given in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.E11" title="Equation 11 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">11</span></a>) to be implemented using a simple circuit that adds currents such as a resistor network (See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS1" title="D.1 Biasing circuit ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">D.D.1</span></a>).</p>
</div>
<div class="ltx_para" id="S3.p10">
<p class="ltx_p">Specifically, the EBMs employed in this work were sparse, deep Boltzmann machines comprising <math alttext="L\times L" class="ltx_Math" display="inline" id="S3.p10.m1" intent=":literal"><semantics><mrow><mi>L</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">L\times L</annotation></semantics></math> grids of binary variables, where <math alttext="L=70" class="ltx_Math" display="inline" id="S3.p10.m2" intent=":literal"><semantics><mrow><mi>L</mi><mo>=</mo><mn>70</mn></mrow><annotation encoding="application/x-tex">L=70</annotation></semantics></math> was used in most cases. Each variable was connected to several (in most cases, 12) of its neighbors following a simple pattern. At random, some of the variables were selected to represent the data <math alttext="x_{t-1}" class="ltx_Math" display="inline" id="S3.p10.m3" intent=":literal"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">x_{t-1}</annotation></semantics></math>, and the rest were assigned to the latent variables <math alttext="z_{t-1}" class="ltx_Math" display="inline" id="S3.p10.m4" intent=":literal"><semantics><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">z_{t-1}</annotation></semantics></math>. Then, an extra node was connected to each data node to implement the coupling to <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.p10.m5" intent=":literal"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math>. See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3" title="Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">C</span></a> for further details on the Boltzmann machine architecture.</p>
</div>
<div class="ltx_para" id="S3.p11">
<p class="ltx_p">Due to our chosen connectivity patterns, our Boltzmann machines are bipartite (two-colorable). Since each color block can be sampled in parallel, a single iteration of Gibbs sampling corresponds to sampling the first color block conditioned on the second and then vice versa. Starting from some random initialization, this block sampling procedure could then be repeated for <math alttext="K" class="ltx_Math" display="inline" id="S3.p11.m1" intent=":literal"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> iterations (where <math alttext="K" class="ltx_Math" display="inline" id="S3.p11.m2" intent=":literal"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> is longer than the mixing time of the sampler, typically <math alttext="K\approx 1000" class="ltx_Math" display="inline" id="S3.p11.m3" intent=":literal"><semantics><mrow><mi>K</mi><mo>≈</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">K\approx 1000</annotation></semantics></math>) to draw samples from Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.E7" title="Equation 7 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">7</span></a>) for each step in the approximation to the reverse process.</p>
</div>
<div class="ltx_para" id="S3.p12">
<p class="ltx_p">To enable a near-term, large-scale realization of the DTCA, we leveraged the shot-noise dynamics of subthreshold transistors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib1" title="Taming non-equilibrium thermal fluctuations in subthreshold CMOS circuits">26</a>]</cite> to build an RNG that is fast, energy-efficient, and small. Our all-transistor RNG is programmable and has the desired sigmoidal response to a control voltage, as shown by experimental measurements in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.F4" title="Figure 4 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">4</span></a> (a). The stochastic voltage signal output from the RNG has an approximately exponential autocorrelation function that decays in around <math alttext="100" class="ltx_Math" display="inline" id="S3.p12.m1" intent=":literal"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math> ns, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.F4" title="Figure 4 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">4</span></a> (b). As shown in Ref. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib1" title="Taming non-equilibrium thermal fluctuations in subthreshold CMOS circuits">26</a>]</cite>, this time constraint is much larger than the lower limit imposed by the correlation time of the noise in our transistors. The RNG could, therefore, be made much faster via an improved design. Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A10" title="Appendix J Some details on our RNG ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">J</span></a> provides further details about our RNG.</p>
</div>
<div class="ltx_para" id="S3.p13">
<p class="ltx_p">A practical advantage to our all-transistor RNG is that detailed and proven foundry-provided models can be used to study the effect of manufacturing variations on our circuit design. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.F4" title="Figure 4 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">4</span></a> (c), we use this process development kit (PDK) to study the speed and energy consumption of our RNG as a function of both systematic inter-wafer skews to the transistor parameters (process corners) and the expected variation within a single chip. We find that the RNG works reliably despite these non-idealities, meaning it can readily be scaled to the massive grids required by the DTCA.</p>
</div>
<div class="ltx_para" id="S3.p14">
<p class="ltx_p">The energy estimates given in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="Figure 1 ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a> for the probabilistic computer were constructed using a physical model of an all-transistor Boltzmann machine Gibbs sampler. The dominant contributions to this model are captured by the formula</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E=TK_{\text{mix}}L^{2}E_{\text{cell}}," class="ltx_Math" display="block" id="S3.E12.m1" intent=":literal"><semantics><mrow><mrow><mi>E</mi><mo>=</mo><mrow><mi>T</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>K</mi><mtext>mix</mtext></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>L</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>E</mi><mtext>cell</mtext></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">E=TK_{\text{mix}}L^{2}E_{\text{cell}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E_{\text{cell}}=E_{\text{rng}}+E_{\text{bias}}+E_{\text{clock}}+E_{\text{comm}}," class="ltx_Math" display="block" id="S3.E13.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>E</mi><mtext>cell</mtext></msub><mo>=</mo><mrow><msub><mi>E</mi><mtext>rng</mtext></msub><mo>+</mo><msub><mi>E</mi><mtext>bias</mtext></msub><mo>+</mo><msub><mi>E</mi><mtext>clock</mtext></msub><mo>+</mo><msub><mi>E</mi><mtext>comm</mtext></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">E_{\text{cell}}=E_{\text{rng}}+E_{\text{bias}}+E_{\text{clock}}+E_{\text{comm}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="E_{\text{rng}}" class="ltx_Math" display="inline" id="S3.p14.m1" intent=":literal"><semantics><msub><mi>E</mi><mtext>rng</mtext></msub><annotation encoding="application/x-tex">E_{\text{rng}}</annotation></semantics></math> comes from the data in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.F4" title="Figure 4 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">4</span></a> (c). The term <math alttext="E_{\text{bias}}" class="ltx_Math" display="inline" id="S3.p14.m2" intent=":literal"><semantics><msub><mi>E</mi><mtext>bias</mtext></msub><annotation encoding="application/x-tex">E_{\text{bias}}</annotation></semantics></math> is estimated using a physical model of a possible biasing circuit, and <math alttext="E_{\text{clock}}" class="ltx_Math" display="inline" id="S3.p14.m3" intent=":literal"><semantics><msub><mi>E</mi><mtext>clock</mtext></msub><annotation encoding="application/x-tex">E_{\text{clock}}</annotation></semantics></math> and <math alttext="E_{\text{comm}}" class="ltx_Math" display="inline" id="S3.p14.m4" intent=":literal"><semantics><msub><mi>E</mi><mtext>comm</mtext></msub><annotation encoding="application/x-tex">E_{\text{comm}}</annotation></semantics></math> are derived from physical reasoning about the costs of the clock and inter-cell communications respectively. <math alttext="K_{\text{mix}}" class="ltx_Math" display="inline" id="S3.p14.m5" intent=":literal"><semantics><msub><mi>K</mi><mtext>mix</mtext></msub><annotation encoding="application/x-tex">K_{\text{mix}}</annotation></semantics></math> is the number of sampling iterations required to satisfactorily mix the chain for inference, which is generally less than the number of iterations used during training. <math alttext="K_{\text{mix}}=250" class="ltx_Math" display="inline" id="S3.p14.m6" intent=":literal"><semantics><mrow><msub><mi>K</mi><mtext>mix</mtext></msub><mo>=</mo><mn>250</mn></mrow><annotation encoding="application/x-tex">K_{\text{mix}}=250</annotation></semantics></math> was used for the DTM (See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS4" title="D.4 Analysis of a complete sampling program ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">D.D.4</span></a>), while the mixing time measured in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.F2" title="Figure 2 ‣ I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">2</span></a> was used for the MEBM.</p>
</div>
<div class="ltx_para" id="S3.p15">
<p class="ltx_p">This model is approximate, but it captures the underlying physics of a real device and provides a reasonable order-of-magnitude estimate of the actual energy consumption. Generally, given the same transistor process we used for our RNG and some reasonable selections for other free parameters of the model, we can estimate <math alttext="E_{\text{cell}}\approx 2\;\text{fJ}" class="ltx_Math" display="inline" id="S3.p15.m1" intent=":literal"><semantics><mrow><msub><mi>E</mi><mtext>cell</mtext></msub><mo>≈</mo><mrow><mn>2</mn><mo lspace="0.280em" rspace="0em">​</mo><mtext>fJ</mtext></mrow></mrow><annotation encoding="application/x-tex">E_{\text{cell}}\approx 2\;\text{fJ}</annotation></semantics></math>. See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4" title="Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">D</span></a> for an exhaustive derivation of this model.</p>
</div>
<div class="ltx_para" id="S3.p16">
<p class="ltx_p">We use a simple model for the energy consumption of the GPU that underestimates the actual values. We compute the total number of floating-point operations (FLOPs) required to generate a sample from the trained model and divide that by the FLOP/joule specification given by the manufacturer. See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A5" title="Appendix E Energy analysis of GPUs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">E</span></a> for further discussion.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="191" id="S3.F5.g1" src="x5.png" width="660"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold">Detailed results on the Fashion-MNIST dataset.</span> <span class="ltx_text ltx_font_bold">(a)</span> Images generated by a denoising model. Here, to achieve better-looking images, several binary variables were combined to represent a single grayscale pixel. The noisiness of the grayscale levels is an artifact of our embedding method; see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A7" title="Appendix G Embedding integers into Boltzmann machines ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">G</span></a>. <span class="ltx_text ltx_font_bold">(b)</span> An experiment showing how DTMs are more stable to train than MEBMs. Complementing DTMs with the ACP completely stabilizes training. For the DTMs, the maximum <math alttext="r_{yy}[K]" class="ltx_Math" display="inline" id="S3.F5.m3" intent=":literal"><semantics><mrow><msub><mi>r</mi><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mi>y</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">r_{yy}[K]</annotation></semantics></math> value over all the layers is shown. <span class="ltx_text ltx_font_bold">(c)</span> The effect of scaling EBM complexity on DTM performance. The grid size <math alttext="L" class="ltx_Math" display="inline" id="S3.F5.m4" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> was modified to change the number of latent variables compared to the (fixed) number of data variables. Generally, EBM layers with more connectivity and longer allowed mixing times can utilize more latent variables and, therefore, achieve higher performance.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span>Training DTMs</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">The EBMs used in the experiments presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="Figure 1 ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a> were trained by applying the standard Monte-Carlo estimator for the gradients of EBMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib57" title="How to train your energy-based models">64</a>]</cite> to Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.E4" title="Equation 4 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">4</span></a>), which yields</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A11.EGx1">
<tbody id="S4.E14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell" colspan="2"><math alttext="\displaystyle\begin{split}\nabla_{\theta}\mathcal{L}_{DN}(\theta)\!=\!\sum_{t=1}^{T}\mathbb{E}_{Q(x_{t-1},x_{t})}\bigg[&amp;\mathbb{E}_{P_{\theta}(z_{t-1}|x_{t-1},x_{t})}\left[\nabla_{\theta}\mathcal{E}^{m}_{t-1}\right]\\
-&amp;\mathbb{E}_{P_{\theta}(x_{t-1},z_{t-1}|x_{t})}\left[\nabla_{\theta}\mathcal{E}^{m}_{t-1}\right]\bigg].\end{split}" class="ltx_Math" display="inline" id="S4.E14.m1" intent=":literal"><semantics><mtable columnspacing="0pt" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><mrow><msub><mo>∇</mo><mi>θ</mi></msub><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.108em" rspace="0.108em">=</mo><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover></mstyle><msub><mi mathvariant="normal">𝔼</mi><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></msub><mo maxsize="2.100em" minsize="2.100em">[</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><msub><mi mathvariant="normal">𝔼</mi><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow><mo>]</mo></mrow></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><mo>−</mo></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><msub><mi mathvariant="normal">𝔼</mi><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence="false">|</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mrow><mo>[</mo><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo>]</mo></mrow><mo maxsize="2.100em" minsize="2.100em">]</mo><mo lspace="0em">.</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\displaystyle\begin{split}\nabla_{\theta}\mathcal{L}_{DN}(\theta)\!=\!\sum_{t=1}^{T}\mathbb{E}_{Q(x_{t-1},x_{t})}\bigg[&amp;\mathbb{E}_{P_{\theta}(z_{t-1}|x_{t-1},x_{t})}\left[\nabla_{\theta}\mathcal{E}^{m}_{t-1}\right]\\
-&amp;\mathbb{E}_{P_{\theta}(x_{t-1},z_{t-1}|x_{t})}\left[\nabla_{\theta}\mathcal{E}^{m}_{t-1}\right]\bigg].\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Notably, each term in the sum over <math alttext="t" class="ltx_Math" display="inline" id="S4.p1.m1" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> can be computed independently. To estimate either term in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S4.E14" title="Equation 14 ‣ IV Training DTMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">14</span></a>), first, sample tuples <math alttext="(x_{t-1},x_{t})" class="ltx_Math" display="inline" id="S4.p1.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_{t-1},x_{t})</annotation></semantics></math> from the forward process <math alttext="Q(x_{t-1},x_{t})" class="ltx_Math" display="inline" id="S4.p1.m3" intent=":literal"><semantics><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q(x_{t-1},x_{t})</annotation></semantics></math>. Then, for each of these tuples, clamp the reverse process EBM to the sampled values appropriately and use a time average over <math alttext="K" class="ltx_Math" display="inline" id="S4.p1.m4" intent=":literal"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> iterations of Gibbs sampling to estimate the inner expectation value. Averaging the result over the tuples yields the desired gradient estimate.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p">It should be noted that the DTCA allows our EBMs to have finite and short mixing times, which enables sufficient sampling iterations to be used to achieve nearly unbiased estimates of the gradient. Unbiased gradient estimates are not possible for MEBMs in most cases due to their long mixing times <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib47" title="On contrastive divergence learning">14</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p">A well-trained denoising model generates new examples that resemble the training data by incrementally pulling them out of noise; the outputs of an 8-step denoising model trained on the Fashion-MNIST dataset are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.F5" title="Figure 5 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">5</span></a> (a). At the final time <math alttext="T" class="ltx_Math" display="inline" id="S4.p3.m1" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>, the images are random bits. Structure begins to emerge as the chain progresses, ultimately resulting in clean images at time <math alttext="t=0" class="ltx_Math" display="inline" id="S4.p3.m2" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t=0</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p">DTMs alleviate the training instability that is fundamental to MEBMs. The parameters of MEBMs are usually initialized using a strategy that results in an easy-to-sample-from energy landscape <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib58" title="A practical guide to training restricted Boltzmann machines">34</a>]</cite>. For this reason, in the early stages of training, sampling from Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.E1" title="Equation 1 ‣ I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a>) is possible, and the gradient estimates produced using Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S4.E14" title="Equation 14 ‣ IV Training DTMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">14</span></a>) are unbiased. However, as these gradients are followed, the MEBM is reshaped according to the data distribution and begins to become complex and multimodal. This induced multimodality greatly increases the sampling complexity of the distribution, causing samples to deviate from equilibrium. Gradients computed using non-equilibrium samples do not necessarily point in a meaningful direction, which can halt or, in some cases, even reverse the training process.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p">This instability in MEBMs leads to unpredictable training dynamics that can be sensitive to implementation details. An example of the training dynamics for several different types of models is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.F5" title="Figure 5 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">5</span></a> (b). The top plot displays the quality of images generated during training, while the bottom plot shows a measure of the sampler’s mixing. Image quality is measured using the FID metric, and mixing quality is measured using the normalized autocorrelation</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r_{yy}[k]=\frac{\mathbb{E}\left[\left(y[j]-\mu\right)\left(y[j+k]-\mu\right)\right]}{\mathbb{E}\left[\left(y[j]-\mu\right)^{2}\right]}," class="ltx_Math" display="block" id="S4.E15.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>r</mi><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mi>y</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="normal">𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mo>(</mo><mrow><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo></mrow></mrow><mo>−</mo><mi>μ</mi></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>j</mi><mo>+</mo><mi>k</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>−</mo><mi>μ</mi></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mrow><mi mathvariant="normal">𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><msup><mrow><mo>(</mo><mrow><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo></mrow></mrow><mo>−</mo><mi>μ</mi></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>]</mo></mrow></mrow></mfrac></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">r_{yy}[k]=\frac{\mathbb{E}\left[\left(y[j]-\mu\right)\left(y[j+k]-\mu\right)\right]}{\mathbb{E}\left[\left(y[j]-\mu\right)^{2}\right]},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(15)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S4.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mu=\mathbb{E}(y[j])," class="ltx_Math" display="block" id="S4.E16.m1" intent=":literal"><semantics><mrow><mrow><mi>μ</mi><mo>=</mo><mrow><mi mathvariant="normal">𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mu=\mathbb{E}(y[j]),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="k" class="ltx_Math" display="inline" id="S4.p5.m1" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> is the delay time; <math alttext="y[j]" class="ltx_Math" display="inline" id="S4.p5.m2" intent=":literal"><semantics><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">y[j]</annotation></semantics></math> is some low dimensional projection of the sampling chain data at iteration <math alttext="j" class="ltx_Math" display="inline" id="S4.p5.m3" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>, <math alttext="y[j]=f(x[j])" class="ltx_Math" display="inline" id="S4.p5.m4" intent=":literal"><semantics><mrow><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">y[j]=f(x[j])</annotation></semantics></math>; and <math alttext="\mathbb{E}\left[\cdot\right]" class="ltx_Math" display="inline" id="S4.p5.m5" intent=":literal"><semantics><mrow><mi mathvariant="normal">𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}\left[\cdot\right]</annotation></semantics></math> indicates expectation values taken over independent Gibbs sampling chains. The lower plot in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.F5" title="Figure 5 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">5</span></a> (b) shows the autocorrelation at a delay equal to the total number of sampling iterations used to estimate the gradients during training. Generally, if <math alttext="r_{yy}" class="ltx_Math" display="inline" id="S4.p5.m6" intent=":literal"><semantics><msub><mi>r</mi><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mi>y</mi></mrow></msub><annotation encoding="application/x-tex">r_{yy}</annotation></semantics></math> is close to 1, gradients were estimated using far-from-equilibrium samples and were likely of low quality. If it is close to zero, the samples should be close to equilibrium and produce high-quality gradient estimates. See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A8" title="Appendix H The autocorrelation function and mixing time ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">H</span></a> for further discussion.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p">The destabilizing effect of non-equilibrium sampling is apparent from the blue curves in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.F5" title="Figure 5 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">5</span></a> (b). At the beginning of training, both quality and <math alttext="r_{yy}" class="ltx_Math" display="inline" id="S4.p6.m1" intent=":literal"><semantics><msub><mi>r</mi><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mi>y</mi></mrow></msub><annotation encoding="application/x-tex">r_{yy}</annotation></semantics></math> increase, indicating that the multimodality of the data is being imprinted on the model. Then <math alttext="r_{yy}" class="ltx_Math" display="inline" id="S4.p6.m2" intent=":literal"><semantics><msub><mi>r</mi><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mi>y</mi></mrow></msub><annotation encoding="application/x-tex">r_{yy}</annotation></semantics></math> becomes so large that the quality of the gradient starts to decline, resulting in a plateau and, ultimately, a degradation of the model’s quality.</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p">Denoising alone significantly stabilizes training. Because the transformation carried out by each layer is simpler, the distribution that the model must learn is less complex and, therefore, easier to sample from. The orange curve in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.F5" title="Figure 5 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">5</span></a> (b) shows the training dynamics for a typical denoising model. The autocorrelation and performance remain good for much longer than the MEBM.</p>
</div>
<div class="ltx_para" id="S4.p8">
<p class="ltx_p">As training progresses, the DTM eventually becomes unstable, which can be attributed to the development of a complex energy landscape among the latent variables. To combat this, we modify the training procedure to penalize models that mix poorly. We add a term to the loss function that nudges the optimization towards a distribution that is easy to sample from, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}^{TC}_{t}=\mathbb{E}_{Q(x^{t})}\left[D\left(\prod_{i=1}^{M}P_{\theta}(s^{t-1}_{i}|x^{t})\middle\|P_{\theta}(s^{t-1}|x^{t})\right)\right]," class="ltx_math_unparsed" display="block" id="S4.E17.m1" intent=":literal"><semantics><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>t</mi><mrow><mi>T</mi><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow></msubsup><mo>=</mo><msub><mi mathvariant="normal">𝔼</mi><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow></mrow></msub><mrow><mo>[</mo><mi>D</mi><mrow><mo>(</mo><munderover><mo lspace="0em" movablelimits="false">∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><msub><mi>P</mi><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><msubsup><mi>s</mi><mi>i</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msup><mi>x</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow><mo fence="true" lspace="0em" rspace="0em" stretchy="true">∥</mo><msub><mi>P</mi><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><msup><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msup><mi>x</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow><mo>)</mo></mrow><mo>]</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}^{TC}_{t}=\mathbb{E}_{Q(x^{t})}\left[D\left(\prod_{i=1}^{M}P_{\theta}(s^{t-1}_{i}|x^{t})\middle\|P_{\theta}(s^{t-1}|x^{t})\right)\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="s^{t-1}=(x^{t-1},z^{t-1})" class="ltx_Math" display="inline" id="S4.p8.m1" intent=":literal"><semantics><mrow><msup><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">s^{t-1}=(x^{t-1},z^{t-1})</annotation></semantics></math> and <math alttext="x^{t-1}_{i}" class="ltx_Math" display="inline" id="S4.p8.m2" intent=":literal"><semantics><msubsup><mi>x</mi><mi>i</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">x^{t-1}_{i}</annotation></semantics></math> indicates the <math alttext="i^{\text{th}}" class="ltx_Math" display="inline" id="S4.p8.m3" intent=":literal"><semantics><msup><mi>i</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">i^{\text{th}}</annotation></semantics></math> of the <math alttext="M" class="ltx_Math" display="inline" id="S4.p8.m4" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> variables in <math alttext="x^{t-1}" class="ltx_Math" display="inline" id="S4.p8.m5" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">x^{t-1}</annotation></semantics></math>. This term penalizes the distance between the learned conditional distribution and a factorized distribution with identical marginals and is a form of total correlation penalty <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib59" title="Isolating sources of disentanglement in variational autoencoders">16</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.p9">
<p class="ltx_p">The total loss function is the sum of Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.E4" title="Equation 4 ‣ II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">4</span></a>) and this total correlation penalty:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E18">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=\mathcal{L}_{DN}+\sum_{t=1}^{T}\lambda_{t}\mathcal{L}^{TC}_{t}." class="ltx_Math" display="block" id="S4.E18.m1" intent=":literal"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></msub><mo rspace="0.055em">+</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><msub><mi>λ</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>t</mi><mrow><mi>T</mi><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow></msubsup></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}=\mathcal{L}_{DN}+\sum_{t=1}^{T}\lambda_{t}\mathcal{L}^{TC}_{t}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The parameters <math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="S4.p9.m1" intent=":literal"><semantics><msub><mi>λ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\lambda_{t}</annotation></semantics></math> control the relative strength of the total correlation penalty for each step in the reverse process.</p>
</div>
<div class="ltx_para" id="S4.p10">
<p class="ltx_p">We use an Adaptive Correlation Penalty (ACP) to set the <math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="S4.p10.m1" intent=":literal"><semantics><msub><mi>λ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\lambda_{t}</annotation></semantics></math> as large as necessary to keep sampling tractable for each layer. During training, we periodically measure the autocorrelations of each learned conditional at a delay equal to the number of sampling iterations used during gradient estimation. If the autocorrelation for the <math alttext="j^{\text{th}}" class="ltx_Math" display="inline" id="S4.p10.m2" intent=":literal"><semantics><msup><mi>j</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">j^{\text{th}}</annotation></semantics></math> layer is close to zero, <math alttext="\lambda_{j}" class="ltx_Math" display="inline" id="S4.p10.m3" intent=":literal"><semantics><msub><mi>λ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\lambda_{j}</annotation></semantics></math> is decreased, and vice-versa.</p>
</div>
<div class="ltx_para" id="S4.p11">
<p class="ltx_p">Our closed-loop control of the correlation penalty strengths is crucial, allowing us to maximize the expressivity of the EBMs while maintaining stable training. The green curves in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.F5" title="Figure 5 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">5</span></a> (b) show an example of training dynamics under this closed-loop control policy. Model quality increases monotonically, and the autocorrelation stays small throughout training. This closed-loop control of the correlation penalty was employed during the training of most models used to produce the results in this article, including those shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="Figure 1 ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S4.p12">
<p class="ltx_p">Generally, the performance of DTMs improves as their size increases. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="Figure 1 ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a>, increasing the depth of the DTM from 2 to 8 substantially improves the quality of generated images. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.F5" title="Figure 5 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">5</span></a> (c), increasing the width, degree, and allowed mixing time of the EBMs in the chain also generally improves performance.</p>
</div>
<div class="ltx_para" id="S4.p13">
<p class="ltx_p">However, some subtleties prevent this specific EBM topology from being scaled indefinitely. The top plot in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.F5" title="Figure 5 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">5</span></a> (c) shows that scaling the number of latent variables (with fixed allowed mixing time) only improves performance if the connectivity of the graph is also scaled; otherwise, performance can decrease. This dependence makes sense, as increasing the number of latent variables in this way increases the depth of the Boltzmann machine, which is known to make sampling more difficult. Beyond a certain point, increasing the model’s ability to express complex energy landscapes may render it unable to learn, given the allowed mixing time of <math alttext="K\approx 1000" class="ltx_Math" display="inline" id="S4.p13.m1" intent=":literal"><semantics><mrow><mi>K</mi><mo>≈</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">K\approx 1000</annotation></semantics></math>. This same effect is shown in the bottom plot of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.F5" title="Figure 5 ‣ III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">5</span></a> (c), which demonstrates that larger values of <math alttext="K" class="ltx_Math" display="inline" id="S4.p13.m2" intent=":literal"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> are required to support wider models holding connectivity constant.</p>
</div>
<div class="ltx_para" id="S4.p14">
<p class="ltx_p">In general, it would be naive to expect that a hardware-efficient EBM topology can be scaled in isolation to model arbitrarily complex datasets. For example, there is no good reason for which a connectivity pattern that is convenient from a wire-routing perspective would also be well suited to represent the correlation structure of a complex real-world dataset.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span>Conclusion: Scaling Thermodynamic Machine Learning</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p">The core doctrine of modern machine learning is the relentless scaling of models as a means of solving ever-harder problems. Models that utilize probabilistic computers may be similarly scaled to enhance their capabilities beyond the relatively simple dataset considered in this work so far.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p">However, we hypothesize that the correct way to scale probabilistic machine learning hardware systems is not in isolation but rather as a component in a larger <em class="ltx_emph ltx_font_italic">hybrid thermodynamic-deterministic machine learning</em> (HTDML) system. Such a hybrid system integrates probabilistic hardware with more traditional machine learning accelerators.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p">A hybrid approach is sensible because there is no a priori reason to believe that a probabilistic computer should handle every part of a machine learning problem, and sometimes a deterministic processor is likely a better tool for the job.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p">The goal of HTDML is to design practical machine learning systems that minimize the energy used to achieve desired modeling fidelity on a particular task. This efficiency will be achieved through a cross-disciplinary effort that eschews the software/hardware abstraction barrier to design computers that respect physical constraints.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p">Mathematically, the landscape of HTDML may be summarized as</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E19">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E_{\text{tot}}(S,D,p)=E_{\text{det}}(S,D,p)+E_{\text{prob}}(S,D,p)," class="ltx_Math" display="block" id="S5.E19.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>E</mi><mtext>tot</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>E</mi><mtext>det</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>E</mi><mtext>prob</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>S</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">E_{\text{tot}}(S,D,p)=E_{\text{det}}(S,D,p)+E_{\text{prob}}(S,D,p),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(19)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="E_{\text{tot}}" class="ltx_Math" display="inline" id="S5.p5.m1" intent=":literal"><semantics><msub><mi>E</mi><mtext>tot</mtext></msub><annotation encoding="application/x-tex">E_{\text{tot}}</annotation></semantics></math> is the total energy consumed by some machine learning system <math alttext="S" class="ltx_Math" display="inline" id="S5.p5.m2" intent=":literal"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> to evaluate a model of some dataset <math alttext="D" class="ltx_Math" display="inline" id="S5.p5.m3" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> with performance <math alttext="p" class="ltx_Math" display="inline" id="S5.p5.m4" intent=":literal"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>. <math alttext="E_{\text{tot}}" class="ltx_Math" display="inline" id="S5.p5.m5" intent=":literal"><semantics><msub><mi>E</mi><mtext>tot</mtext></msub><annotation encoding="application/x-tex">E_{\text{tot}}</annotation></semantics></math> decomposes into an energy term that comes from the deterministic computer <math alttext="E_{\text{det}}" class="ltx_Math" display="inline" id="S5.p5.m6" intent=":literal"><semantics><msub><mi>E</mi><mtext>det</mtext></msub><annotation encoding="application/x-tex">E_{\text{det}}</annotation></semantics></math> and a term that comes from the probabilistic computer <math alttext="E_{\text{prob}}" class="ltx_Math" display="inline" id="S5.p5.m7" intent=":literal"><semantics><msub><mi>E</mi><mtext>prob</mtext></msub><annotation encoding="application/x-tex">E_{\text{prob}}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p">Only the extremes of HTDML have been explored thus far. The existing body of work on machine learning has <math alttext="E_{\text{tot}}=E_{\text{det}}" class="ltx_Math" display="inline" id="S5.p6.m1" intent=":literal"><semantics><mrow><msub><mi>E</mi><mtext>tot</mtext></msub><mo>=</mo><msub><mi>E</mi><mtext>det</mtext></msub></mrow><annotation encoding="application/x-tex">E_{\text{tot}}=E_{\text{det}}</annotation></semantics></math> and the early demonstrations in this work have <math alttext="E_{\text{tot}}=E_{\text{prob}}" class="ltx_Math" display="inline" id="S5.p6.m2" intent=":literal"><semantics><mrow><msub><mi>E</mi><mtext>tot</mtext></msub><mo>=</mo><msub><mi>E</mi><mtext>prob</mtext></msub></mrow><annotation encoding="application/x-tex">E_{\text{tot}}=E_{\text{prob}}</annotation></semantics></math>. Like many engineered systems, optimal solutions will be found somewhere in the middle, where the contributions from the various subsystems are nearly balanced <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib99" title="Validity of the single processor approach to achieving large scale computing capabilities">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib100" title="Roofline: an insightful visual performance model for multicore architectures">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib101" title="A power/area optimal approach to vlsi signal processing">47</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p7">
<p class="ltx_p">System designs between the extremes represent completely unexplored territory. Many foundational problems in HTDML still need to be solved.</p>
</div>
<div class="ltx_para" id="S5.p8">
<p class="ltx_p">For example, more rigorous methods of embedding data into hardware EBMs will need to be developed to go beyond the relatively simple datasets considered here. Indeed, binarization is not viable in general, and embedding into richer types of variables (such as categorical) at the probabilistic hardware level is not particularly efficient or principled.</p>
</div>
<div class="ltx_para" id="S5.p9">
<p class="ltx_p">One way to solve the embedding problem is to use a small neural network to map data into the probabilistic hardware. A naive experiment demonstrating this is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.F6" title="Figure 6 ‣ V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">6</span></a>. Here, we train a small neural network to embed the CIFAR-10 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib119" title="Learning multiple layers of features from tiny images">43</a>]</cite> into a binary DTM. The embedding network was trained using an autoencoder loss to binarize the data, which was then used to train a DTM. The decoder of the embedding network was then trained further using a GAN objective to increase the quality of the generated images. This training procedure is described in further detail in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A9" title="Appendix I Deterministic embeddings for DTMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<div class="ltx_para" id="S5.p10">
<p class="ltx_p">Despite the overhead of the embedding neural network, this primitive hybrid model is efficient. As shown in the figure, the generator of the traditional GAN has to be roughly 10 times larger than the decoder of our embedding network to match the performance of the hybrid model.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="386" id="S5.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold">Embedding data into a DTM using a neural network.</span> Here, we show the results of using a simple embedding model in combination with a DTM. The DTM is trained to generate CIFAR-10 images and achieves performance parity with a traditional GAN using a <math alttext="\sim 10\times" class="ltx_math_unparsed" display="inline" id="S5.F6.m2" intent=":literal"><semantics><mrow><mo>∼</mo><mn>10</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">\sim 10\times</annotation></semantics></math> smaller deterministic neural network.</figcaption>
</figure>
<div class="ltx_para" id="S5.p11">
<p class="ltx_p">The embedding procedure employed in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.F6" title="Figure 6 ‣ V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">6</span></a> will likely be significantly improved through further study. One major flaw with our method is that the autoencoder and DTM are not jointly trained, which means that the embedding learned by the autoencoder may not be well-suited to the way information can flow in the DTM, given its limited connectivity. The problem of using a denoising chain of EBMs in latent space has been studied in the deep learning literature, and some of this work may be leveraged to solve the embedding problem discussed here <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib80" title="Latent Diffusion Energy-Based Model for Interpretable Text Modelling">74</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p12">
<p class="ltx_p">The models used in this article are small compared to what could be implemented using even an early probabilistic computer based on the DTCA. Based on the size of our RNG, it can be estimated that <math alttext="\sim 10^{6}" class="ltx_Math" display="inline" id="S5.p12.m1" intent=":literal"><semantics><mrow><mi></mi><mo>∼</mo><msup><mn>10</mn><mn>6</mn></msup></mrow><annotation encoding="application/x-tex">\sim 10^{6}</annotation></semantics></math> sampling cells could be fit into a <math alttext="6\times 6\;\text{µm}" class="ltx_Math" display="inline" id="S5.p12.m2" intent=":literal"><semantics><mrow><mrow><mn>6</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>6</mn></mrow><mo lspace="0.280em" rspace="0em">​</mo><mtext>µm</mtext></mrow><annotation encoding="application/x-tex">6\times 6\;\text{µm}</annotation></semantics></math> chip (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A10" title="Appendix J Some details on our RNG ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">J</span></a>). In contrast, the largest DTM shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="Figure 1 ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a> would use only around <span class="ltx_text ltx_number">50,000</span> cells.</p>
</div>
<div class="ltx_para" id="S5.p13">
<p class="ltx_p">Given this gap between the size of our models and the capabilities of a potential hardware device, a natural question to study is how these probabilistic models can be scaled outside the obvious approaches considered here. This scaling likely corresponds to developing architectures that fuse multiple EBMs to implement each step in the reverse process. One possible approach is to construct software-defined graphical models of EBMs that enable non-local information routing, which could alleviate some of the issues associated with a fixed and local interaction structure.</p>
</div>
<div class="ltx_para" id="S5.p14">
<p class="ltx_p">One difficulty with HTDML research is that simulating large hardware EBMs on GPUs can be a challenging task. GPUs run these EBMs much less efficiently than probabilistic computers and the sparse data structures that naturally arise when working with hardware EBMs do not mesh well with regular tensor data types. We have both short and long-term solutions to these challenges.</p>
</div>
<div class="ltx_para" id="S5.p15">
<p class="ltx_p">To address these challenges in the short term, we have open-sourced a software library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib113" title="thrml: Thermodynamic Hypergraphical Model Library">24</a>]</cite> that enables XLA-accelerated <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib105" title="XLA : Compiling Machine Learning for Peak Performance">55</a>]</cite> simulation of hardware EBMs. This library is written in JAX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib106" title="JAX: composable transformations of Python+NumPy programs">11</a>]</cite> and automates the complex slicing operations that enable hardware EBM sampling. We also provide additional code that wraps this library to implement the specific experiments presented in this article <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib114" title="github.com/pschilliOrange/dtm-replication">29</a>]</cite>. In the longer term, the realization of large-scale probabilistic computers, such as the one proposed in this article, using advanced transistor processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib115" title="Calculated threshold-voltage characteristics of an XMOS transistor having an additional bottom gate">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib116" title="A 3nm CMOS FinFlex™ platform technology with enhanced power efficiency and performance for mobile SoC and high performance computing applications">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#bib.bib117" title="A Reliability Enhanced 5nm CMOS Technology Featuring 5th Generation FinFET with Fully-Developed EUV and High Mobility Channel for Mobile SoC and High Performance Computing Application">46</a>]</cite> will significantly alleviate the challenges associated with HTDML research and accelerate the pace of progress.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography ltx_list_bu1" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist" id="bib.L1">
<li class="ltx_bibitem ltx_bib_article" id="bib.bib97">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. A. Abeed and S. Bandyopadhyay</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Sensitivity of the Power Spectra of Thermal Magnetization Fluctuations in Low Barrier Nanomagnets Proposed for Stochastic Computing to In-Plane Barrier Height Variations and Structural Defects</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">SPIN</span> <span class="ltx_text ltx_bib_volume">10</span> (<span class="ltx_text ltx_bib_number">01</span>), <span class="ltx_text ltx_bib_pages"> pp. 2050001</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1142/S2010324720500010" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1142/S2010324720500010" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p8" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib96">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Md. A. Abeed and S. Bandyopadhyay</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Low Energy Barrier Nanomagnet Design for Binary Stochastic Neurons: Design Challenges for Real Nanomagnets With Fabrication Defects</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Magn. Lett.</span> <span class="ltx_text ltx_bib_volume">10</span> (<span class="ltx_text ltx_bib_number"></span>), <span class="ltx_text ltx_bib_pages"> pp. 1–5</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1109/LMAG.2019.2929484" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1109/LMAG.2019.2929484" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p8" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib62">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. H. Adachi and M. P. Henderson</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Application of Quantum Annealing to Training of Deep Neural Networks</em></span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.1510.06356" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p7" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib95">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Aljbour, T. Wilson, and P. Patel</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Powering Intelligence: Analyzing Artificial Intelligence and Data Center Energy Consumption</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">EPRI White Paper no. 3002028905</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://www.epri.com/research/products/000000003002028905" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p1" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib44">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Ambrogio, P. Narayanan, A. Okazaki, A. Fasoli, C. Mackin, K. Hosokawa, A. Nomura, T. Yasuda, A. Chen, A. Friz, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">An analog-AI chip for energy-efficient speech recognition and transcription</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nature</span> <span class="ltx_text ltx_bib_volume">620</span> (<span class="ltx_text ltx_bib_number">7975</span>), <span class="ltx_text ltx_bib_pages"> pp. 768–775</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1038/s41586-023-06337-5" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p4" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib99">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. M. Amdahl</span><span class="ltx_text ltx_bib_year"> (1967)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Validity of the single processor approach to achieving large scale computing capabilities</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the April 18-20, 1967, spring joint computer conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 483–485</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1145/1465482.1465560" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1145/1465482.1465560" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.p6" title="V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib43">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Bandyopadhyay, A. Sludds, S. Krastanov, R. Hamerly, N. Harris, D. Bunandar, M. Streshinsky, M. Hochberg, and D. Englund</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Single-chip photonic deep neural network with forward-only training</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nat. Photon.</span> <span class="ltx_text ltx_bib_volume">18</span> (<span class="ltx_text ltx_bib_number">12</span>), <span class="ltx_text ltx_bib_pages"> pp. 1335–1343</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1038/s41566-024-01567-z" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p4" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_report" id="bib.bib28">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Bick, A. Blandin, and D. J. Deming</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">The rapid adoption of generative ai</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Technical report</span>
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">National Bureau of Economic Research</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.3386/w32966" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p2" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib74">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. M. Bishop</span><span class="ltx_text ltx_bib_year"> (1994)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mixture density networks</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://api.semanticscholar.org/CorpusID:118227751" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.p3" title="I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib64">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. A. Borders, A. Z. Pervaiz, S. Fukami, K. Y. Camsari, H. Ohno, and S. Datta</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Integer factorization using stochastic magnetic tunnel junctions</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nature</span> <span class="ltx_text ltx_bib_volume">573</span> (<span class="ltx_text ltx_bib_number">7774</span>), <span class="ltx_text ltx_bib_pages"> pp. 390–393</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1038/s41586-019-1557-9" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1038/s41586-019-1557-9" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p7" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib106">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">JAX: composable transformations of Python+NumPy programs</em></span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="http://github.com/jax-ml/jax" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.p15" title="V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib30">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">E. Brynjolfsson, D. Li, and L. Raymond</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Generative AI at work</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Q. J. Econ.</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1093/qje/qjae044" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p2" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib75">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Carbone, M. Hua, S. Coste, and E. Vanden-Eijnden</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Efficient training of energy-based models using Jarzynski equality</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Adv. Neural Inf. Process. Syst.</span> <span class="ltx_text ltx_bib_volume">36</span>, <span class="ltx_text ltx_bib_pages"> pp. 52583–52614</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.48550/arXiv.2307.03577" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2307.03577" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.p6" title="I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib47">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. A. Carreira-Perpinan and G. Hinton</span><span class="ltx_text ltx_bib_year"> (2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">On contrastive divergence learning</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International workshop on artificial intelligence and statistics</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 33–40</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S4.p2" title="IV Training DTMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§IV</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib38">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Chellapilla, S. Puri, and P. Simard</span><span class="ltx_text ltx_bib_year"> (2006-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">High Performance Convolutional Neural Networks for Document Processing</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Tenth International Workshop on Frontiers in Handwriting Recognition</span>,  <span class="ltx_text ltx_bib_editor">G. Lorette (Ed.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">La Baule (France)</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://inria.hal.science/inria-00112631" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p2" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib59">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. T. Chen, X. Li, R. B. Grosse, and D. K. Duvenaud</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Isolating sources of disentanglement in variational autoencoders</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Adv. Neural Inf. Process. Syst.</span> <span class="ltx_text ltx_bib_volume">31</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.1802.04942" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S4.p8" title="IV Training DTMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§IV</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib32">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. A. Chien</span><span class="ltx_text ltx_bib_year"> (2023-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">GenAI: Giga$$$, TeraWatt-Hours, and GigaTons of CO2</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Commun. ACM</span> <span class="ltx_text ltx_bib_volume">66</span> (<span class="ltx_text ltx_bib_number">8</span>), <span class="ltx_text ltx_bib_pages"> pp. 5</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0001-0782</span>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1145/3606254" title="">Link</a>,
<a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1145/3606254" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p1" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib37">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Coates, B. Huval, T. Wang, D. J. Wu, A. Y. Ng, and B. Catanzaro</span><span class="ltx_text ltx_bib_year"> (2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep learning with cots hpc systems</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">ICML’13</span>, <span class="ltx_text ltx_bib_pages"> pp. III–1337–III–1345</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://dl.acm.org/doi/10.5555/3042817.3043086" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p2" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib51">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Conte, E. DeBenedictis, N. Ganesh, T. Hylton, J. P. Strachan, R. S. Williams, A. Alemi, L. Altenberg, G. Crooks, J. Crutchfield, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Thermodynamic computing</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv [cs.CY]</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.1911.01968" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p7" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib73">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. P. Dempster, N. M. Laird, and D. B. Rubin</span><span class="ltx_text ltx_bib_year"> (1977)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Maximum likelihood from incomplete data via the EM algorithm</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">J. R. Stat. Soc. Ser. B (Methodol.)</span> <span class="ltx_text ltx_bib_volume">39</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 1–22</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1111/j.2517-6161.1977.tb01614.x" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1111/j.2517-6161.1977.tb01614.x" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.p3" title="I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib76">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Desjardins, A. Courville, Y. Bengio, P. Vincent, O. Delalleau, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Parallel tempering for training of restricted Boltzmann machines</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the thirteenth international conference on artificial intelligence and statistics</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 145–152</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="http://proceedings.mlr.press/v9/desjardins10a.html" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.p6" title="I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib98">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. L. Drobitch and S. Bandyopadhyay</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Reliability and Scalability of p-Bits Implemented With Low Energy Barrier Nanomagnets</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Magn. Lett.</span> <span class="ltx_text ltx_bib_volume">10</span> (<span class="ltx_text ltx_bib_number"></span>), <span class="ltx_text ltx_bib_pages"> pp. 1–4</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1109/LMAG.2019.2956913" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1109/LMAG.2019.2956913" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p8" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib52">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Du and I. Mordatch</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Implicit generation and modeling with energy based models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,  <span class="ltx_text ltx_bib_editor">H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">32</span>, <span class="ltx_text ltx_bib_pages"> pp. </span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://proceedings.neurips.cc/paper_files/paper/2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p8" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib113">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Extropic</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">thrml: Thermodynamic Hypergraphical Model Library</em></span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://github.com/extropic-ai/thrml" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.p15" title="V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib66">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Faria, K. Y. Camsari, and S. Datta</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Low-Barrier Nanomagnets as p-Bits for Spin Logic</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Magn. Lett.</span> <span class="ltx_text ltx_bib_volume">8</span> (<span class="ltx_text ltx_bib_number"></span>), <span class="ltx_text ltx_bib_pages"> pp. 1–5</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1109/LMAG.2017.2685358" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1109/LMAG.2017.2685358" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p7" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib1">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Freitas, G. Massarelli, J. Rothschild, D. Keane, E. Dawe, S. Hwang, A. Garlapati, and T. McCourt</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Taming non-equilibrium thermal fluctuations in subthreshold CMOS circuits</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Phys. Rev. Lett.</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Submitted</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.p12" title="III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§III</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p11" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib69">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Gao, Y. Song, B. Poole, Y. N. Wu, and D. P. Kingma</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Learning Energy-Based Models by Diffusion Recovery Likelihood</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv [cs.LG]</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2012.08125" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.p3" title="II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§II</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.p8" title="II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§II</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib108">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Ghahramani</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Probabilistic machine learning and artificial intelligence</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nature</span> <span class="ltx_text ltx_bib_volume">521</span> (<span class="ltx_text ltx_bib_number">7553</span>), <span class="ltx_text ltx_bib_pages"> pp. 452–459</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1038/nature14541" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.p1" title="I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib114">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">github.com/pschilliOrange/dtm-replication</em></span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://github.com/pschilliOrange/dtm-replication" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.p15" title="V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib41">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. A. Gonzalez, J. Huang, F. Kelber, K. K. Nazeer, T. Langer, C. Liu, M. Lohrmann, A. Rostami, M. Schone, B. Vogginger, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">SpiNNaker2: A large-scale neuromorphic system for event-based and asynchronous machine learning</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv [cs.ET]</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2401.04491" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p4" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib18">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio</span><span class="ltx_text ltx_bib_year"> (2014)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generative adversarial nets</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,  <span class="ltx_text ltx_bib_editor">Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (Eds.)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">27</span>, <span class="ltx_text ltx_bib_pages"> pp. </span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib120">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">GANs trained by a two time-scale update rule converge to a local nash equilibrium</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,  <span class="ltx_text ltx_bib_editor">I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">30</span>, <span class="ltx_text ltx_bib_pages"> pp. </span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib22">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G.E. Hinton</span><span class="ltx_text ltx_bib_year"> (1984)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Boltzmann Machines: Constraint Satisfaction Networks that Learn</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Carnegie-Mellon University. Department of Computer Science</span>,  <span class="ltx_text ltx_bib_publisher">Carnegie-Mellon University, Department of Computer Science</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://books.google.ca/books?id=IniqXwAACAAJ" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.p1" title="I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_incollection" id="bib.bib58">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. E. Hinton</span><span class="ltx_text ltx_bib_year"> (2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">A practical guide to training restricted Boltzmann machines</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Neural Networks: Tricks of the Trade: Second Edition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 599–619</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S4.p4" title="IV Training DTMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§IV</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib70">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Ho, A. Jain, and P. Abbeel</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Denoising Diffusion Probabilistic Models</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">33</span>, <span class="ltx_text ltx_bib_pages"> pp. 6840–6851</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.48550/arXiv.2006.11239" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2006.11239" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p10" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib109">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Cascaded diffusion models for high fidelity image generation</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">J. Mach. Learn. Res.</span> <span class="ltx_text ltx_bib_volume">23</span> (<span class="ltx_text ltx_bib_number">47</span>), <span class="ltx_text ltx_bib_pages"> pp. 1–33</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.48550/arXiv.2106.15282" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2106.15282" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.p2" title="II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§II</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib39">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Hooker</span><span class="ltx_text ltx_bib_year"> (2021-11)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The hardware lottery</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Commun. ACM</span> <span class="ltx_text ltx_bib_volume">64</span> (<span class="ltx_text ltx_bib_number">12</span>), <span class="ltx_text ltx_bib_pages"> pp. 58–65</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0001-0782</span>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1145/3467017" title="">Link</a>,
<a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1145/3467017" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p3" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib93">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Horodynski, C. Roques-Carmes, Y. Salamin, S. Choi, J. Sloan, D. Luo, and M. Soljačić</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Stochastic logic in biased coupled photonic probabilistic bits</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Commun. Phys.</span> <span class="ltx_text ltx_bib_volume">8</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 31</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1038/s42005-024-01826-6" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1038/s42005-024-01826-6" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p8" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib54">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Janner, Y. Du, J. Tenenbaum, and S. Levine</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Planning with Diffusion for Flexible Behavior Synthesis</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Machine Learning</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 9902–9915</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2205.09991" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p6" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib107">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. I. Jordan and T. M. Mitchell</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Machine learning: Trends, perspectives, and prospects</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Science</span> <span class="ltx_text ltx_bib_volume">349</span> (<span class="ltx_text ltx_bib_number">6245</span>), <span class="ltx_text ltx_bib_pages"> pp. 255–260</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1126/science.aaa8415" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://www.science.org/doi/abs/10.1126/science.aaa8415" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.p1" title="I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib24">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. M. Katz, M. J. Bommarito, S. Gao, and P. Arredondo</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">GPT-4 passes the bar exam</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Philos. Trans. R. Soc. A</span> <span class="ltx_text ltx_bib_volume">382</span> (<span class="ltx_text ltx_bib_number">2270</span>), <span class="ltx_text ltx_bib_pages"> pp. 20230254</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1098/rsta.2023.0254" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2023.0254" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p2" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib20">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. P. Kingma and M. Welling</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Auto-Encoding Variational Bayes</em></span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/1312.6114" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_report" id="bib.bib119">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Krizhevsky and G. Hinton</span><span class="ltx_text ltx_bib_year"> (2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning multiple layers of features from tiny images</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Technical report</span>
</span>
<span class="ltx_bibblock">Technical Report <span class="ltx_text ltx_bib_number">0</span>,  <span class="ltx_text ltx_bib_publisher">Technical report, University of Toronto</span>,  <span class="ltx_text ltx_bib_publisher">University of Toronto</span>,  <span class="ltx_text ltx_bib_place">Toronto, Ontario</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://www.cs.toronto.edu/%CB%9Ckriz/learning-features-2009-TR.pdf" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.p9" title="V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib90">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. Lee, H. Kim, H. Jung, Y. Choi, J. Jeon, and C. Kim</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Correlation free large-scale probabilistic computing using a true-random chaotic oscillator p-bit</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Sci. Rep.</span> <span class="ltx_text ltx_bib_volume">15</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 8018</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1038/s41598-025-58784-y" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1038/s41598-025-58784-y" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p8" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib19">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy, C. de Masson d’Autume, I. Babuschkin, X. Chen, P. Huang, J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson, P. Kohli, N. de Freitas, K. Kavukcuoglu, and O. Vinyals</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Competition-level code generation with AlphaCode</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Science</span> <span class="ltx_text ltx_bib_volume">378</span> (<span class="ltx_text ltx_bib_number">6624</span>), <span class="ltx_text ltx_bib_pages"> pp. 1092–1097</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1126/science.abq1158" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://www.science.org/doi/abs/10.1126/science.abq1158" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p2" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib117">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J.C. Liu, S. Mukhopadhyay, A. Kundu, S.H. Chen, H.C. Wang, D.S. Huang, J.H. Lee, M.I. Wang, R. Lu, S.S. Lin, Y.M. Chen, H.L. Shang, P.W. Wang, H.C. Lin, G. Yeap, and J. He</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">A Reliability Enhanced 5nm CMOS Technology Featuring 5th Generation FinFET with Fully-Developed EUV and High Mobility Channel for Mobile SoC and High Performance Computing Application</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2020 IEEE International Electron Devices Meeting (IEDM)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume"></span>, <span class="ltx_text ltx_bib_pages"> pp. 9.2.1–9.2.4</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1109/IEDM13553.2020.9372009" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1109/IEDM13553.2020.9372009" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.p15" title="V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_thesis" id="bib.bib101">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. M. Markovic</span><span class="ltx_text ltx_bib_year"> (2006-05)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A power/area optimal approach to vlsi signal processing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Ph.D. Thesis</span>, <span class="ltx_text ltx_bib_publisher">EECS Department, University of California, Berkeley</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="http://www2.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-65.html" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.p6" title="V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib6">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. P. Murphy</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Probabilistic Machine Learning: Advanced Topics</em></span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">MIT Press</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="http://probml.github.io/book2" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S1.p5" title="I The Challenge with EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib67">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Niazi, S. Chowdhury, N. A. Aadit, M. Mohseni, Y. Qin, and K. Y. Camsari</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Training deep Boltzmann networks with sparse Ising machines</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nat. Electron.</span> <span class="ltx_text ltx_bib_volume">7</span> (<span class="ltx_text ltx_bib_number">7</span>), <span class="ltx_text ltx_bib_pages"> pp. 610–619</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1038/s41928-024-01188-7" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1038/s41928-024-01188-7" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S3.p2" title="III Denoising Thermodynamic Computers ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§III</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p7" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib25">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Capabilities of gpt-4 on medical challenge problems</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv [cs.CL]</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2303.13375" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p2" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib29">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Noy and W. Zhang</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Experimental evidence on the productivity effects of generative artificial intelligence</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Science</span> <span class="ltx_text ltx_bib_volume">381</span> (<span class="ltx_text ltx_bib_number">6654</span>), <span class="ltx_text ltx_bib_pages"> pp. 187–192</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1126/science.adh2586" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://www.science.org/doi/abs/10.1126/science.adh2586" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p2" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib31">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Peng, E. Kalliamvakou, P. Cihon, and M. Demirer</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">The impact of ai on developer productivity: Evidence from github copilot</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv [cs.SE]</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2302.06590" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p2" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib60">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Pratt, K. Ray, and J. Crutchfield</span><span class="ltx_text ltx_bib_year"> (2023-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Dynamical Computing on the Nanoscale: Superconducting Circuits for Thermodynamically-Efficient Classical Information Processing</em></span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2307.01926" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p7" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib110">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">High-resolution image synthesis with latent diffusion models</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 10684–10695</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.48550/arXiv.2112.10752" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2112.10752" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.p2" title="II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§II</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib105">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Sabne</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">XLA : Compiling Machine Learning for Peak Performance</em></span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.p15" title="V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib111">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Photorealistic text-to-image diffusion models with deep language understanding</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,  <span class="ltx_text ltx_bib_editor">S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">35</span>, <span class="ltx_text ltx_bib_pages"> pp. 36479–36494</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.p2" title="II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§II</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib15">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. M. H. Sajeeb, N. A. Aadit, S. Chowdhury, T. Wu, C. Smith, D. Chinmay, A. Raut, K. Y. Camsari, C. Delacour, and T. Srimani</span><span class="ltx_text ltx_bib_year"> (2025-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Scalable connectivity for ising machines: dense to sparse</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Phys. Rev. Appl.</span> <span class="ltx_text ltx_bib_volume">24</span>, <span class="ltx_text ltx_bib_pages"> pp. 014005</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1103/kx8m-5h3h" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://link.aps.org/doi/10.1103/kx8m-5h3h" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p7" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib115">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Sekigawa and Y. Hayashi</span><span class="ltx_text ltx_bib_year"> (1984)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Calculated threshold-voltage characteristics of an XMOS transistor having an additional bottom gate</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Solid-State Electron.</span> <span class="ltx_text ltx_bib_volume">27</span> (<span class="ltx_text ltx_bib_number">8-9</span>), <span class="ltx_text ltx_bib_pages"> pp. 827–828</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1016/0038-1101%2884%2990036-4" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1016/0038-1101(84)90036-4" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.p15" title="V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib40">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. B. Shrestha, J. Timcheck, P. Frady, L. Campos-Macias, and M. Davies</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Efficient Video and Audio Processing with Loihi 2</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume"></span>, <span class="ltx_text ltx_bib_pages"> pp. 13481–13485</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1109/ICASSP48485.2024.10448003" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1109/ICASSP48485.2024.10448003" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p4" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib65">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. S. Singh, K. Kobayashi, Q. Cao, K. Selcuk, T. Hu, S. Niazi, N. A. Aadit, S. Kanai, H. Ohno, S. Fukami, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">CMOS plus stochastic nanomagnets enabling heterogeneous computers for probabilistic inference and learning</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nat. Commun.</span> <span class="ltx_text ltx_bib_volume">15</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 2685</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1038/s41467-024-48358-z" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1038/s41467-024-48358-z" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p7" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p8" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib42">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. S. Singh, K. Kobayashi, Q. Cao, K. Selcuk, T. Hu, S. Niazi, N. A. Aadit, S. Kanai, H. Ohno, S. Fukami, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">CMOS plus stochastic nanomagnets enabling heterogeneous computers for probabilistic inference and learning</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nat. Commun.</span> <span class="ltx_text ltx_bib_volume">15</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 2685</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1038/s41467-024-48358-z" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1038/s41467-024-48358-z" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p7" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib68">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli</span><span class="ltx_text ltx_bib_year"> (2015-07–09 Jul)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 32nd International Conference on Machine Learning</span>,  <span class="ltx_text ltx_bib_editor">F. Bach and D. Blei (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Proceedings of Machine Learning Research</span>, Vol. <span class="ltx_text ltx_bib_volume">37</span>, <span class="ltx_text ltx_bib_place">Lille, France</span>, <span class="ltx_text ltx_bib_pages"> pp. 2256–2265</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://proceedings.mlr.press/v37/sohl-dickstein15.html" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.p2" title="II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§II</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p10" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib53">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Song and S. Ermon</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generative modeling by estimating gradients of the data distribution</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,  <span class="ltx_text ltx_bib_editor">H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">32</span>, <span class="ltx_text ltx_bib_pages"> pp. </span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p6" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib57">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Song and D. P. Kingma</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">How to train your energy-based models</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv [cs.LG]</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2101.03288" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S4.p1" title="IV Training DTMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§IV</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_report" id="bib.bib33">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. D. Stine</span><span class="ltx_text ltx_bib_year"> (2009-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The manhattan project, the apollo program, and federal energy technology r&amp;d programs: a comparative analysis</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Report</span>
</span>
<span class="ltx_bibblock">Technical Report <span class="ltx_text ltx_bib_number">RL34645</span>,  <span class="ltx_text ltx_bib_publisher">Congressional Research Service</span>,  <span class="ltx_text ltx_bib_place">Washington, D.C.</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p1" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib45">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Sun, N. B. Agostini, S. Dong, and D. Kaeli</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Summarizing CPU and GPU design trends with product data</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv [cs.DC]</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.1911.11313" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p5" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib63">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Sutton, K. Y. Camsari, B. Behin-Aein, and S. Datta</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Intrinsic optimization using stochastic nanomagnets</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Sci. Rep.</span> <span class="ltx_text ltx_bib_volume">7</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 44370</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1038/s41598-017-44754-3" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1038/s41598-017-44754-3" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p7" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib35">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Attention is all you need</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,  <span class="ltx_text ltx_bib_editor">I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">30</span>, <span class="ltx_text ltx_bib_pages"> pp. </span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p2" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib100">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Williams, A. Waterman, and D. Patterson</span><span class="ltx_text ltx_bib_year"> (2009-04)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Roofline: an insightful visual performance model for multicore architectures</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Commun. ACM</span> <span class="ltx_text ltx_bib_volume">52</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 65–76</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0001-0782</span>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1145/1498765.1498785" title="">Link</a>,
<a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1145/1498765.1498785" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.p6" title="V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib61">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Wimsatt, O. Saira, A. B. Boyd, M. H. Matheny, S. Han, M. L. Roukes, and J. P. Crutchfield</span><span class="ltx_text ltx_bib_year"> (2021-08)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Harnessing fluctuations in thermodynamic computing via time-reversal symmetries</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Phys. Rev. Res.</span> <span class="ltx_text ltx_bib_volume">3</span>, <span class="ltx_text ltx_bib_pages"> pp. 033115</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1103/PhysRevResearch.3.033115" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://link.aps.org/doi/10.1103/PhysRevResearch.3.033115" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#p7" title="An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_title">An efficient probabilistic hardware architecture for diffusion-like models</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib116">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Wu, C. Chang, M. Chiang, C. Lin, J. Liaw, J. Cheng, J. Yeh, H. Chen, S. Chang, K. Lai, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">A 3nm CMOS FinFlex™ platform technology with enhanced power efficiency and performance for mobile SoC and high performance computing applications</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2022 International Electron Devices Meeting (IEDM)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 27–5</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1109/IEDM45625.2022.10019558" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1109/IEDM45625.2022.10019558" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.p15" title="V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib118">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Xiao, K. Rasul, and R. Vollgraf</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv [cs.LG]</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/1708.07747" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S0.F1" title="In An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib81">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Xu, T. Geffner, K. Kreis, W. Nie, Y. Xu, J. Leskovec, S. Ermon, and A. Vahdat</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Energy-based diffusion language models for text generation</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv [cs.CL]</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2410.21357" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.p3" title="II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§II</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib80">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Yu, S. Xie, X. Ma, B. Jia, B. Pang, R. Gao, Y. Zhu, S. Zhu, and Y. N. Wu</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Latent Diffusion Energy-Based Model for Interpretable Text Modelling</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Machine Learning</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 25702–25720</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://proceedings.mlr.press/v162/yu22e.html" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.p3" title="II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§II</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S5.p11" title="V Conclusion: Scaling Thermodynamic Machine Learning ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib82">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Zhu, J. Xie, Y. N. Wu, and R. Gao</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">The Twelfth International Conference on Learning Representations</span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2212.00168" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#S2.p3" title="II Denoising thermodynamic models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§II</span></a>.
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Denoising Diffusion Models </h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p">Denoising diffusion models try to learn to time-reverse a random process that converts data into simple noise. Here, we will review some details on how these models work to support the analysis in the main text.</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Forward Processes</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p">The forward process is a random process that is used to convert the data distribution into noise. This conversion into noise is achieved through a stochastic differential equation in the continuous-variable case and a Markov jump process in the discrete case.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.1 </span>Continuous Variables</h4>
<div class="ltx_para" id="A1.SS1.SSS1.p1">
<p class="ltx_p">In the continuous case, the typical choice of forward process is the Itô diffusion,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E20">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="dX(t)=-X(t)dt+\sqrt{2}\sigma dW" class="ltx_Math" display="block" id="A1.E20.m1" intent=":literal"><semantics><mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>−</mo><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow><mo>+</mo><mrow><msqrt><mn>2</mn></msqrt><mo lspace="0em" rspace="0em">​</mo><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>W</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">dX(t)=-X(t)dt+\sqrt{2}\sigma dW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(20)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="X(t)" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.m1" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(t)</annotation></semantics></math> is a length <math alttext="N" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.m2" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> vector representing the state variable at time <math alttext="t" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.m3" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, <math alttext="\sigma" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.m4" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> is a constant, and <math alttext="dW" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.m5" intent=":literal"><semantics><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">dW</annotation></semantics></math> is a length <math alttext="N" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.m6" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> vector of independent Wiener processes.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS1.p2">
<p class="ltx_p">The transition kernel for a random process defines how the probability distribution evolves in time,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E21">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q_{t|0}(x^{\prime}|x)=\mathbb{P}(X(t)=x^{\prime}|X(0)=x)" class="ltx_Math" display="block" id="A1.E21.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Q</mi><mrow><mi>t</mi><mo fence="false">|</mo><mn>0</mn></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mo>′</mo></msup><mo fence="false">|</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi mathvariant="normal">ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>x</mi><mo>′</mo></msup><mo fence="false">|</mo><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">Q_{t|0}(x^{\prime}|x)=\mathbb{P}(X(t)=x^{\prime}|X(0)=x)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(21)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">For the case of Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E20" title="Equation 20 ‣ A.1.1 Continuous Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">20</span></a>) the transition kernel is,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E22">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q_{t+s|s}(x^{\prime}|x)\propto e^{-\frac{1}{2}(x^{\prime}-\mu)^{T}\Sigma^{-1}(x^{\prime}-\mu)}" class="ltx_Math" display="block" id="A1.E22.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Q</mi><mrow><mi>t</mi><mo>+</mo><mrow><mi>s</mi><mo fence="false">|</mo><mi>s</mi></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mo>′</mo></msup><mo fence="false">|</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><msup><mi>e</mi><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mo>′</mo></msup><mo>−</mo><mi>μ</mi></mrow><mo stretchy="false">)</mo></mrow><mi>T</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi mathvariant="normal">Σ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mo>′</mo></msup><mo>−</mo><mi>μ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></msup></mrow><annotation encoding="application/x-tex">Q_{t+s|s}(x^{\prime}|x)\propto e^{-\frac{1}{2}(x^{\prime}-\mu)^{T}\Sigma^{-1}(x^{\prime}-\mu)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(22)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="A1.E23">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mu=e^{-t}x" class="ltx_Math" display="block" id="A1.E23.m1" intent=":literal"><semantics><mrow><mi>μ</mi><mo>=</mo><mrow><msup><mi>e</mi><mrow><mo>−</mo><mi>t</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi>x</mi></mrow></mrow><annotation encoding="application/x-tex">\mu=e^{-t}x</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(23)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="A1.E24">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Sigma=\sigma^{2}I\left(1-e^{-2t}\right)" class="ltx_Math" display="block" id="A1.E24.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Σ</mi><mo>=</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>I</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow></msup></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\Sigma=\sigma^{2}I\left(1-e^{-2t}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(24)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">this solution can be verified by direct substitution into the corresponding Fokker-Planck equation. In the limit of infinite time, <math alttext="\mu\to 0" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p2.m1" intent=":literal"><semantics><mrow><mi>μ</mi><mo stretchy="false">→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu\to 0</annotation></semantics></math> and <math alttext="\Sigma\to\sigma^{2}I" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p2.m2" intent=":literal"><semantics><mrow><mi mathvariant="normal">Σ</mi><mo stretchy="false">→</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>I</mi></mrow></mrow><annotation encoding="application/x-tex">\Sigma\to\sigma^{2}I</annotation></semantics></math>. Therefore, the stationary distribution of this process is zero-mean Gaussian noise with a standard deviation of <math alttext="\sigma" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p2.m3" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.2 </span>Discrete Variables</h4>
<div class="ltx_para" id="A1.SS1.SSS2.p1">
<p class="ltx_p">The stochastic dynamics of some discrete variable <math alttext="X" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p1.m1" intent=":literal"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> may be described by the Markov jump process,</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS2.p2">
<table class="ltx_equation ltx_eqn_table" id="A1.E25">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{dQ_{t}}{dt}=\mathcal{L}Q_{t}" class="ltx_Math" display="block" id="A1.E25.m1" intent=":literal"><semantics><mrow><mfrac><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>Q</mi><mi>t</mi></msub></mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mfrac><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>Q</mi><mi>t</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\frac{dQ_{t}}{dt}=\mathcal{L}Q_{t}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(25)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p2.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math> is the generator of the dynamics, which is an <math alttext="M\times M" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p2.m2" intent=":literal"><semantics><mrow><mi>M</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">M\times M</annotation></semantics></math> matrix that stores the transition rates between the various states. <math alttext="Q_{t}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p2.m3" intent=":literal"><semantics><msub><mi>Q</mi><mi>t</mi></msub><annotation encoding="application/x-tex">Q_{t}</annotation></semantics></math> is a length <math alttext="M" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p2.m4" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> vector that assigns a probability to each possible state <math alttext="X" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p2.m5" intent=":literal"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> may take at time <math alttext="t" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p2.m6" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS2.p3">
<p class="ltx_p">The transition rate from the <math alttext="i^{\text{th}}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p3.m1" intent=":literal"><semantics><msup><mi>i</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">i^{\text{th}}</annotation></semantics></math> state to the <math alttext="j^{\text{th}}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p3.m2" intent=":literal"><semantics><msup><mi>j</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">j^{\text{th}}</annotation></semantics></math> state is given by the matrix element <math alttext="\mathcal{L}\left[j,i\right]" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p3.m3" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo>]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}\left[j,i\right]</annotation></semantics></math>, which here takes the particular form,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E26">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}\left[j,i\right]=\gamma\left(-(M-1)\delta_{j,i}+(1-\delta_{j,i})\right)" class="ltx_Math" display="block" id="A1.E26.m1" intent=":literal"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><mo>−</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>M</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>δ</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></mrow><mo>+</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>δ</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}\left[j,i\right]=\gamma\left(-(M-1)\delta_{j,i}+(1-\delta_{j,i})\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(26)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\delta" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p3.m4" intent=":literal"><semantics><mi>δ</mi><annotation encoding="application/x-tex">\delta</annotation></semantics></math> is used to indicate the Kronecker delta function. Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E26" title="Equation 26 ‣ A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">26</span></a>) describes a random process where the probability per unit time to jump between any two states is <math alttext="\gamma" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p3.m5" intent=":literal"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS2.p4">
<p class="ltx_p">Since Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E25" title="Equation 25 ‣ A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">25</span></a>) is linear, the dynamics of <math alttext="Q_{t}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p4.m1" intent=":literal"><semantics><msub><mi>Q</mi><mi>t</mi></msub><annotation encoding="application/x-tex">Q_{t}</annotation></semantics></math> can be understood entirely via the eigenvalues and eigenvectors of <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p4.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E27">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}v_{k}=\lambda_{k}v_{k}" class="ltx_Math" display="block" id="A1.E27.m1" intent=":literal"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>v</mi><mi>k</mi></msub></mrow><mo>=</mo><mrow><msub><mi>λ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>v</mi><mi>k</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}v_{k}=\lambda_{k}v_{k}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(27)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Note that by symmetry of <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p4.m3" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math>, we do not distinguish between right and left eigenvectors.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS2.p5">
<p class="ltx_p">One eigenvector-eigenvalue pair <math alttext="\left(v_{0},\lambda_{0}=0\right)" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p5.m1" intent=":literal"><semantics><mrow><mo>(</mo><mrow><mrow><msub><mi>v</mi><mn>0</mn></msub><mo>,</mo><msub><mi>λ</mi><mn>0</mn></msub></mrow><mo>=</mo><mn>0</mn></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">\left(v_{0},\lambda_{0}=0\right)</annotation></semantics></math> corresponds to the unique stationary state of <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p5.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math>, with all entries of <math alttext="v_{0}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p5.m3" intent=":literal"><semantics><msub><mi>v</mi><mn>0</mn></msub><annotation encoding="application/x-tex">v_{0}</annotation></semantics></math> being equal to some constant (if normalized, then <math alttext="v_{0}[j]=\frac{1}{M}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p5.m4" intent=":literal"><semantics><mrow><mrow><msub><mi>v</mi><mn>0</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mi>M</mi></mfrac></mrow><annotation encoding="application/x-tex">v_{0}[j]=\frac{1}{M}</annotation></semantics></math> for all <math alttext="j" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p5.m5" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>).
The long-time dynamics of this MJP transform any initial distribution to a uniform distribution over all states.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS2.p6">
<p class="ltx_p">The remaining eigenvectors are decaying modes associated with negative eigenvalues. These additional <math alttext="M-1" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p6.m1" intent=":literal"><semantics><mrow><mi>M</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">M-1</annotation></semantics></math> eigenvectors take the form,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E28">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="v_{j}[i]=-\delta_{i,0}+\delta_{i,j}" class="ltx_Math" display="block" id="A1.E28.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>v</mi><mi>j</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>−</mo><msub><mi>δ</mi><mrow><mi>i</mi><mo>,</mo><mn>0</mn></mrow></msub></mrow><mo>+</mo><msub><mi>δ</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">v_{j}[i]=-\delta_{i,0}+\delta_{i,j}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(28)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="A1.E29">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\lambda_{j}=-\gamma M" class="ltx_Math" display="block" id="A1.E29.m1" intent=":literal"><semantics><mrow><msub><mi>λ</mi><mi>j</mi></msub><mo>=</mo><mrow><mo>−</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mi>M</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\lambda_{j}=-\gamma M</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(29)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E28" title="Equation 28 ‣ A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">28</span></a>) and Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E29" title="Equation 29 ‣ A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">29</span></a>) are valid for <math alttext="j\in[1,M-1]" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p6.m2" intent=":literal"><semantics><mrow><mi>j</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mrow><mi>M</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">j\in[1,M-1]</annotation></semantics></math>. Therefore, all solutions to this MJP decay exponentially to the uniform distribution with rate <math alttext="\gamma M" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p6.m3" intent=":literal"><semantics><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">\gamma M</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS2.p7">
<p class="ltx_p">The time-evolution of <math alttext="Q" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p7.m1" intent=":literal"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> is given by the matrix exponential,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E30">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q_{t}=e^{\mathcal{L}t}Q_{0}." class="ltx_Math" display="block" id="A1.E30.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Q</mi><mi>t</mi></msub><mo>=</mo><mrow><msup><mi>e</mi><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>Q</mi><mn>0</mn></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">Q_{t}=e^{\mathcal{L}t}Q_{0}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(30)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A1.SS1.SSS2.p8">
<p class="ltx_p">This matrix exponential is evaluated by diagonalizing <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p8.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E31">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="e^{\mathcal{L}t}=Pe^{Dt}P^{-1}" class="ltx_Math" display="block" id="A1.E31.m1" intent=":literal"><semantics><mrow><msup><mi>e</mi><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>P</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">e^{\mathcal{L}t}=Pe^{Dt}P^{-1}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(31)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the columns of <math alttext="P" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p8.m2" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> are the <math alttext="M" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p8.m3" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> eigenvectors <math alttext="v_{k}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p8.m4" intent=":literal"><semantics><msub><mi>v</mi><mi>k</mi></msub><annotation encoding="application/x-tex">v_{k}</annotation></semantics></math> and <math alttext="D" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p8.m5" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> is a diagonal matrix of the eigenvalues <math alttext="\lambda_{k}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p8.m6" intent=":literal"><semantics><msub><mi>λ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\lambda_{k}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS2.p9">
<p class="ltx_p">Using the solution for the eigenvalues and eigenvectors found above, we can solve for the matrix elements of <math alttext="e^{\mathcal{L}t}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p9.m1" intent=":literal"><semantics><msup><mi>e</mi><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><annotation encoding="application/x-tex">e^{\mathcal{L}t}</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E32">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="e^{\mathcal{L}t}\left[j,i\right]=\delta_{i,j}\left(\frac{1+(M-1)e^{-\gamma Mt}}{M}\right)+(1-\delta_{i,j})\left(\frac{1-e^{-\gamma Mt}}{M}\right)" class="ltx_Math" display="block" id="A1.E32.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>e</mi><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>δ</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mfrac><mrow><mn>1</mn><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>M</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mo>−</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mi>M</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow></msup></mrow></mrow><mi>M</mi></mfrac><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>δ</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mfrac><mrow><mn>1</mn><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mi>M</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow></msup></mrow><mi>M</mi></mfrac><mo>)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">e^{\mathcal{L}t}\left[j,i\right]=\delta_{i,j}\left(\frac{1+(M-1)e^{-\gamma Mt}}{M}\right)+(1-\delta_{i,j})\left(\frac{1-e^{-\gamma Mt}}{M}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(32)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A1.SS1.SSS2.p10">
<p class="ltx_p">Using this solution, we can deduce an exponential form for the matrix elements of <math alttext="e^{\mathcal{L}t}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p10.m1" intent=":literal"><semantics><msup><mi>e</mi><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><annotation encoding="application/x-tex">e^{\mathcal{L}t}</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E33">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="e^{\mathcal{L}t}\left[j,i\right]=\frac{1}{Z(t)}e^{\Gamma(t)\delta_{i,j}}" class="ltx_Math" display="block" id="A1.E33.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>e</mi><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mi mathvariant="normal">Γ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>δ</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">e^{\mathcal{L}t}\left[j,i\right]=\frac{1}{Z(t)}e^{\Gamma(t)\delta_{i,j}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(33)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="A1.E34">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Gamma(t)=\ln{\left(\frac{1+(M-1)e^{-\gamma t}}{1-e^{-\gamma t}}\right)}" class="ltx_Math" display="block" id="A1.E34.m1" intent=":literal"><semantics><mrow><mrow><mi mathvariant="normal">Γ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>ln</mi><mo>⁡</mo><mrow><mo>(</mo><mfrac><mrow><mn>1</mn><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>M</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mo>−</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow></msup></mrow></mrow><mrow><mn>1</mn><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow></msup></mrow></mfrac><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\Gamma(t)=\ln{\left(\frac{1+(M-1)e^{-\gamma t}}{1-e^{-\gamma t}}\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(34)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="A1.E35">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Z(t)=\frac{M}{1-e^{-\gamma t}}" class="ltx_Math" display="block" id="A1.E35.m1" intent=":literal"><semantics><mrow><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mi>M</mi><mrow><mn>1</mn><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">Z(t)=\frac{M}{1-e^{-\gamma t}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(35)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A1.SS1.SSS2.p11">
<p class="ltx_p">Now consider a process in which each element of the vector of <math alttext="N" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p11.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> discrete variables <math alttext="X" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p11.m2" intent=":literal"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> undergoes the dynamics described by Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E25" title="Equation 25 ‣ A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">25</span></a>) independently. In that case, the differential equation describing the dynamics of the joint distribution <math alttext="Q_{t}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p11.m3" intent=":literal"><semantics><msub><mi>Q</mi><mi>t</mi></msub><annotation encoding="application/x-tex">Q_{t}</annotation></semantics></math> is,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E36">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{dQ_{t}}{dt}=\sum_{k=1}^{N}\left(I_{1}\otimes\dots\otimes\mathcal{L}_{k}\otimes\dots I_{N}\right)Q_{t}" class="ltx_Math" display="block" id="A1.E36.m1" intent=":literal"><semantics><mrow><mfrac><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>Q</mi><mi>t</mi></msub></mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mfrac><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mrow><mo>(</mo><mrow><mrow><msub><mi>I</mi><mn>1</mn></msub><mo lspace="0.222em" rspace="0.222em">⊗</mo><mi mathvariant="normal">⋯</mi><mo lspace="0.222em" rspace="0.222em">⊗</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>k</mi></msub><mo lspace="0.222em" rspace="0.222em">⊗</mo><mi mathvariant="normal">…</mi></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>I</mi><mi>N</mi></msub></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>Q</mi><mi>t</mi></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\frac{dQ_{t}}{dt}=\sum_{k=1}^{N}\left(I_{1}\otimes\dots\otimes\mathcal{L}_{k}\otimes\dots I_{N}\right)Q_{t}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(36)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="I_{j}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p11.m4" intent=":literal"><semantics><msub><mi>I</mi><mi>j</mi></msub><annotation encoding="application/x-tex">I_{j}</annotation></semantics></math> indicates the identity operator and <math alttext="\mathcal{L}_{j}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p11.m5" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{j}</annotation></semantics></math> the operator from Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E26" title="Equation 26 ‣ A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">26</span></a>) acting on the subspace of the <math alttext="j^{\text{th}}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p11.m6" intent=":literal"><semantics><msup><mi>j</mi><mtext>th</mtext></msup><annotation encoding="application/x-tex">j^{\text{th}}</annotation></semantics></math> discrete variable.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS2.p12">
<p class="ltx_p">The Kronecker product of the matrix exponentials gives the time-evolution of the joint distribution,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E37">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="e^{\mathcal{L}t}=\bigotimes_{k=1}^{N}e^{\mathcal{L}_{k}t}" class="ltx_Math" display="block" id="A1.E37.m1" intent=":literal"><semantics><mrow><msup><mi>e</mi><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">⨂</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mi>e</mi><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">e^{\mathcal{L}t}=\bigotimes_{k=1}^{N}e^{\mathcal{L}_{k}t}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(37)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">with the matrix elements,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E38">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="e^{\mathcal{L}t}[j,i]=\prod_{k=1}^{N}e^{\mathcal{L}_{k}t}[j_{k},i_{k}]" class="ltx_Math" display="block" id="A1.E38.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>e</mi><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo stretchy="false">]</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∏</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msup><mi>e</mi><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msub><mi>j</mi><mi>k</mi></msub><mo>,</mo><msub><mi>i</mi><mi>k</mi></msub><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">e^{\mathcal{L}t}[j,i]=\prod_{k=1}^{N}e^{\mathcal{L}_{k}t}[j_{k},i_{k}]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(38)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="j" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p12.m1" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> and <math alttext="i" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p12.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> are now vectors with <math alttext="N" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p12.m3" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> elements, indexed as <math alttext="i_{k}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p12.m4" intent=":literal"><semantics><msub><mi>i</mi><mi>k</mi></msub><annotation encoding="application/x-tex">i_{k}</annotation></semantics></math> or <math alttext="j_{k}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p12.m5" intent=":literal"><semantics><msub><mi>j</mi><mi>k</mi></msub><annotation encoding="application/x-tex">j_{k}</annotation></semantics></math> respectively.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS2.p13">
<p class="ltx_p">Using Eqs. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E33" title="Equation 33 ‣ A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">33</span></a>) - (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E35" title="Equation 35 ‣ A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">35</span></a>), we can find an exponential form for the joint process transition kernel (as defined in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E21" title="Equation 21 ‣ A.1.1 Continuous Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">21</span></a>)),</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E39">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q_{t|0}(x^{\prime}|x)=\frac{1}{Z(t)}\exp{\left(\sum_{k=1}^{N}\Gamma_{k}(t)\delta_{x^{\prime}[k],x[k]}\right)}" class="ltx_Math" display="block" id="A1.E39.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Q</mi><mrow><mi>t</mi><mo fence="false">|</mo><mn>0</mn></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mo>′</mo></msup><mo fence="false">|</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><munderover><mo lspace="0em" movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi mathvariant="normal">Γ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>δ</mi><mrow><mrow><msup><mi>x</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">Q_{t|0}(x^{\prime}|x)=\frac{1}{Z(t)}\exp{\left(\sum_{k=1}^{N}\Gamma_{k}(t)\delta_{x^{\prime}[k],x[k]}\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(39)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="A1.E40">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Z(t)=\prod_{k=1}^{N}Z_{k}(t)" class="ltx_Math" display="block" id="A1.E40.m1" intent=":literal"><semantics><mrow><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∏</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>Z</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">Z(t)=\prod_{k=1}^{N}Z_{k}(t)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(40)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><math alttext="\Gamma_{k}(t)" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p13.m1" intent=":literal"><semantics><mrow><msub><mi mathvariant="normal">Γ</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Gamma_{k}(t)</annotation></semantics></math> and <math alttext="Z_{k}(t)" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p13.m2" intent=":literal"><semantics><mrow><msub><mi>Z</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Z_{k}(t)</annotation></semantics></math> are as given in Eqs. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E34" title="Equation 34 ‣ A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">34</span></a>) and (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E35" title="Equation 35 ‣ A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">35</span></a>), with each dimension potentially having it’s own transition rate <math alttext="\gamma_{k}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p13.m3" intent=":literal"><semantics><msub><mi>γ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\gamma_{k}</annotation></semantics></math> and number of categories <math alttext="M_{k}" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p13.m4" intent=":literal"><semantics><msub><mi>M</mi><mi>k</mi></msub><annotation encoding="application/x-tex">M_{k}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Reverse Processes</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p">In general, a random process for some variable <math alttext="X" class="ltx_Math" display="inline" id="A1.SS2.p1.m1" intent=":literal"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> can be reversed using Bayes’ rule,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E41">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q_{t|t+\Delta t}(x^{\prime}|x)=\frac{Q_{t+\Delta t|t}(x|x^{\prime})Q_{t}(x^{\prime})}{Q_{t+\Delta t}(x)}" class="ltx_Math" display="block" id="A1.E41.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Q</mi><mrow><mi>t</mi><mo fence="false">|</mo><mrow><mi>t</mi><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mo>′</mo></msup><mo fence="false">|</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mi>Q</mi><mrow><mi>t</mi><mo>+</mo><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow><mo fence="false">|</mo><mi>t</mi></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo fence="false">|</mo><msup><mi>x</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>Q</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>Q</mi><mrow><mi>t</mi><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">Q_{t|t+\Delta t}(x^{\prime}|x)=\frac{Q_{t+\Delta t|t}(x|x^{\prime})Q_{t}(x^{\prime})}{Q_{t+\Delta t}(x)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(41)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the conditionals are as defined in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E21" title="Equation 21 ‣ A.1.1 Continuous Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">21</span></a>), and the marginals are,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E42">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q_{t}(x)=\mathbb{P}\left(X(t)=x\right)" class="ltx_Math" display="block" id="A1.E42.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Q</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi mathvariant="normal">ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi>x</mi></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">Q_{t}(x)=\mathbb{P}\left(X(t)=x\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(42)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p">A differential equation that describes the reverse process can be found by analyzing Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E41" title="Equation 41 ‣ A.2 Reverse Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">41</span></a>) in the infinitesimal time limit. Specifically, defining a reversed time <math alttext="t=T-s" class="ltx_Math" display="inline" id="A1.SS2.p2.m1" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mi>T</mi><mo>−</mo><mi>s</mi></mrow></mrow><annotation encoding="application/x-tex">t=T-s</annotation></semantics></math> given some arbitrary endpoint <math alttext="T" class="ltx_Math" display="inline" id="A1.SS2.p2.m2" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> and expanding Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E41" title="Equation 41 ‣ A.2 Reverse Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">41</span></a>) in <math alttext="\Delta s" class="ltx_Math" display="inline" id="A1.SS2.p2.m3" intent=":literal"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow><annotation encoding="application/x-tex">\Delta s</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E43">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q_{T-s|T-(s-\Delta s)}(x^{\prime}|x)\approx\delta_{x,x^{\prime}}+\Delta s\&gt;\mathcal{L}_{\text{rev}}(x^{\prime},x)" class="ltx_Math" display="block" id="A1.E43.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Q</mi><mrow><mi>T</mi><mo>−</mo><mrow><mi>s</mi><mo fence="false">|</mo><mrow><mi>T</mi><mo>−</mo><mrow><mo stretchy="false">(</mo><mrow><mi>s</mi><mo>−</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mo>′</mo></msup><mo fence="false">|</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><msub><mi>δ</mi><mrow><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>′</mo></msup></mrow></msub><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi><mo lspace="0.220em" rspace="0em">​</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>rev</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mo>′</mo></msup><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">Q_{T-s|T-(s-\Delta s)}(x^{\prime}|x)\approx\delta_{x,x^{\prime}}+\Delta s\&gt;\mathcal{L}_{\text{rev}}(x^{\prime},x)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(43)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathcal{L}_{\text{rev}}" class="ltx_Math" display="inline" id="A1.SS2.p2.m4" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>rev</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{rev}}</annotation></semantics></math> is the generator of the reverse process,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E44">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{rev}}(x^{\prime},x)=\frac{Q_{T-s}(x^{\prime})}{Q_{T-s}(x)}\lim_{\Delta s\to 0}\left[\frac{d}{d\Delta s}Q_{T-(s-\Delta s)|T-s}(x|x^{\prime})\right]+\delta_{x,x^{\prime}}\left(\frac{1}{Q_{T-s}(x)}\frac{dQ_{T-s}(x)}{ds}\right)" class="ltx_Math" display="block" id="A1.E44.m1" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>rev</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mo>′</mo></msup><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mfrac><mrow><msub><mi>Q</mi><mrow><mi>T</mi><mo>−</mo><mi>s</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>Q</mi><mrow><mi>T</mi><mo>−</mo><mi>s</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mrow><munder><mo movablelimits="false" rspace="0em">lim</mo><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow><mo stretchy="false">→</mo><mn>0</mn></mrow></munder><mrow><mo>[</mo><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>Q</mi><mrow><mi>T</mi><mo>−</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>s</mi><mo>−</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo fence="false">|</mo><mrow><mi>T</mi><mo>−</mo><mi>s</mi></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo fence="false">|</mo><msup><mi>x</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>δ</mi><mrow><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>′</mo></msup></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><msub><mi>Q</mi><mrow><mi>T</mi><mo>−</mo><mi>s</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mfrac><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>Q</mi><mrow><mi>T</mi><mo>−</mo><mi>s</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{rev}}(x^{\prime},x)=\frac{Q_{T-s}(x^{\prime})}{Q_{T-s}(x)}\lim_{\Delta s\to 0}\left[\frac{d}{d\Delta s}Q_{T-(s-\Delta s)|T-s}(x|x^{\prime})\right]+\delta_{x,x^{\prime}}\left(\frac{1}{Q_{T-s}(x)}\frac{dQ_{T-s}(x)}{ds}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(44)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">here, <math alttext="\delta_{x,x^{\prime}}" class="ltx_Math" display="inline" id="A1.SS2.p2.m5" intent=":literal"><semantics><msub><mi>δ</mi><mrow><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>′</mo></msup></mrow></msub><annotation encoding="application/x-tex">\delta_{x,x^{\prime}}</annotation></semantics></math> is used to indicate the Dirac delta function in the continuous case and the Kronecker delta in the discrete case.</p>
</div>
<div class="ltx_para" id="A1.SS2.p3">
<p class="ltx_p">If the dynamics of Q are linear and generated by <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="A1.SS2.p3.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math> like Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E25" title="Equation 25 ‣ A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">25</span></a>), we can simplify,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E45">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\lim_{\Delta s\to 0}\left[\frac{d}{d\Delta s}Q_{T-(s-\Delta s)|T-s}(x|x^{\prime})\right]=\mathcal{L}(x,x^{\prime})" class="ltx_Math" display="block" id="A1.E45.m1" intent=":literal"><semantics><mrow><mrow><munder><mo movablelimits="false">lim</mo><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow><mo stretchy="false">→</mo><mn>0</mn></mrow></munder><mrow><mo lspace="0em">[</mo><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>Q</mi><mrow><mi>T</mi><mo>−</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>s</mi><mo>−</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo fence="false">|</mo><mrow><mi>T</mi><mo>−</mo><mi>s</mi></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo fence="false">|</mo><msup><mi>x</mi><mo>′</mo></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\lim_{\Delta s\to 0}\left[\frac{d}{d\Delta s}Q_{T-(s-\Delta s)|T-s}(x|x^{\prime})\right]=\mathcal{L}(x,x^{\prime})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(45)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In this case, we can re-write Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E44" title="Equation 44 ‣ A.2 Reverse Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">44</span></a>) in the operator form,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E46">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{rev}}=Q\mathcal{L}^{\dagger}Q^{-1}+Q^{-1}\frac{dQ}{ds}" class="ltx_Math" display="block" id="A1.E46.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>rev</mtext></msub><mo>=</mo><mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>†</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>Q</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo>+</mo><mrow><msup><mi>Q</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mfrac><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>Q</mi></mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></mfrac></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{rev}}=Q\mathcal{L}^{\dagger}Q^{-1}+Q^{-1}\frac{dQ}{ds}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(46)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathcal{L}^{\dagger}" class="ltx_Math" display="inline" id="A1.SS2.p3.m2" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>†</mo></msup><annotation encoding="application/x-tex">\mathcal{L}^{\dagger}</annotation></semantics></math> is the adjoint operator to <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="A1.SS2.p3.m3" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math>. For continuous variables, the adjoint operator is defined as,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E47">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\int\psi_{2}\mathcal{L}\psi_{1}dx=\int\psi_{1}\mathcal{L}^{\dagger}\psi_{2}dx" class="ltx_Math" display="block" id="A1.E47.m1" intent=":literal"><semantics><mrow><mrow><mo>∫</mo><mrow><msub><mi>ψ</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>ψ</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">𝑑</mo><mi>x</mi></mrow></mrow></mrow><mo rspace="0.111em">=</mo><mrow><mo>∫</mo><mrow><msub><mi>ψ</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>†</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>ψ</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo rspace="0em">𝑑</mo><mi>x</mi></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\int\psi_{2}\mathcal{L}\psi_{1}dx=\int\psi_{1}\mathcal{L}^{\dagger}\psi_{2}dx</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(47)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">for any test functions <math alttext="\psi_{1}" class="ltx_Math" display="inline" id="A1.SS2.p3.m4" intent=":literal"><semantics><msub><mi>ψ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\psi_{1}</annotation></semantics></math> and <math alttext="\psi_{2}" class="ltx_Math" display="inline" id="A1.SS2.p3.m5" intent=":literal"><semantics><msub><mi>ψ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\psi_{2}</annotation></semantics></math>. For discrete variables, <math alttext="\mathcal{L}^{\dagger}=\mathcal{L}^{T}" class="ltx_Math" display="inline" id="A1.SS2.p3.m6" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>†</mo></msup><mo>=</mo><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{L}^{\dagger}=\mathcal{L}^{T}</annotation></semantics></math>.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span>Continuous variables</h4>
<div class="ltx_para" id="A1.SS2.SSS1.p1">
<p class="ltx_p">In the case that the forward process is an Itô diffusion, <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math> is the generator for the corresponding Fokker-Planck equation,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E48">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=-\sum_{i}\frac{\partial}{\partial x_{i}}f_{i}(x,t)+\frac{1}{2}\sum_{i,j}\frac{\partial}{\partial x_{i}}\frac{\partial}{\partial x_{j}}D_{ij}(t)" class="ltx_Math" display="block" id="A1.E48.m1" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>=</mo><mrow><mrow><mo>−</mo><mrow><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder><mrow><mfrac><mo>∂</mo><mrow><mo rspace="0em">∂</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>f</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><mrow><mfrac><mo>∂</mo><mrow><mo rspace="0em">∂</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mfrac><mo>∂</mo><mrow><mo rspace="0em">∂</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>D</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}=-\sum_{i}\frac{\partial}{\partial x_{i}}f_{i}(x,t)+\frac{1}{2}\sum_{i,j}\frac{\partial}{\partial x_{i}}\frac{\partial}{\partial x_{j}}D_{ij}(t)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(48)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="D" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p1.m2" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> is a symmetric matrix, <math alttext="D_{ij}=D_{ji}" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p1.m3" intent=":literal"><semantics><mrow><msub><mi>D</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>D</mi><mrow><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{ij}=D_{ji}</annotation></semantics></math> that does not depend on <math alttext="x" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p1.m4" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A1.SS2.SSS1.p2">
<p class="ltx_p">Using Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E47" title="Equation 47 ‣ A.2 Reverse Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">47</span></a>) and integration by parts, it can be shown that the adjoint operator is,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E49">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}^{\dagger}=\sum_{i}f_{i}\frac{\partial}{\partial x_{i}}+\frac{1}{2}\sum_{i,j}D_{ij}\frac{\partial}{\partial x_{i}}\frac{\partial}{\partial x_{i}}" class="ltx_Math" display="block" id="A1.E49.m1" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>†</mo></msup><mo rspace="0.111em">=</mo><mrow><mrow><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder><mrow><msub><mi>f</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mfrac><mo>∂</mo><mrow><mo rspace="0em">∂</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><mrow><msub><mi>D</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mfrac><mo>∂</mo><mrow><mo rspace="0em">∂</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mfrac><mo>∂</mo><mrow><mo rspace="0em">∂</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}^{\dagger}=\sum_{i}f_{i}\frac{\partial}{\partial x_{i}}+\frac{1}{2}\sum_{i,j}D_{ij}\frac{\partial}{\partial x_{i}}\frac{\partial}{\partial x_{i}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(49)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">By directly substituting Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E49" title="Equation 49 ‣ A.2.1 Continuous variables ‣ A.2 Reverse Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">49</span></a>) into Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E46" title="Equation 46 ‣ A.2 Reverse Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">46</span></a>) and simplifying, <math alttext="\mathcal{L}_{\text{rev}}" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p2.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>rev</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{rev}}</annotation></semantics></math> can be reduced to,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E50">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{rev}}=\sum_{i}\frac{\partial}{\partial x_{i}}g_{i}+\frac{1}{2}\sum_{i,j}\frac{\partial}{\partial x_{i}}\frac{\partial}{\partial x_{j}}D_{ij}" class="ltx_Math" display="block" id="A1.E50.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>rev</mtext></msub><mo rspace="0.111em">=</mo><mrow><mrow><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder><mrow><mfrac><mo>∂</mo><mrow><mo rspace="0em">∂</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>g</mi><mi>i</mi></msub></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><mrow><mfrac><mo>∂</mo><mrow><mo rspace="0em">∂</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mfrac><mo>∂</mo><mrow><mo rspace="0em">∂</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>D</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{rev}}=\sum_{i}\frac{\partial}{\partial x_{i}}g_{i}+\frac{1}{2}\sum_{i,j}\frac{\partial}{\partial x_{i}}\frac{\partial}{\partial x_{j}}D_{ij}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(50)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">with the drift vector <math alttext="g" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p2.m2" intent=":literal"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E51">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="g_{i}(x,t)=f_{i}(x,t)-\frac{1}{Q_{t}(x)}\sum_{j}\frac{\partial}{\partial x_{j}}\left[D_{ij}(x,t)Q_{t}(x)\right]" class="ltx_Math" display="block" id="A1.E51.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>g</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>f</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mrow><msub><mi>Q</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munder><mo movablelimits="false">∑</mo><mi>j</mi></munder><mrow><mfrac><mo>∂</mo><mrow><mo rspace="0em">∂</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><msub><mi>D</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>Q</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">g_{i}(x,t)=f_{i}(x,t)-\frac{1}{Q_{t}(x)}\sum_{j}\frac{\partial}{\partial x_{j}}\left[D_{ij}(x,t)Q_{t}(x)\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(51)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A1.SS2.SSS1.p3">
<p class="ltx_p">If <math alttext="\Delta t" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p3.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">\Delta t</annotation></semantics></math> is chosen to be sufficiently small, Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E51" title="Equation 51 ‣ A.2.1 Continuous variables ‣ A.2 Reverse Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">51</span></a>) can be linearized and the transition kernel is Gaussian,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E52">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q_{t|t+\Delta t}(x^{\prime}|x)\propto\exp{\left(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right)}" class="ltx_Math" display="block" id="A1.E52.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Q</mi><mrow><mi>t</mi><mo fence="false">|</mo><mrow><mi>t</mi><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mo>′</mo></msup><mo fence="false">|</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><mo stretchy="false">)</mo></mrow><mi>T</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi mathvariant="normal">Σ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">Q_{t|t+\Delta t}(x^{\prime}|x)\propto\exp{\left(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(52)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="A1.E53">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mu=x+\Delta t\&gt;g_{i}(x,t)" class="ltx_Math" display="block" id="A1.E53.m1" intent=":literal"><semantics><mrow><mi>μ</mi><mo>=</mo><mrow><mi>x</mi><mo>+</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi><mo lspace="0.220em" rspace="0em">​</mo><msub><mi>g</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mu=x+\Delta t\&gt;g_{i}(x,t)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(53)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="A1.E54">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Sigma=\Delta t\&gt;D(t)" class="ltx_Math" display="block" id="A1.E54.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Σ</mi><mo>=</mo><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi><mo lspace="0.220em" rspace="0em">​</mo><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\Sigma=\Delta t\&gt;D(t)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(54)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Therefore, one can build a continuous diffusion model with arbitrary approximation power by working in the small <math alttext="\Delta t" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p3.m2" intent=":literal"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">\Delta t</annotation></semantics></math> limit and approximating the reverse process using a Gaussian distribution with a neural network defining the mean vector <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib5" title="Deep unsupervised learning using nonequilibrium thermodynamics">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib4" title="Denoising diffusion probabilistic models">4</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.2 </span>Discrete variables</h4>
<div class="ltx_para" id="A1.SS2.SSS2.p1">
<p class="ltx_p">In a discrete diffusion model, <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math> is given by Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E36" title="Equation 36 ‣ A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">36</span></a>). This tensor product form for <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math> guarantees that <math alttext="\mathcal{L}(x^{\prime},x)=0" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.m3" intent=":literal"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mo>′</mo></msup><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathcal{L}(x^{\prime},x)=0</annotation></semantics></math> for any vectors <math alttext="x^{\prime}" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.m4" intent=":literal"><semantics><msup><mi>x</mi><mo>′</mo></msup><annotation encoding="application/x-tex">x^{\prime}</annotation></semantics></math> and <math alttext="x" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.m5" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> that have a Hamming distance greater than one (which means they have at least <math alttext="N-1" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.m6" intent=":literal"><semantics><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N-1</annotation></semantics></math> matching elements). As such, in discrete diffusion models, neural networks trained to approximate ratios of the data distribution <math alttext="\frac{Q_{T-s}(x^{\prime})}{Q_{T-s}(x)}" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.m7" intent=":literal"><semantics><mfrac><mrow><msub><mi>Q</mi><mrow><mi>T</mi><mo>−</mo><mi>s</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>Q</mi><mrow><mi>T</mi><mo>−</mo><mi>s</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac><annotation encoding="application/x-tex">\frac{Q_{T-s}(x^{\prime})}{Q_{T-s}(x)}</annotation></semantics></math> for neighboring <math alttext="x^{\prime}" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.m8" intent=":literal"><semantics><msup><mi>x</mi><mo>′</mo></msup><annotation encoding="application/x-tex">x^{\prime}</annotation></semantics></math> and <math alttext="x" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.m9" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> can be used to implement an arbitrarily good approximation to the actual reverse process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib3" title="Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution">7</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>The Diffusion Loss</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p">As discussed in the main text, a diffusion model is trained by minimizing the distributional distance between the joint distributions of the forward process <math alttext="Q_{0,\dots,T}" class="ltx_Math" display="inline" id="A1.SS3.p1.m1" intent=":literal"><semantics><msub><mi>Q</mi><mrow><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">Q_{0,\dots,T}</annotation></semantics></math> and our learned approximation to the reverse process <math alttext="P_{0,\dots,T}^{\theta}" class="ltx_Math" display="inline" id="A1.SS3.p1.m2" intent=":literal"><semantics><msubsup><mi>P</mi><mrow><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>T</mi></mrow><mi>θ</mi></msubsup><annotation encoding="application/x-tex">P_{0,\dots,T}^{\theta}</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E55">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{DN}(\theta)=D\left(Q_{0,\dots,T}(\cdot)||P^{\theta}_{0,\dots,T}(\cdot)\right)" class="ltx_math_unparsed" display="block" id="A1.E55.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mi>D</mi><mrow><mo>(</mo><msub><mi>Q</mi><mrow><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>T</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msubsup><mi>P</mi><mrow><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>T</mi></mrow><mi>θ</mi></msubsup><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{DN}(\theta)=D\left(Q_{0,\dots,T}(\cdot)||P^{\theta}_{0,\dots,T}(\cdot)\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(55)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">the Markovian nature of <math alttext="Q" class="ltx_Math" display="inline" id="A1.SS3.p1.m3" intent=":literal"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> can be taken advantage of to simplify Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E55" title="Equation 55 ‣ A.3 The Diffusion Loss ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">55</span></a>) into a layerwise form,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E56">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{DN}(\theta)+C=-\sum_{t=1}^{T}\mathbb{E}_{Q(x_{t-1},x_{t})}\left[\log{\left(P_{\theta}(x_{t-1}|x_{t}\right)}\right]" class="ltx_math_unparsed" display="block" id="A1.E56.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mo>+</mo><mi>C</mi><mo rspace="0em">=</mo><mo lspace="0em" rspace="0.055em">−</mo><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><msub><mi mathvariant="normal">𝔼</mi><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></msub><mrow><mo>[</mo><mi>log</mi><mrow><mo>(</mo><msub><mi>P</mi><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>x</mi><mi>t</mi></msub><mo>)</mo></mrow><mo>]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{DN}(\theta)+C=-\sum_{t=1}^{T}\mathbb{E}_{Q(x_{t-1},x_{t})}\left[\log{\left(P_{\theta}(x_{t-1}|x_{t}\right)}\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(56)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="C" class="ltx_Math" display="inline" id="A1.SS3.p1.m4" intent=":literal"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> does not depend on <math alttext="\theta" class="ltx_Math" display="inline" id="A1.SS3.p1.m5" intent=":literal"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>. For denoising algorithms that operate in the infinitesimal limit, the simple form of <math alttext="P_{\theta}" class="ltx_Math" display="inline" id="A1.SS3.p1.m6" intent=":literal"><semantics><msub><mi>P</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">P_{\theta}</annotation></semantics></math> allows for <math alttext="\mathcal{L}_{DN}" class="ltx_Math" display="inline" id="A1.SS3.p1.m7" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></msub><annotation encoding="application/x-tex">\mathcal{L}_{DN}</annotation></semantics></math> and its gradients to be computed exactly.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.1 </span>A Monte-Carlo gradient estimator</h4>
<div class="ltx_para" id="A1.SS3.SSS1.p1">
<p class="ltx_p">In the case where <math alttext="P_{\theta}\left(x^{t-1}|x^{t}\right)" class="ltx_Math" display="inline" id="A1.SS3.SSS1.p1.m1" intent=":literal"><semantics><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{\theta}\left(x^{t-1}|x^{t}\right)</annotation></semantics></math> is an EBM, there exists no simple closed-form expression for <math alttext="\nabla_{\theta}\mathcal{L}_{DN}(\theta)" class="ltx_Math" display="inline" id="A1.SS3.SSS1.p1.m2" intent=":literal"><semantics><mrow><mrow><msub><mo>∇</mo><mi>θ</mi></msub><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\theta}\mathcal{L}_{DN}(\theta)</annotation></semantics></math>. In that case, one must employ a Monte Carlo estimator to approximate the gradient. This estimator can be derived directly by taking the gradient of Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E56" title="Equation 56 ‣ A.3 The Diffusion Loss ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">56</span></a>),</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E57">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\nabla_{\theta}\mathcal{L}_{DN}(\theta)=-\sum_{t=1}^{T}\mathbb{E}_{Q(x^{t-1},x^{t})}\left[\nabla_{\theta}\log{\left(P_{\theta}(x^{t-1}|x^{t})\right)}\right]" class="ltx_Math" display="block" id="A1.E57.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mo>∇</mo><mi>θ</mi></msub><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><msub><mi mathvariant="normal">𝔼</mi><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>x</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><mi>log</mi></mrow><mo>⁡</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\theta}\mathcal{L}_{DN}(\theta)=-\sum_{t=1}^{T}\mathbb{E}_{Q(x^{t-1},x^{t})}\left[\nabla_{\theta}\log{\left(P_{\theta}(x^{t-1}|x^{t})\right)}\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(57)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">If we have an EBM parameterization for <math alttext="P_{\theta}\left(x^{t-1}|x^{t}\right)" class="ltx_Math" display="inline" id="A1.SS3.SSS1.p1.m3" intent=":literal"><semantics><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{\theta}\left(x^{t-1}|x^{t}\right)</annotation></semantics></math> this may be simplified further. Specifically, given the latent variable from Eq. 8 in the main text, the gradient of log-likelihood may be simplified to,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E58">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\nabla_{\theta}\log{\left(P_{\theta}(x^{t-1}|x^{t})\right)}=\mathbb{E}_{P_{\theta}(x^{t-1},z^{t-1}|x^{t})}\left[\nabla_{\theta}\mathcal{E}^{m}_{t-1}\right]-\mathbb{E}_{P_{\theta}(z^{t-1}|x^{t-1},x^{t})}\left[\nabla_{\theta}\mathcal{E}^{m}_{t-1}\right]" class="ltx_Math" display="block" id="A1.E58.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mo>∇</mo><mi>θ</mi></msub><mi>log</mi></mrow><mo>⁡</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi mathvariant="normal">𝔼</mi><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><mrow><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow><mo>]</mo></mrow></mrow><mo>−</mo><mrow><msub><mi mathvariant="normal">𝔼</mi><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><mrow><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>x</mi><mi>t</mi></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow><mo>]</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\theta}\log{\left(P_{\theta}(x^{t-1}|x^{t})\right)}=\mathbb{E}_{P_{\theta}(x^{t-1},z^{t-1}|x^{t})}\left[\nabla_{\theta}\mathcal{E}^{m}_{t-1}\right]-\mathbb{E}_{P_{\theta}(z^{t-1}|x^{t-1},x^{t})}\left[\nabla_{\theta}\mathcal{E}^{m}_{t-1}\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(58)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Inserting this into Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E57" title="Equation 57 ‣ A.3.1 A Monte-Carlo gradient estimator ‣ A.3 The Diffusion Loss ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">57</span></a>) yields the final result given in Eq. 14 in the article.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Simplification of the Energy Landscape</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p">As the forward process timestep is made smaller, the energy landscape of the EBM-based approximation to the reverse process becomes simpler. A simple 1D example serves as a good demonstration of this concept. Consider the marginal energy function,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E59">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{E}_{t-1}^{\theta}\left(x_{t-1}\right)=\left(x_{t-1}^{2}-1\right)^{2}" class="ltx_Math" display="block" id="A1.E59.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>=</mo><msup><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo>−</mo><mn>1</mn></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\mathcal{E}_{t-1}^{\theta}\left(x_{t-1}\right)=\left(x_{t-1}^{2}-1\right)^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(59)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and a forward process energy function that corresponds to Gaussian diffusion (Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E20" title="Equation 20 ‣ A.1.1 Continuous Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">20</span></a>)),</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E60">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{E}_{t-1}^{f}\left(x_{t-1},x_{t}\right)=\lambda\left(\frac{x_{t-1}}{x_{t}}-1\right)^{2}" class="ltx_Math" display="block" id="A1.E60.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>f</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mfrac><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>x</mi><mi>t</mi></msub></mfrac><mo>−</mo><mn>1</mn></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}_{t-1}^{f}\left(x_{t-1},x_{t}\right)=\lambda\left(\frac{x_{t-1}}{x_{t}}-1\right)^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(60)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The parameter <math alttext="\lambda" class="ltx_Math" display="inline" id="A1.SS4.p1.m1" intent=":literal"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> scales inversely with the size of the forward process timestep; that is, <math alttext="\lim\limits_{\Delta t\to 0}\lambda=\infty" class="ltx_Math" display="inline" id="A1.SS4.p1.m2" intent=":literal"><semantics><mrow><mrow><munder><mo movablelimits="false">lim</mo><mrow><mrow><mi mathvariant="normal">Δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow><mo stretchy="false">→</mo><mn>0</mn></mrow></munder><mi>λ</mi></mrow><mo>=</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\lim\limits_{\Delta t\to 0}\lambda=\infty</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A1.SS4.p2">
<p class="ltx_p">The reverse process conditional energy landscape is then <math alttext="\mathcal{E}_{t-1}^{\theta}+\mathcal{E}_{t-1}^{f}" class="ltx_Math" display="inline" id="A1.SS4.p2.m1" intent=":literal"><semantics><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><mo>+</mo><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>f</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{E}_{t-1}^{\theta}+\mathcal{E}_{t-1}^{f}</annotation></semantics></math>. The effect of <math alttext="\lambda" class="ltx_Math" display="inline" id="A1.SS4.p2.m2" intent=":literal"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> on this is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.F7" title="Figure 7 ‣ A.4 Simplification of the Energy Landscape ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="374" id="A1.F7.g1" src="x7.png" width="498"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span class="ltx_text ltx_font_bold">Conditioning of the energy landscape</span> As <math alttext="\lambda" class="ltx_Math" display="inline" id="A1.F7.m3" intent=":literal"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> is increased, the energy landscape is reshaped from a strongly bimodal distribution towards a simple Gaussian centered at <math alttext="x_{t}=-0.5" class="ltx_Math" display="inline" id="A1.F7.m4" intent=":literal"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>=</mo><mrow><mo>−</mo><mn>0.5</mn></mrow></mrow><annotation encoding="application/x-tex">x_{t}=-0.5</annotation></semantics></math>. The latter is much easier to sample from.</figcaption>
</figure>
<div class="ltx_para" id="A1.SS4.p3">
<p class="ltx_p">The energy landscape is bimodal at <math alttext="\lambda=0" class="ltx_Math" display="inline" id="A1.SS4.p3.m1" intent=":literal"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda=0</annotation></semantics></math> and gradually becomes distorted towards an unimodal distribution centered at <math alttext="x_{t}" class="ltx_Math" display="inline" id="A1.SS4.p3.m2" intent=":literal"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math> as <math alttext="\lambda" class="ltx_Math" display="inline" id="A1.SS4.p3.m3" intent=":literal"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> increases. This reshaping is intuitive, as shortening the forward process timestep should more strongly constrain <math alttext="x_{t-1}" class="ltx_Math" display="inline" id="A1.SS4.p3.m4" intent=":literal"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">x_{t-1}</annotation></semantics></math> to <math alttext="x_{t}" class="ltx_Math" display="inline" id="A1.SS4.p3.m5" intent=":literal"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Conditional Generation</h3>
<div class="ltx_para" id="A1.SS5.p1">
<p class="ltx_p">The denoising framework can be adapted for conditional generation tasks, such as generating MNIST digits given a specific class label. In principle, this is very simple: we concatenate the target (in our case, the images) and a one-hot encoding of the labels into a contiguous binary vector and treat that whole thing as our training data on which we train the denoising model as described above.</p>
</div>
<div class="ltx_para" id="A1.SS5.p2">
<p class="ltx_p">In this case, the visible nodes of the Boltzmann machine are partitioned into "pixel nodes" <math alttext="V_{X}" class="ltx_Math" display="inline" id="A1.SS5.p2.m1" intent=":literal"><semantics><msub><mi>V</mi><mi>X</mi></msub><annotation encoding="application/x-tex">V_{X}</annotation></semantics></math> and "label nodes" <math alttext="V_{L}" class="ltx_Math" display="inline" id="A1.SS5.p2.m2" intent=":literal"><semantics><msub><mi>V</mi><mi>L</mi></msub><annotation encoding="application/x-tex">V_{L}</annotation></semantics></math>. All visible nodes come in pairs of input and output nodes (drawn in blue and green resp. in Fig. 3 in the main paper body and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3.F9" title="Figure 9 ‣ C.1 Implementation of the forward process energy function ‣ Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">9</span></a> below), so the set of visible nodes now consists of <math alttext="V^{\text{in}}_{X},V^{\text{out}}_{X},V^{\text{in}}_{L}," class="ltx_Math" display="inline" id="A1.SS5.p2.m3" intent=":literal"><semantics><mrow><mrow><msubsup><mi>V</mi><mi>X</mi><mtext>in</mtext></msubsup><mo>,</mo><msubsup><mi>V</mi><mi>X</mi><mtext>out</mtext></msubsup><mo>,</mo><msubsup><mi>V</mi><mi>L</mi><mtext>in</mtext></msubsup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">V^{\text{in}}_{X},V^{\text{out}}_{X},V^{\text{in}}_{L},</annotation></semantics></math> and <math alttext="V^{\text{out}}_{L}" class="ltx_Math" display="inline" id="A1.SS5.p2.m4" intent=":literal"><semantics><msubsup><mi>V</mi><mi>L</mi><mtext>out</mtext></msubsup><annotation encoding="application/x-tex">V^{\text{out}}_{L}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A1.SS5.p3">
<p class="ltx_p">The training procedure works the same way as before, just using this label-augmented data. We obtain the noised training images <math alttext="X_{n}" class="ltx_Math" display="inline" id="A1.SS5.p3.m1" intent=":literal"><semantics><msub><mi>X</mi><mi>n</mi></msub><annotation encoding="application/x-tex">X_{n}</annotation></semantics></math> and labels <math alttext="L_{n}" class="ltx_Math" display="inline" id="A1.SS5.p3.m2" intent=":literal"><semantics><msub><mi>L</mi><mi>n</mi></msub><annotation encoding="application/x-tex">L_{n}</annotation></semantics></math> by noising each entry of <math alttext="X_{0}" class="ltx_Math" display="inline" id="A1.SS5.p3.m3" intent=":literal"><semantics><msub><mi>X</mi><mn>0</mn></msub><annotation encoding="application/x-tex">X_{0}</annotation></semantics></math> and <math alttext="L_{0}" class="ltx_Math" display="inline" id="A1.SS5.p3.m4" intent=":literal"><semantics><msub><mi>L</mi><mn>0</mn></msub><annotation encoding="application/x-tex">L_{0}</annotation></semantics></math> resp. independently using the forward process described in subsection <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS1.SSS2" title="A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">A.1.2</span></a>. Then we train the <math alttext="n" class="ltx_Math" display="inline" id="A1.SS5.p3.m5" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>th step model <math alttext="P_{\theta_{n}}\big(V^{\text{out}}_{X}=x,V^{\text{out}}_{L}=l\,|\,V^{\text{in}}_{X}=x^{\prime},V^{\text{in}}_{L}=l^{\prime}\big)" class="ltx_math_unparsed" display="inline" id="A1.SS5.p3.m6" intent=":literal"><semantics><mrow><msub><mi>P</mi><msub><mi>θ</mi><mi>n</mi></msub></msub><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><msubsup><mi>V</mi><mi>X</mi><mtext>out</mtext></msubsup><mo>=</mo><mi>x</mi><mo>,</mo><msubsup><mi>V</mi><mi>L</mi><mtext>out</mtext></msubsup><mo>=</mo><mi>l</mi><mo fence="false" lspace="0.170em" rspace="0.337em" stretchy="false">|</mo><msubsup><mi>V</mi><mi>X</mi><mtext>in</mtext></msubsup><mo>=</mo><msup><mi>x</mi><mo>′</mo></msup><mo>,</mo><msubsup><mi>V</mi><mi>L</mi><mtext>in</mtext></msubsup><mo>=</mo><msup><mi>l</mi><mo>′</mo></msup><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{\theta_{n}}\big(V^{\text{out}}_{X}=x,V^{\text{out}}_{L}=l\,|\,V^{\text{in}}_{X}=x^{\prime},V^{\text{in}}_{L}=l^{\prime}\big)</annotation></semantics></math> to approximate (in terms Kullback-Leibler divergence) the distribution <math alttext="\mathbb{P}\big(X_{n}=x,L_{n}=l\,|\,X_{n+1}=x^{\prime},L_{n+1}=l^{\prime}\big)" class="ltx_math_unparsed" display="inline" id="A1.SS5.p3.m7" intent=":literal"><semantics><mrow><mi mathvariant="normal">ℙ</mi><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><msub><mi>X</mi><mi>n</mi></msub><mo>=</mo><mi>x</mi><mo>,</mo><msub><mi>L</mi><mi>n</mi></msub><mo>=</mo><mi>l</mi><mo fence="false" lspace="0.170em" rspace="0.337em" stretchy="false">|</mo><msub><mi>X</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>x</mi><mo>′</mo></msup><mo>,</mo><msub><mi>L</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>l</mi><mo>′</mo></msup><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{P}\big(X_{n}=x,L_{n}=l\,|\,X_{n+1}=x^{\prime},L_{n+1}=l^{\prime}\big)</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A1.SS5.p4">
<p class="ltx_p">At inference time, we have two cases:</p>
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p">Unconditional inference proceeds as with regular denoising. We pass the pixel and label values backward through all the step models, and at the end, we record the pixel values.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p">For conditional generation we clamp all label output nodes <math alttext="V^{\text{out}}_{L}" class="ltx_Math" display="inline" id="A1.I1.i2.p1.m1" intent=":literal"><semantics><msubsup><mi>V</mi><mi>L</mi><mtext>out</mtext></msubsup><annotation encoding="application/x-tex">V^{\text{out}}_{L}</annotation></semantics></math> in all step models to <math alttext="l_{0}" class="ltx_Math" display="inline" id="A1.I1.i2.p1.m2" intent=":literal"><semantics><msub><mi>l</mi><mn>0</mn></msub><annotation encoding="application/x-tex">l_{0}</annotation></semantics></math> and sample <math alttext="\widehat{X}_{n}\sim P_{\theta_{n}}\big(V^{\text{out}}_{X}=\cdot\,|\,V^{\text{in}}_{X}=\widehat{X}_{n+1},V^{\text{out}}_{L}=V^{\text{in}}_{L}=l_{0}\big)" class="ltx_math_unparsed" display="inline" id="A1.I1.i2.p1.m3" intent=":literal"><semantics><mrow><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mi>n</mi></msub><mo>∼</mo><msub><mi>P</mi><msub><mi>θ</mi><mi>n</mi></msub></msub><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><msubsup><mi>V</mi><mi>X</mi><mtext>out</mtext></msubsup><mo rspace="0em">=</mo><mo lspace="0em">⋅</mo><mo fence="false" rspace="0.337em" stretchy="false">|</mo><msubsup><mi>V</mi><mi>X</mi><mtext>in</mtext></msubsup><mo>=</mo><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><msubsup><mi>V</mi><mi>L</mi><mtext>out</mtext></msubsup><mo>=</mo><msubsup><mi>V</mi><mi>L</mi><mtext>in</mtext></msubsup><mo>=</mo><msub><mi>l</mi><mn>0</mn></msub><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow><annotation encoding="application/x-tex">\widehat{X}_{n}\sim P_{\theta_{n}}\big(V^{\text{out}}_{X}=\cdot\,|\,V^{\text{in}}_{X}=\widehat{X}_{n+1},V^{\text{out}}_{L}=V^{\text{in}}_{L}=l_{0}\big)</annotation></semantics></math>, where <math alttext="l_{0}" class="ltx_Math" display="inline" id="A1.I1.i2.p1.m4" intent=":literal"><semantics><msub><mi>l</mi><mn>0</mn></msub><annotation encoding="application/x-tex">l_{0}</annotation></semantics></math> is an unnoised label and <math alttext="\widehat{X}_{n+1}" class="ltx_Math" display="inline" id="A1.I1.i2.p1.m5" intent=":literal"><semantics><msub><mover accent="true"><mi>X</mi><mo>^</mo></mover><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\widehat{X}_{n+1}</annotation></semantics></math> is the output of <math alttext="P_{\theta_{n+1}}" class="ltx_Math" display="inline" id="A1.I1.i2.p1.m6" intent=":literal"><semantics><msub><mi>P</mi><msub><mi>θ</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></msub><annotation encoding="application/x-tex">P_{\theta_{n+1}}</annotation></semantics></math> (or uniform noise if <math alttext="n=N" class="ltx_Math" display="inline" id="A1.I1.i2.p1.m7" intent=":literal"><semantics><mrow><mi>n</mi><mo>=</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">n=N</annotation></semantics></math>).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A1.SS5.p5">
<p class="ltx_p">Note that all step models except <math alttext="\theta_{0}" class="ltx_Math" display="inline" id="A1.SS5.p5.m1" intent=":literal"><semantics><msub><mi>θ</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\theta_{0}</annotation></semantics></math> will be trained on somewhat noised labels, so they might never have seen a pristine unnoised label during training (if there are 10 classes and five label repetitions, a strongly noised label has an approximately <math alttext="10\times 2^{-50}" class="ltx_Math" display="inline" id="A1.SS5.p5.m2" intent=":literal"><semantics><mrow><mn>10</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>2</mn><mrow><mo>−</mo><mn>50</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10\times 2^{-50}</annotation></semantics></math> chance of being a valid unnoised label). However, during conditional inference, the models will have their label nodes clamped to an unnoised label <math alttext="l_{0}" class="ltx_Math" display="inline" id="A1.SS5.p5.m3" intent=":literal"><semantics><msub><mi>l</mi><mn>0</mn></msub><annotation encoding="application/x-tex">l_{0}</annotation></semantics></math>, and they may not know how this should influence the generated image (and this problem would only be exacerbated if we clamped to a noised label instead).</p>
</div>
<div class="ltx_para" id="A1.SS5.p6">
<p class="ltx_p">This issue can be mitigated by using a rate <math alttext="\gamma_{X}" class="ltx_Math" display="inline" id="A1.SS5.p6.m1" intent=":literal"><semantics><msub><mi>γ</mi><mi>X</mi></msub><annotation encoding="application/x-tex">\gamma_{X}</annotation></semantics></math> when noising image entries in the training data and a different rate <math alttext="\gamma_{L}" class="ltx_Math" display="inline" id="A1.SS5.p6.m2" intent=":literal"><semantics><msub><mi>γ</mi><mi>L</mi></msub><annotation encoding="application/x-tex">\gamma_{L}</annotation></semantics></math> for noising label entries. Recall that the higher the <math alttext="\gamma" class="ltx_Math" display="inline" id="A1.SS5.p6.m3" intent=":literal"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>, the noisier the data will become as <math alttext="n" class="ltx_Math" display="inline" id="A1.SS5.p6.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> increases.</p>
</div>
<div class="ltx_para" id="A1.SS5.p7">
<p class="ltx_p">We consider two extremes:</p>
<ul class="ltx_itemize" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p">If <math alttext="\gamma_{L}\geq\gamma_{X}" class="ltx_Math" display="inline" id="A1.I2.i1.p1.m1" intent=":literal"><semantics><mrow><msub><mi>γ</mi><mi>L</mi></msub><mo>≥</mo><msub><mi>γ</mi><mi>X</mi></msub></mrow><annotation encoding="application/x-tex">\gamma_{L}\geq\gamma_{X}</annotation></semantics></math>, then we have the exact same problem as before.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i2.p1">
<p class="ltx_p">If <math alttext="\gamma_{L}=0" class="ltx_Math" display="inline" id="A1.I2.i2.p1.m1" intent=":literal"><semantics><mrow><msub><mi>γ</mi><mi>L</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\gamma_{L}=0</annotation></semantics></math>, then the labels in the training data are a zero-temperature distribution. This low temperature can lead to freezing, potentially negating the benefits denoising could otherwise bring.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A1.SS5.p8">
<p class="ltx_p">Experimentally, we observed that settings in the ranges <math alttext="\gamma_{L}\in[0.1,0.3]" class="ltx_Math" display="inline" id="A1.SS5.p8.m1" intent=":literal"><semantics><mrow><msub><mi>γ</mi><mi>L</mi></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0.1</mn><mo>,</mo><mn>0.3</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma_{L}\in[0.1,0.3]</annotation></semantics></math> and <math alttext="\gamma_{X}\in[0.7,1.5]" class="ltx_Math" display="inline" id="A1.SS5.p8.m2" intent=":literal"><semantics><mrow><msub><mi>γ</mi><mi>X</mi></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0.7</mn><mo>,</mo><mn>1.5</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma_{X}\in[0.7,1.5]</annotation></semantics></math> (for models with four to 12 steps) yielded good conditional generation performance while avoiding the freezing problem.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>Learning the marginal</h3>
<div class="ltx_para" id="A1.SS6.p1">
<p class="ltx_p">If a DTM is trained to match the conditional distribution of the reverse process perfectly, the learned energy function <math alttext="\mathcal{E}_{t-1}^{\theta}" class="ltx_Math" display="inline" id="A1.SS6.p1.m1" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><annotation encoding="application/x-tex">\mathcal{E}_{t-1}^{\theta}</annotation></semantics></math> is the energy function of the true marginal distribution, that is, <math alttext="\mathcal{E}_{t-1}^{\theta}(x)\propto\log Q(x^{t-1})" class="ltx_Math" display="inline" id="A1.SS6.p1.m2" intent=":literal"><semantics><mrow><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>Q</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}_{t-1}^{\theta}(x)\propto\log Q(x^{t-1})</annotation></semantics></math>. To show this, we start by applying the Bayes’ rule to the learned reverse process conditional in the limit that it perfectly matches the true reverse process,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E61">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{Q(x^{t}|x^{t-1})Q(x^{t-1})}{Q(x^{t})}=\frac{1}{Z(\theta,x^{t})}e^{-\left(\mathcal{E}^{f}_{t-1}\left(x^{t-1},x^{t}\right)+\mathcal{E}^{\theta}_{t-1}\left(x^{t-1},\theta\right)\right)}" class="ltx_Math" display="block" id="A1.E61.m1" intent=":literal"><semantics><mrow><mfrac><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><mi>t</mi></msup><mo fence="false">|</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><msup><mi>x</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mo>−</mo><mrow><mo>(</mo><mrow><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>f</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\frac{Q(x^{t}|x^{t-1})Q(x^{t-1})}{Q(x^{t})}=\frac{1}{Z(\theta,x^{t})}e^{-\left(\mathcal{E}^{f}_{t-1}\left(x^{t-1},x^{t}\right)+\mathcal{E}^{\theta}_{t-1}\left(x^{t-1},\theta\right)\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(61)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">defining the distribution,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E62">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="H(x^{t-1})=\frac{1}{Z(\theta)}\sum_{z^{t-1}}e^{-\mathcal{E}^{\theta}_{t-1}\left(x^{t-1},z^{t-1},\theta\right)}" class="ltx_Math" display="block" id="A1.E62.m1" intent=":literal"><semantics><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munder><mo movablelimits="false">∑</mo><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></munder><msup><mi>e</mi><mrow><mo>−</mo><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow></msup></mrow></mrow></mrow><annotation encoding="application/x-tex">H(x^{t-1})=\frac{1}{Z(\theta)}\sum_{z^{t-1}}e^{-\mathcal{E}^{\theta}_{t-1}\left(x^{t-1},z^{t-1},\theta\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(62)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="A1.E63">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Z(\theta)=\sum_{x^{t-1},z^{t-1}}e^{-\mathcal{E}^{\theta}_{t-1}\left(x^{t-1},z^{t-1},\theta\right)}" class="ltx_Math" display="block" id="A1.E63.m1" intent=":literal"><semantics><mrow><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow></munder><msup><mi>e</mi><mrow><mo>−</mo><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">Z(\theta)=\sum_{x^{t-1},z^{t-1}}e^{-\mathcal{E}^{\theta}_{t-1}\left(x^{t-1},z^{t-1},\theta\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(63)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">extracting the forward process from the RHS of Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E61" title="Equation 61 ‣ A.6 Learning the marginal ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">61</span></a>) and using Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E62" title="Equation 62 ‣ A.6 Learning the marginal ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">62</span></a>),</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E64">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{Q(x^{t-1})}{Q(x^{t})}=\frac{Z(\theta)Z}{Z(\theta,x^{t})}H(x^{t-1})" class="ltx_Math" display="block" id="A1.E64.m1" intent=":literal"><semantics><mrow><mfrac><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo>=</mo><mrow><mfrac><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>Z</mi></mrow><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><msup><mi>x</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\frac{Q(x^{t-1})}{Q(x^{t})}=\frac{Z(\theta)Z}{Z(\theta,x^{t})}H(x^{t-1})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(64)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A1.SS6.p2">
<p class="ltx_p">Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E64" title="Equation 64 ‣ A.6 Learning the marginal ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">64</span></a>) can easily be re-arranged into a form where the LHS depends only on <math alttext="x^{t}" class="ltx_Math" display="inline" id="A1.SS6.p2.m1" intent=":literal"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding="application/x-tex">x^{t}</annotation></semantics></math>, and the RHS depends only on <math alttext="x^{t-1}" class="ltx_Math" display="inline" id="A1.SS6.p2.m2" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">x^{t-1}</annotation></semantics></math>. From this, we can deduce,</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E65">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{Q(x^{t-1})}{H(x^{t-1})}=c" class="ltx_Math" display="block" id="A1.E65.m1" intent=":literal"><semantics><mrow><mfrac><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo>=</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">\frac{Q(x^{t-1})}{H(x^{t-1})}=c</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(65)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">from the fact that <math alttext="Q" class="ltx_Math" display="inline" id="A1.SS6.p2.m3" intent=":literal"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> and <math alttext="H" class="ltx_Math" display="inline" id="A1.SS6.p2.m4" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> are both normalized, we can find that <math alttext="c=1" class="ltx_Math" display="inline" id="A1.SS6.p2.m5" intent=":literal"><semantics><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">c=1</annotation></semantics></math>, which establishes the desired equivalence.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Hardware accelerators for EBMs</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p">In this work, we focus on a hardware architecture for EBMs that are naturally expressed as Probabilistic Graphical Models (PGMs). In a PGM-EBM, the random variables involved in the model map to the nodes of a graph, which are connected by edges that indicate dependence between variables.</p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p">PGMs form a natural basis for a hardware architecture because they can be sampled using a modular procedure that respects the graph’s structure. Specifically, the state of a PGM can be updated by iteratively stepping through each node of the graph and resampling one variable at a time, using only information about the current node and its immediate neighbors. Therefore, if a PGM is local, sparse, and somewhat heterogeneous, a piece of hardware can be built to efficiently sample from it that involves spatially arraying probabilistic sampling circuits that interact with each other cheaply via short wires.</p>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p">This local PGM sampler represents a type of compute-in-memory approach, where the state of the sampling program is spatially distributed throughout the array of sampling circuitry. Since the sampling circuits only communicate locally, this type of computer will spend significantly less energy on communication than one built on a Von-Neumann-like architecture, which constantly shuttles data between compute and memory.</p>
</div>
<div class="ltx_para" id="A2.p4">
<p class="ltx_p">Formally, the algorithm that defines this modular sampling procedure for PGMs is called Gibbs sampling. In Gibbs sampling, samples are drawn from the joint distribution <math alttext="p(x_{1},x_{2},\dots,x_{N})" class="ltx_Math" display="inline" id="A2.p4.m1" intent=":literal"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(x_{1},x_{2},\dots,x_{N})</annotation></semantics></math> by iteratively updating the state of each node conditioned on the current state of its neighbors. For the <math alttext="i^{th}" class="ltx_Math" display="inline" id="A2.p4.m2" intent=":literal"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>h</mi></mrow></msup><annotation encoding="application/x-tex">i^{th}</annotation></semantics></math> node, this means sampling from the distribution,</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E66">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{i}[t+1]\sim p(x_{i}|nb(x_{i})[t])." class="ltx_Math" display="block" id="A2.E66.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>∼</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo fence="false">|</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">x_{i}[t+1]\sim p(x_{i}|nb(x_{i})[t]).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(66)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This procedure defines a Markov chain whose stationary distribution can be easily controlled by adjusting the conditional update distributions of each node (see the next section for an example). Starting from some random initialization, this iterative update must be applied potentially many times to all graph nodes before the Markov chain converges to the desired stationary distribution, allowing us to draw samples from it.</p>
</div>
<figure class="ltx_figure" id="A2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="403" id="A2.F8.g1" src="x8.png" width="499"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span class="ltx_text ltx_font_bold">Chromatic Gibbs Sampling</span> A schematic view of an abstract hardware accelerator for a simple EBM. Each of the model’s variables is assigned to a node. Each node is capable of receiving information from its neighbors and updating its state according to the appropriate conditional distribution. Since each node’s update distribution only depends on the state of its neighbors and because nodes of the same color do not neighbor each other, they can all be updated in parallel.</figcaption>
</figure>
<div class="ltx_para" id="A2.p5">
<p class="ltx_p">Gibbs sampling allows for any two nodes that are not neighbors to be updated in parallel, meaning that the state can be updated in batches corresponding to different color groups of the graph. For a more thorough explanation of how Gibbs sampling works, see  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib6" title="Probabilistic Machine Learning: Advanced Topics">8</a>]</cite>.</p>
</div>
<div class="ltx_para" id="A2.p6">
<p class="ltx_p">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A2.F8" title="Figure 8 ‣ Appendix B Hardware accelerators for EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">8</span></a> shows a simple example of a PGM with two color groups that would be amenable to Gibbs sampling. Since <math alttext="x_{1}" class="ltx_Math" display="inline" id="A2.p6.m1" intent=":literal"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_{1}</annotation></semantics></math> is only connected to <math alttext="x_{2}" class="ltx_Math" display="inline" id="A2.p6.m2" intent=":literal"><semantics><msub><mi>x</mi><mn>2</mn></msub><annotation encoding="application/x-tex">x_{2}</annotation></semantics></math> and <math alttext="x_{4}" class="ltx_Math" display="inline" id="A2.p6.m3" intent=":literal"><semantics><msub><mi>x</mi><mn>4</mn></msub><annotation encoding="application/x-tex">x_{4}</annotation></semantics></math>, the update rule from Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A2.E66" title="Equation 66 ‣ Appendix B Hardware accelerators for EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">66</span></a>) would take the form,</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E67">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{1}[t+1]\sim p(x_{1}|x_{4}[t],x_{2}[t])" class="ltx_Math" display="block" id="A2.E67.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>∼</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo fence="false">|</mo><mrow><mrow><msub><mi>x</mi><mn>4</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">x_{1}[t+1]\sim p(x_{1}|x_{4}[t],x_{2}[t])</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(67)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">If the joint distribution had sufficient structure such that the conditional for each node had the same form, a piece of hardware could be built to sample from this PGM by building a 3x3 grid of sampling circuits that communicate only with their immediate neighbors.</p>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Quadratic EBMs</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p">The primary constraint around building a hardware device that implements Gibbs sampling is that the conditional update given in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A2.E66" title="Equation 66 ‣ Appendix B Hardware accelerators for EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">66</span></a>) must be efficiently implementable. Generally, this means that one wants it to take a form that is "natural" to the hardware substrate being used to build the computer.</p>
</div>
<div class="ltx_para" id="A2.SS1.p2">
<p class="ltx_p">To satisfy this constraint, it is generally necessary to limit the types of joint distributions that a hardware device can sample from. An example of such a restricted family of distributions is quadratic EBMs.</p>
</div>
<div class="ltx_para" id="A2.SS1.p3">
<p class="ltx_p">Quadratic EBMs have energy functions that are quadratic in the model’s variables, which generally leads to conditional updates computed by biasing a simple sampling circuit (Bernoulli, categorical, Gaussian, etc.) with the output of a linear function of the neighbor states and the model parameters. These simple interactions are efficient to implement in various types of hardware. As such, Quadratic EBMs have been the focus of most work on hardware accelerators for Gibbs sampling to date.</p>
</div>
<div class="ltx_para" id="A2.SS1.p4">
<p class="ltx_p">In the main text, we discuss Boltzmann machines, which involve only binary random variables and are the most basic form of quadratic EBM. The Conditional Update for Boltzmann Machines requires biasing a Bernoulli random variable according to a sigmoid function of a linear combination of the model parameters and the binary neighbor states, as shown in the main text, Eq. 11. This conditional update is efficiently implementable using an RNG with a sigmoidal bias and resistors, as discussed in section <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A10" title="Appendix J Some details on our RNG ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">J</span></a>.</p>
</div>
<div class="ltx_para" id="A2.SS1.p5">
<p class="ltx_p">Here, we will touch on a few other types of quadratic EBM that are more general. Although the experiments in this paper focused on Boltzmann machines, they could be trivially extended to these more expressive classes of distributions.</p>
</div>
<section class="ltx_subsubsection" id="A2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">B.1.1 </span>Potts models</h4>
<div class="ltx_para" id="A2.SS1.SSS1.p1">
<p class="ltx_p">Potts models generalize the concept of Boltzmann machines to <math alttext="k" class="ltx_Math" display="inline" id="A2.SS1.SSS1.p1.m1" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-state variables. They have the energy function,</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E68">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E(x)=\sum_{i,j=1}^{N}\sum_{m,n=1}^{M}x_{m}^{i}J_{mn}^{ij}x_{n}^{j}+\sum_{i=1}^{N}\sum_{m=1}^{M}h_{m}^{i}x_{m}^{i}" class="ltx_Math" display="block" id="A2.E68.m1" intent=":literal"><semantics><mrow><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msubsup><mi>x</mi><mi>m</mi><mi>i</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>J</mi><mrow><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi></mrow><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>x</mi><mi>n</mi><mi>j</mi></msubsup></mrow></mrow></mrow><mo rspace="0.055em">+</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msubsup><mi>h</mi><mi>m</mi><mi>i</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>x</mi><mi>m</mi><mi>i</mi></msubsup></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">E(x)=\sum_{i,j=1}^{N}\sum_{m,n=1}^{M}x_{m}^{i}J_{mn}^{ij}x_{n}^{j}+\sum_{i=1}^{N}\sum_{m=1}^{M}h_{m}^{i}x_{m}^{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(68)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="A2.E69">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="J^{ii}_{mn}=0" class="ltx_Math" display="block" id="A2.E69.m1" intent=":literal"><semantics><mrow><msubsup><mi>J</mi><mrow><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi></mrow><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi></mrow></msubsup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">J^{ii}_{mn}=0</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(69)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><math alttext="x_{m}^{i}" class="ltx_Math" display="inline" id="A2.SS1.SSS1.p1.m2" intent=":literal"><semantics><msubsup><mi>x</mi><mi>m</mi><mi>i</mi></msubsup><annotation encoding="application/x-tex">x_{m}^{i}</annotation></semantics></math> is a one-hot encoding of the state of variable <math alttext="x^{i}" class="ltx_Math" display="inline" id="A2.SS1.SSS1.p1.m3" intent=":literal"><semantics><msup><mi>x</mi><mi>i</mi></msup><annotation encoding="application/x-tex">x^{i}</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E70">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{m}^{i}\in\{0,1\}" class="ltx_Math" display="block" id="A2.E70.m1" intent=":literal"><semantics><mrow><msubsup><mi>x</mi><mi>m</mi><mi>i</mi></msubsup><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">x_{m}^{i}\in\{0,1\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(70)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="A2.E71">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\sum_{m}x_{m}^{i}=1" class="ltx_Math" display="block" id="A2.E71.m1" intent=":literal"><semantics><mrow><mrow><munder><mo movablelimits="false">∑</mo><mi>m</mi></munder><msubsup><mi>x</mi><mi>m</mi><mi>i</mi></msubsup></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sum_{m}x_{m}^{i}=1</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(71)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which implies that <math alttext="x_{m}^{i}=1" class="ltx_Math" display="inline" id="A2.SS1.SSS1.p1.m4" intent=":literal"><semantics><mrow><msubsup><mi>x</mi><mi>m</mi><mi>i</mi></msubsup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x_{m}^{i}=1</annotation></semantics></math> for a single value of <math alttext="m" class="ltx_Math" display="inline" id="A2.SS1.SSS1.p1.m5" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>, and is zero otherwise.
The distribution of any individual variable conditioned on it’s Markov blanket is,</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E72">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(x_{m}^{i}=1|\text{mb}(x^{i}))=\frac{1}{Z}\exp{\left(-\beta\left(\sum_{j\in\text{mb}(x^{i}),n}J_{mn}^{ij}x_{n}^{j}+\sum_{j\in\text{mb}(x^{i}),n}x_{n}^{j}J_{nm}^{ji}+h_{m}^{i}\right)\right)}" class="ltx_Math" display="block" id="A2.E72.m1" intent=":literal"><semantics><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>x</mi><mi>m</mi><mi>i</mi></msubsup><mo>=</mo><mrow><mn>1</mn><mo fence="false">|</mo><mrow><mtext>mb</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>Z</mi></mfrac><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>−</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><munder><mo lspace="0em" movablelimits="false">∑</mo><mrow><mi>j</mi><mo>∈</mo><mrow><mrow><mtext>mb</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>n</mi></mrow></mrow></munder><mrow><msubsup><mi>J</mi><mrow><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi></mrow><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>x</mi><mi>n</mi><mi>j</mi></msubsup></mrow></mrow><mo rspace="0.055em">+</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>∈</mo><mrow><mrow><mtext>mb</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>n</mi></mrow></mrow></munder><mrow><msubsup><mi>x</mi><mi>n</mi><mi>j</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>J</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi></mrow><mrow><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi></mrow></msubsup></mrow></mrow><mo>+</mo><msubsup><mi>h</mi><mi>m</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">p(x_{m}^{i}=1|\text{mb}(x^{i}))=\frac{1}{Z}\exp{\left(-\beta\left(\sum_{j\in\text{mb}(x^{i}),n}J_{mn}^{ij}x_{n}^{j}+\sum_{j\in\text{mb}(x^{i}),n}x_{n}^{j}J_{nm}^{ji}+h_{m}^{i}\right)\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(72)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In the case that <math alttext="J" class="ltx_Math" display="inline" id="A2.SS1.SSS1.p1.m6" intent=":literal"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math> has the symmetry,</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E73">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="J_{mn}^{ij}=J_{nm}^{ji}" class="ltx_Math" display="block" id="A2.E73.m1" intent=":literal"><semantics><mrow><msubsup><mi>J</mi><mrow><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi></mrow><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msubsup><mo>=</mo><msubsup><mi>J</mi><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi></mrow><mrow><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">J_{mn}^{ij}=J_{nm}^{ji}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(73)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">this reduces to,</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E74">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(x_{m}^{i}=1|\text{mb}(x^{i}))\propto\frac{1}{Z}e^{-\theta_{m}^{i}}" class="ltx_Math" display="block" id="A2.E74.m1" intent=":literal"><semantics><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>x</mi><mi>m</mi><mi>i</mi></msubsup><mo>=</mo><mrow><mn>1</mn><mo fence="false">|</mo><mrow><mtext>mb</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>∝</mo><mrow><mfrac><mn>1</mn><mi>Z</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mo>−</mo><msubsup><mi>θ</mi><mi>m</mi><mi>i</mi></msubsup></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">p(x_{m}^{i}=1|\text{mb}(x^{i}))\propto\frac{1}{Z}e^{-\theta_{m}^{i}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(74)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A2.SS1.SSS1.p2">
<table class="ltx_equation ltx_eqn_table" id="A2.E75">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\theta_{m}^{i}=\beta\left(2\sum_{j\in\text{mb}(x^{i}),n}J_{mn}^{ij}x_{n}^{j}+h_{m}^{i}\right)" class="ltx_Math" display="block" id="A2.E75.m1" intent=":literal"><semantics><mrow><msubsup><mi>θ</mi><mi>m</mi><mi>i</mi></msubsup><mo>=</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>∈</mo><mrow><mrow><mtext>mb</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>n</mi></mrow></mrow></munder><mrow><msubsup><mi>J</mi><mrow><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi></mrow><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>x</mi><mi>n</mi><mi>j</mi></msubsup></mrow></mrow></mrow><mo>+</mo><msubsup><mi>h</mi><mi>m</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\theta_{m}^{i}=\beta\left(2\sum_{j\in\text{mb}(x^{i}),n}J_{mn}^{ij}x_{n}^{j}+h_{m}^{i}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(75)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The parameters <math alttext="\theta" class="ltx_Math" display="inline" id="A2.SS1.SSS1.p2.m1" intent=":literal"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> are defined to make it clear that this is a <span class="ltx_text ltx_font_italic">softmax</span> distribution.</p>
</div>
<div class="ltx_para" id="A2.SS1.SSS1.p3">
<p class="ltx_p">Therefore, to build a hardware device that samples from Potts models using Gibbs sampling, one would have to build a softmax sampling circuit parameterized by a linear function of the model weights and neighbor states. Potts model sampling is slightly more complicated than Boltzmann machine sampling, but it is likely possible.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">B.1.2 </span>Gaussian-Bernoulli EBMs</h4>
<div class="ltx_para" id="A2.SS1.SSS2.p1">
<p class="ltx_p">Gaussian-Bernoulli EBMs extend Boltzmann machines to continuous, binary mixtures. In general, this type of model can have continuous-continuous, binary-binary, and binary-continuous interactions. For simplicity, if we consider only binary-continuous interactions, the energy function may be written as,</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E76">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E(v,h)=\sum_{i=1}^{N_{v}}\frac{(v_{i}-b_{i})^{2}}{2\sigma_{i}^{2}}-\sum_{i=1}^{N_{v}}\sum_{j=1}^{N_{h}}\frac{v_{i}W_{ij}h_{j}}{\sigma_{i}^{2}}-\sum_{j=1}^{N_{h}}c_{j}h_{j}," class="ltx_Math" display="block" id="A2.E76.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>v</mi><mo>,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mi>v</mi></msub></munderover><mfrac><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>−</mo><msub><mi>b</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mfrac></mrow><mo rspace="0.055em">−</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mi>v</mi></msub></munderover><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mi>h</mi></msub></munderover><mfrac><mrow><msub><mi>v</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>W</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>h</mi><mi>j</mi></msub></mrow><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></mfrac></mrow></mrow><mo rspace="0.055em">−</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mi>h</mi></msub></munderover><mrow><msub><mi>c</mi><mi>j</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>h</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">E(v,h)=\sum_{i=1}^{N_{v}}\frac{(v_{i}-b_{i})^{2}}{2\sigma_{i}^{2}}-\sum_{i=1}^{N_{v}}\sum_{j=1}^{N_{h}}\frac{v_{i}W_{ij}h_{j}}{\sigma_{i}^{2}}-\sum_{j=1}^{N_{h}}c_{j}h_{j},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(76)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="v_{i}\!\in\!\mathbb{R}" class="ltx_Math" display="inline" id="A2.SS1.SSS2.p1.m1" intent=":literal"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub><mo lspace="0.108em" rspace="0.108em">∈</mo><mi mathvariant="normal">ℝ</mi></mrow><annotation encoding="application/x-tex">v_{i}\!\in\!\mathbb{R}</annotation></semantics></math> are continuous variables with biases <math alttext="b_{i}" class="ltx_Math" display="inline" id="A2.SS1.SSS2.p1.m2" intent=":literal"><semantics><msub><mi>b</mi><mi>i</mi></msub><annotation encoding="application/x-tex">b_{i}</annotation></semantics></math> and variances <math alttext="\sigma_{i}^{2}" class="ltx_Math" display="inline" id="A2.SS1.SSS2.p1.m3" intent=":literal"><semantics><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\sigma_{i}^{2}</annotation></semantics></math>, <math alttext="h_{j}\!\in\!\{-1,1\}" class="ltx_Math" display="inline" id="A2.SS1.SSS2.p1.m4" intent=":literal"><semantics><mrow><msub><mi>h</mi><mi>j</mi></msub><mo lspace="0.108em" rspace="0.108em">∈</mo><mrow><mo stretchy="false">{</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">h_{j}\!\in\!\{-1,1\}</annotation></semantics></math> are binary variables with biases <math alttext="c_{j}" class="ltx_Math" display="inline" id="A2.SS1.SSS2.p1.m5" intent=":literal"><semantics><msub><mi>c</mi><mi>j</mi></msub><annotation encoding="application/x-tex">c_{j}</annotation></semantics></math>, and <math alttext="W_{ij}" class="ltx_Math" display="inline" id="A2.SS1.SSS2.p1.m6" intent=":literal"><semantics><msub><mi>W</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">W_{ij}</annotation></semantics></math> are interaction weights.</p>
</div>
<div class="ltx_para" id="A2.SS1.SSS2.p2">
<p class="ltx_p">Due to the structure of the energy function, the update rule for the continuous variables corresponds to drawing a sample from a Gaussian distribution with a mean that is a linear function of the neighbor states,</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E77">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p\!\left(v_{i}\,\middle|\,\text{mb}(v_{i})\right)=\mathcal{N}\!\bigl(\mu_{i},\sigma_{i}^{2}/\beta\bigr),\qquad\mu_{i}=b_{i}+\sigma_{i}^{2}\sum_{j\in\text{mb}(v_{i})}W_{ij}h_{j}." class="ltx_math_unparsed" display="block" id="A2.E77.m1" intent=":literal"><semantics><mrow><mpadded style="width:0.333em;" width="0.333em"><mi>p</mi></mpadded><mrow><mo>(</mo><msub><mi>v</mi><mi>i</mi></msub><mo lspace="0em" rspace="0.170em" stretchy="true">|</mo><mtext>mb</mtext><mrow><mo stretchy="false">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo>)</mo></mrow><mo>=</mo><mpadded style="width:0.798em;" width="0.798em"><mi class="ltx_font_mathcaligraphic">𝒩</mi></mpadded><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><msub><mi>μ</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup><mo>/</mo><mi>β</mi><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow><mo rspace="2.167em">,</mo><msub><mi>μ</mi><mi>i</mi></msub><mo>=</mo><msub><mi>b</mi><mi>i</mi></msub><mo>+</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup><munder><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>∈</mo><mrow><mtext>mb</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><msub><mi>W</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><msub><mi>h</mi><mi>j</mi></msub><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">p\!\left(v_{i}\,\middle|\,\text{mb}(v_{i})\right)=\mathcal{N}\!\bigl(\mu_{i},\sigma_{i}^{2}/\beta\bigr),\qquad\mu_{i}=b_{i}+\sigma_{i}^{2}\sum_{j\in\text{mb}(v_{i})}W_{ij}h_{j}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(77)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A2.SS1.SSS2.p3">
<p class="ltx_p">The binary update rule is similar to the rule for Boltzmann machines,</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E78">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p\!\left(h_{j}=1\,\middle|\,\text{mb}(h_{j})\right)=\sigma\!\left(2\beta\,\left(\sum_{i\in\text{mb}(h_{j})}\dfrac{v_{i}W_{ij}}{\sigma_{i}^{2}}+c_{j}\right)\right)" class="ltx_math_unparsed" display="block" id="A2.E78.m1" intent=":literal"><semantics><mrow><mpadded style="width:0.333em;" width="0.333em"><mi>p</mi></mpadded><mrow><mo>(</mo><msub><mi>h</mi><mi>j</mi></msub><mo>=</mo><mn>1</mn><mo lspace="0.170em" rspace="0.170em" stretchy="true">|</mo><mtext>mb</mtext><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mo>)</mo></mrow><mo>=</mo><mpadded style="width:0.437em;" width="0.437em"><mi>σ</mi></mpadded><mrow><mo>(</mo><mn>2</mn><mi>β</mi><mrow><mo lspace="0.170em">(</mo><munder><mo lspace="0em" movablelimits="false">∑</mo><mrow><mi>i</mi><mo>∈</mo><mrow><mtext>mb</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></munder><mfrac><mrow><msub><mi>v</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>W</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub></mrow><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></mfrac><mo>+</mo><msub><mi>c</mi><mi>j</mi></msub><mo>)</mo></mrow><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">p\!\left(h_{j}=1\,\middle|\,\text{mb}(h_{j})\right)=\sigma\!\left(2\beta\,\left(\sum_{i\in\text{mb}(h_{j})}\dfrac{v_{i}W_{ij}}{\sigma_{i}^{2}}+c_{j}\right)\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(78)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A2.SS1.SSS2.p4">
<p class="ltx_p">Hardware implementations of Gaussian-Bernoulli EBMs are more difficult than the strictly discrete models because the signals being passed during conditional sampling of the binary variables are continuous. To pass these continuous values, they must either be embedded into several discrete variables or an analog signaling system must be used. Both of these solutions would incur significant overhead compared to the purely discrete models.</p>
</div>
</section>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>A hardware architecture for denoising</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p">The denoising models used in this work exclusively modeled distributions of binary variables. The reverse process energy function (Eq. 7 in the main text) was implemented using a Boltzmann machine. The forward process energy function <math alttext="\mathcal{E}^{f}_{t-1}" class="ltx_Math" display="inline" id="A3.p1.m1" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>f</mi></msubsup><annotation encoding="application/x-tex">\mathcal{E}^{f}_{t-1}</annotation></semantics></math> was implemented using a simple set of pairwise couplings between <math alttext="x^{t}" class="ltx_Math" display="inline" id="A3.p1.m2" intent=":literal"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding="application/x-tex">x^{t}</annotation></semantics></math> (blue nodes) and <math alttext="x^{t-1}" class="ltx_Math" display="inline" id="A3.p1.m3" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">x^{t-1}</annotation></semantics></math> (green nodes). The marginal energy function <math alttext="\mathcal{E}^{\theta}_{t-1}" class="ltx_Math" display="inline" id="A3.p1.m4" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><annotation encoding="application/x-tex">\mathcal{E}^{\theta}_{t-1}</annotation></semantics></math> was implemented using a latent variable model (latent nodes are drawn in orange) with a sparse, local coupling structure.</p>
</div>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Implementation of the forward process energy function</h3>
<figure class="ltx_figure" id="A3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="324" id="A3.F9.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span class="ltx_text ltx_font_bold">Our hardware denoising architecture (a)</span> An example of a possible connectivity pattern as specified in Table. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3.T1" title="Table 1 ‣ C.2 Implementation of the marginal energy function ‣ Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a>. For clarity, the pattern is illustrated as applied to a single cell; however, in reality, the pattern is repeated for every cell in the grid. <span class="ltx_text ltx_font_bold">(b)</span> A graph for hardware denoising. The grid is subdivided at random into visible (green) nodes, representing the variables <math alttext="x^{t-1}" class="ltx_Math" display="inline" id="A3.F9.m5" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">x^{t-1}</annotation></semantics></math>, and latent (orange) nodes, representing <math alttext="z^{t-1}" class="ltx_Math" display="inline" id="A3.F9.m6" intent=":literal"><semantics><msup><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">z^{t-1}</annotation></semantics></math>. Each visible node <math alttext="x^{t-1}_{j}" class="ltx_Math" display="inline" id="A3.F9.m7" intent=":literal"><semantics><msubsup><mi>x</mi><mi>j</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">x^{t-1}_{j}</annotation></semantics></math> is coupled to a (blue) node carrying the value from the previous step of denoising <math alttext="x^{t}_{j}" class="ltx_Math" display="inline" id="A3.F9.m8" intent=":literal"><semantics><msubsup><mi>x</mi><mi>j</mi><mi>t</mi></msubsup><annotation encoding="application/x-tex">x^{t}_{j}</annotation></semantics></math> (note that these blue nodes stay fixed throughout the Gibbs sampling). </figcaption>
</figure>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p">From the exponential form of the discrete-variable forward process transition kernel given in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.E39" title="Equation 39 ‣ A.1.2 Discrete Variables ‣ A.1 Forward Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">39</span></a>), it is straightforward to derive a Boltzmann machine-style energy function that implements the forward process,</p>
<table class="ltx_equation ltx_eqn_table" id="A3.E79">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{E}^{f}_{t-1}=\sum_{i}\frac{\Gamma_{i}(t)}{2}x^{t}_{i}x^{t-1}_{i}" class="ltx_Math" display="block" id="A3.E79.m1" intent=":literal"><semantics><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>f</mi></msubsup><mo rspace="0.111em">=</mo><mrow><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder><mrow><mfrac><mrow><msub><mi mathvariant="normal">Γ</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}^{f}_{t-1}=\sum_{i}\frac{\Gamma_{i}(t)}{2}x^{t}_{i}x^{t-1}_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(79)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="x_{t}[i]\&gt;\in\{-1,1\}" class="ltx_Math" display="inline" id="A3.SS1.p1.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>x</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>i</mi><mo rspace="0.220em" stretchy="false">]</mo></mrow></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">x_{t}[i]\&gt;\in\{-1,1\}</annotation></semantics></math> indicates the <math alttext="i^{th}" class="ltx_Math" display="inline" id="A3.SS1.p1.m2" intent=":literal"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>h</mi></mrow></msup><annotation encoding="application/x-tex">i^{th}</annotation></semantics></math> element of the vector of random variables <math alttext="x_{t}" class="ltx_Math" display="inline" id="A3.SS1.p1.m3" intent=":literal"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math> as usual.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Implementation of the marginal energy function</h3>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p">We use a Boltzmann machine based on a grid graph to implement the marginal energy function. Our grids have both nearest-neighbor and long-range skip connections. A simple example of this is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3.F9" title="Figure 9 ‣ C.1 Implementation of the forward process energy function ‣ Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">9</span></a> (a). This connectivity pattern is tiled such that every node in the bulk of the grid has the same connectivity to its neighbors. At the boundaries, connections that extend beyond the grid’s edges are not formed.</p>
</div>
<div class="ltx_para" id="A3.SS2.p2">
<p class="ltx_p">Within the grid, we randomly choose some subset of the nodes to represent the data variables <math alttext="x_{t-1}" class="ltx_Math" display="inline" id="A3.SS2.p2.m1" intent=":literal"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">x_{t-1}</annotation></semantics></math>. The remaining nodes then implement the latent variable <math alttext="z_{t-1}" class="ltx_Math" display="inline" id="A3.SS2.p2.m2" intent=":literal"><semantics><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">z_{t-1}</annotation></semantics></math>. The grid is, therefore, a deep Boltzmann machine with a sparse connectivity structure and multiple hidden layers.</p>
</div>
<div class="ltx_para" id="A3.SS2.p3">
<p class="ltx_p">We use a particular set of connectivity patterns in the experiments in this article, which are specified in Table. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3.T1" title="Table 1 ‣ C.2 Implementation of the marginal energy function ‣ Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a>. We say that node <math alttext="(x,y)" class="ltx_Math" display="inline" id="A3.SS2.p3.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x,y)</annotation></semantics></math> has a connection rule of the form <math alttext="(a,b)" class="ltx_Math" display="inline" id="A3.SS2.p3.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(a,b)</annotation></semantics></math> if it is connected to nodes at positions <math alttext="(x+a,y+b),(x-b,y+a),(x-a,y-b),(x+b,y-a)" class="ltx_Math" display="inline" id="A3.SS2.p3.m3" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo>+</mo><mi>a</mi></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mi>b</mi></mrow><mo stretchy="false">)</mo></mrow><mo>,</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo>−</mo><mi>b</mi></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mi>a</mi></mrow><mo stretchy="false">)</mo></mrow><mo>,</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo>−</mo><mi>a</mi></mrow><mo>,</mo><mrow><mi>y</mi><mo>−</mo><mi>b</mi></mrow><mo stretchy="false">)</mo></mrow><mo>,</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo>+</mo><mi>b</mi></mrow><mo>,</mo><mrow><mi>y</mi><mo>−</mo><mi>a</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(x+a,y+b),(x-b,y+a),(x-a,y-b),(x+b,y-a)</annotation></semantics></math>, so each connection rule adds up to 4 edges from this node.</p>
</div>
<figure class="ltx_table" id="A3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_ll ltx_border_t" style="padding-bottom:2.15277pt;">Pattern</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;">Connectivity</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_ll ltx_border_tt"><math alttext="G_{8}" class="ltx_Math" display="inline" id="A3.T1.m1" intent=":literal"><semantics><msub><mi>G</mi><mn>8</mn></msub><annotation encoding="application/x-tex">G_{8}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><math alttext="(0,1),\;(4,1)" class="ltx_Math" display="inline" id="A3.T1.m2" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>4</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(0,1),\;(4,1)</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_ll ltx_border_t"><math alttext="G_{12}" class="ltx_Math" display="inline" id="A3.T1.m3" intent=":literal"><semantics><msub><mi>G</mi><mn>12</mn></msub><annotation encoding="application/x-tex">G_{12}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><math alttext="(0,1),\;(4,1),\;(9,10)" class="ltx_Math" display="inline" id="A3.T1.m4" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>4</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>9</mn><mo>,</mo><mn>10</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(0,1),\;(4,1),\;(9,10)</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_ll ltx_border_t"><math alttext="G_{16}" class="ltx_Math" display="inline" id="A3.T1.m5" intent=":literal"><semantics><msub><mi>G</mi><mn>16</mn></msub><annotation encoding="application/x-tex">G_{16}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><math alttext="(0,1),\;(4,1),\;(8,7),\;(14,9)" class="ltx_Math" display="inline" id="A3.T1.m6" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>4</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>8</mn><mo>,</mo><mn>7</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>14</mn><mo>,</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(0,1),\;(4,1),\;(8,7),\;(14,9)</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_ll ltx_border_t"><math alttext="G_{20}" class="ltx_Math" display="inline" id="A3.T1.m7" intent=":literal"><semantics><msub><mi>G</mi><mn>20</mn></msub><annotation encoding="application/x-tex">G_{20}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><math alttext="(0,1),\;(4,1),\;(3,6),\;(8,7),\;(14,9)" class="ltx_Math" display="inline" id="A3.T1.m8" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>4</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>3</mn><mo>,</mo><mn>6</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>8</mn><mo>,</mo><mn>7</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>14</mn><mo>,</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(0,1),\;(4,1),\;(3,6),\;(8,7),\;(14,9)</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_ll ltx_border_t" style="padding-bottom:4.30554pt;"><math alttext="G_{24}" class="ltx_Math" display="inline" id="A3.T1.m9" intent=":literal"><semantics><msub><mi>G</mi><mn>24</mn></msub><annotation encoding="application/x-tex">G_{24}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t" style="padding-bottom:4.30554pt;"><math alttext="(0,1),\;(1,2),\;(4,1),\;(3,6),\;(8,7),\;(14,9)" class="ltx_Math" display="inline" id="A3.T1.m10" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>4</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>3</mn><mo>,</mo><mn>6</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>8</mn><mo>,</mo><mn>7</mn><mo stretchy="false">)</mo></mrow><mo rspace="0.447em">,</mo><mrow><mo stretchy="false">(</mo><mn>14</mn><mo>,</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(0,1),\;(1,2),\;(4,1),\;(3,6),\;(8,7),\;(14,9)</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Edges (ordered pairs) associated with graphs of various degrees.</figcaption>
</figure>
<div class="ltx_para" id="A3.SS2.p4">
<p class="ltx_p">As explicitly stated in Eq. 7 of the article, our variational approximation to the reverse process conditional has an energy function that is the sum of the forward process energy function and the marginal energy function. Physically, this corresponds to adding nodes to our grid that implement <math alttext="x_{t}" class="ltx_Math" display="inline" id="A3.SS2.p4.m1" intent=":literal"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math>, which are connected pairwise to the data nodes implementing <math alttext="x_{t-1}" class="ltx_Math" display="inline" id="A3.SS2.p4.m2" intent=":literal"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">x_{t-1}</annotation></semantics></math> via the coupling defined in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3.E79" title="Equation 79 ‣ C.1 Implementation of the forward process energy function ‣ Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">79</span></a>). This connectivity is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3.F9" title="Figure 9 ‣ C.1 Implementation of the forward process energy function ‣ Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">9</span></a> (b).</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Energetic analysis of the hardware architecture</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p">Our RNG design uses only transistors and can integrate tightly with other traditional circuit components on a chip to implement a large-scale sampling system. Since there are no exotic components involved that introduce unknown integration barriers, it is straightforward to build a simple physical model to predict how this device utilizes energy.</p>
</div>
<div class="ltx_para" id="A4.p2">
<p class="ltx_p">The performance of the device can be understood by analyzing the unit sampling cell that lives on each node of the PGM implemented by the hardware. The function of this cell is to implement the Boltzmann machine conditional update, as given in Eq. 11 in the main text.</p>
</div>
<div class="ltx_para" id="A4.p3">
<p class="ltx_p">There are many possible designs for the sampling cell. The design considered here utilizes a linear analog circuit to combine the neighboring states and model weights, producing a control voltage for an RNG. This RNG then produces a random bit that is biased by a sigmoidal function of the control voltage. This updated state is then broadcast back to the neighbors. The cell must also support initialization and readout (get/set state operations). A schematic of a unit cell is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A2.F8" title="Figure 8 ‣ Appendix B Hardware accelerators for EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<div class="ltx_para" id="A4.p4">
<p class="ltx_p">We provide experimental measurements of our novel RNG circuitry in the main text, which establish that random bits can be produced at a rate of <math alttext="\tau_{rng}^{-1}\approx\&gt;10\&gt;\text{MHz}" class="ltx_Math" display="inline" id="A4.p4.m1" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>g</mi></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mo>≈</mo><mrow><mn> 10</mn><mo lspace="0.220em" rspace="0em">​</mo><mtext>MHz</mtext></mrow></mrow><annotation encoding="application/x-tex">\tau_{rng}^{-1}\approx\&gt;10\&gt;\text{MHz}</annotation></semantics></math> using <math alttext="\sim 350\text{aJ}" class="ltx_Math" display="inline" id="A4.p4.m2" intent=":literal"><semantics><mrow><mi></mi><mo>∼</mo><mrow><mn>350</mn><mo lspace="0em" rspace="0em">​</mo><mtext>aJ</mtext></mrow></mrow><annotation encoding="application/x-tex">\sim 350\text{aJ}</annotation></semantics></math> of energy per bit. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A10.F15" title="Figure 15 ‣ Appendix J Some details on our RNG ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">15</span></a> (a) shows an output voltage waveform from the RNG circuit. It wanders randomly between high and low states. Critically, the bias of the RNG circuit (the probability of finding it in the high or low state) is a sigmoidal function of its control voltage, which allows for a straightforward implementation of the conditional update using linear circuitry.</p>
</div>
<figure class="ltx_figure" id="A4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="325" id="A4.F10.g1" src="x10.png" width="498"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span class="ltx_text ltx_font_bold">A schematic of a possible Boltzmann machine sampling cell</span> A linear resistor network computes a biasing voltage given the sign-corrected neighbor states <math alttext="y_{n}=x_{n}\oplus s_{n}" class="ltx_Math" display="inline" id="A4.F10.m2" intent=":literal"><semantics><mrow><msub><mi>y</mi><mi>n</mi></msub><mo>=</mo><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>⊕</mo><msub><mi>s</mi><mi>n</mi></msub></mrow></mrow><annotation encoding="application/x-tex">y_{n}=x_{n}\oplus s_{n}</annotation></semantics></math>. The output of this circuit biases an RNG that responds in a sigmoidal manner. This RNG processes freely when the clock is low and latches to a state when the clock is high. Upon the clock going high, the sampled state is broadcasted to the neighbors of the cell over wires.</figcaption>
</figure>
<div class="ltx_para" id="A4.p5">
<p class="ltx_p">The size of the RNG circuit can be used to anchor the dimensions of a future large-scale Gibbs sampling device. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A10.F15" title="Figure 15 ‣ Appendix J Some details on our RNG ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">15</span></a> (b), the RNG itself involves around 10 transistors and takes up <math alttext="\sim\&gt;3\mu m\times 3\mu m" class="ltx_Math" display="inline" id="A4.p5.m1" intent=":literal"><semantics><mrow><mi></mi><mo>∼</mo><mrow><mrow><mrow><mn> 3</mn><mo lspace="0em" rspace="0em">​</mo><mi>μ</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mn>3</mn></mrow><mo lspace="0em" rspace="0em">​</mo><mi>μ</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi></mrow></mrow><annotation encoding="application/x-tex">\sim\&gt;3\mu m\times 3\mu m</annotation></semantics></math> on the die. It is reasonable to imagine that the whole sampling cell could fit in <math alttext="4\times" class="ltx_math_unparsed" display="inline" id="A4.p5.m2" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">4\times</annotation></semantics></math> this area and have a side length of <math alttext="6\mu m" class="ltx_Math" display="inline" id="A4.p5.m3" intent=":literal"><semantics><mrow><mn>6</mn><mo lspace="0em" rspace="0em">​</mo><mi>μ</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">6\mu m</annotation></semantics></math>. Given this area, a <math alttext="1000\times 1000" class="ltx_Math" display="inline" id="A4.p5.m4" intent=":literal"><semantics><mrow><mn>1000</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">1000\times 1000</annotation></semantics></math> grid of sampling cells would fit within a <math alttext="6\text{mm}\times 6\text{mm}" class="ltx_Math" display="inline" id="A4.p5.m5" intent=":literal"><semantics><mrow><mrow><mrow><mn>6</mn><mo lspace="0em" rspace="0em">​</mo><mtext>mm</mtext></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mn>6</mn></mrow><mo lspace="0em" rspace="0em">​</mo><mtext>mm</mtext></mrow><annotation encoding="application/x-tex">6\text{mm}\times 6\text{mm}</annotation></semantics></math> chip.</p>
</div>
<div class="ltx_para" id="A4.p6">
<p class="ltx_p">Building on the measured characteristics of our RNG, we will now develop simple physical models for the remaining components of the sampling system. These models can then be combined to estimate the energy consumption of the diffusion models developed in this article running on our hardware.</p>
</div>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Biasing circuit</h3>
<div class="ltx_para" id="A4.SS1.p1">
<p class="ltx_p">The multiply-accumulation of the model weights and neighbor states can be performed using a resistor network, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.F10" title="Figure 10 ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">10</span></a>. The dynamics of this resistor network are described by the differential equation,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E80">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\sum_{j=1}^{n+2}G_{j}\left(V_{dd}\&gt;y_{j}-V_{b}\right)=C\frac{dV_{b}}{dt}" class="ltx_Math" display="block" id="A4.E80.m1" intent=":literal"><semantics><mrow><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow></munderover><mrow><msub><mi>G</mi><mi>j</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mrow><msub><mi>V</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>y</mi><mi>j</mi></msub></mrow><mo>−</mo><msub><mi>V</mi><mi>b</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mi>C</mi><mo lspace="0em" rspace="0em">​</mo><mfrac><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>V</mi><mi>b</mi></msub></mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mfrac></mrow></mrow><annotation encoding="application/x-tex">\sum_{j=1}^{n+2}G_{j}\left(V_{dd}\&gt;y_{j}-V_{b}\right)=C\frac{dV_{b}}{dt}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(80)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="y_{i}=x_{i}\oplus s_{i}" class="ltx_Math" display="inline" id="A4.SS1.p1.m1" intent=":literal"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>⊕</mo><msub><mi>s</mi><mi>i</mi></msub></mrow></mrow><annotation encoding="application/x-tex">y_{i}=x_{i}\oplus s_{i}</annotation></semantics></math> is the XOR of the neighbor state <math alttext="x_{i}" class="ltx_Math" display="inline" id="A4.SS1.p1.m2" intent=":literal"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math> with a sign bit <math alttext="s_{i}" class="ltx_Math" display="inline" id="A4.SS1.p1.m3" intent=":literal"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding="application/x-tex">s_{i}</annotation></semantics></math>. There are <math alttext="n" class="ltx_Math" display="inline" id="A4.SS1.p1.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> variable neighbor states and two fixed inputs (<math alttext="y_{n+1}=1" class="ltx_Math" display="inline" id="A4.SS1.p1.m5" intent=":literal"><semantics><mrow><msub><mi>y</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y_{n+1}=1</annotation></semantics></math>, <math alttext="y_{n+2}=0" class="ltx_Math" display="inline" id="A4.SS1.p1.m6" intent=":literal"><semantics><mrow><msub><mi>y</mi><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y_{n+2}=0</annotation></semantics></math>), which are important for implementing the fixed bias term in the conditional update. <math alttext="V_{dd}" class="ltx_Math" display="inline" id="A4.SS1.p1.m7" intent=":literal"><semantics><msub><mi>V</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><annotation encoding="application/x-tex">V_{dd}</annotation></semantics></math> is the supply voltage and <math alttext="V_{b}" class="ltx_Math" display="inline" id="A4.SS1.p1.m8" intent=":literal"><semantics><msub><mi>V</mi><mi>b</mi></msub><annotation encoding="application/x-tex">V_{b}</annotation></semantics></math> is the output voltage that biases the RNG. <math alttext="G_{i}" class="ltx_Math" display="inline" id="A4.SS1.p1.m9" intent=":literal"><semantics><msub><mi>G</mi><mi>i</mi></msub><annotation encoding="application/x-tex">G_{i}</annotation></semantics></math> represents the conductance of the resistor corresponding to the <math alttext="i^{th}" class="ltx_Math" display="inline" id="A4.SS1.p1.m10" intent=":literal"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>h</mi></mrow></msup><annotation encoding="application/x-tex">i^{th}</annotation></semantics></math> input. The capacitance <math alttext="C" class="ltx_Math" display="inline" id="A4.SS1.p1.m11" intent=":literal"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> represents the parasitic capacitance to ground associated with any real implementation of this circuit and is critical to forming realistic estimates of speed and energy consumption. Realistic values for an implementation of this circuit in our transistor process are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.F11" title="Figure 11 ‣ D.2 Local communication ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">11</span></a> (a).</p>
</div>
<div class="ltx_para" id="A4.SS1.p2">
<p class="ltx_p">Since this equation is first order, the dynamics exponentially relax to some fixed point <math alttext="V_{b}^{\infty}" class="ltx_Math" display="inline" id="A4.SS1.p2.m1" intent=":literal"><semantics><msubsup><mi>V</mi><mi>b</mi><mi mathvariant="normal">∞</mi></msubsup><annotation encoding="application/x-tex">V_{b}^{\infty}</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E81">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="V_{b}(t)=c\&gt;e^{-t/\tau_{bias}}+V_{b}^{\infty}" class="ltx_Math" display="block" id="A4.E81.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>V</mi><mi>b</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>c</mi><mo lspace="0.220em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mo>−</mo><mrow><mi>t</mi><mo>/</mo><msub><mi>τ</mi><mrow><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></msub></mrow></mrow></msup></mrow><mo>+</mo><msubsup><mi>V</mi><mi>b</mi><mi mathvariant="normal">∞</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">V_{b}(t)=c\&gt;e^{-t/\tau_{bias}}+V_{b}^{\infty}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(81)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">the time constant <math alttext="\tau_{bias}" class="ltx_Math" display="inline" id="A4.SS1.p2.m2" intent=":literal"><semantics><msub><mi>τ</mi><mrow><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></msub><annotation encoding="application/x-tex">\tau_{bias}</annotation></semantics></math> is,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E82">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tau_{bias}=\frac{C}{G_{\Sigma}}" class="ltx_Math" display="block" id="A4.E82.m1" intent=":literal"><semantics><mrow><msub><mi>τ</mi><mrow><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></msub><mo>=</mo><mfrac><mi>C</mi><msub><mi>G</mi><mi mathvariant="normal">Σ</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">\tau_{bias}=\frac{C}{G_{\Sigma}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(82)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and the fixed point is,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E83">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="V_{b}^{\infty}=\sum_{j=1}^{n+2}\frac{G_{j}}{G_{\Sigma}}V_{dd}y_{j}" class="ltx_Math" display="block" id="A4.E83.m1" intent=":literal"><semantics><mrow><msubsup><mi>V</mi><mi>b</mi><mi mathvariant="normal">∞</mi></msubsup><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow></munderover><mrow><mfrac><msub><mi>G</mi><mi>j</mi></msub><msub><mi>G</mi><mi mathvariant="normal">Σ</mi></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>V</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>y</mi><mi>j</mi></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">V_{b}^{\infty}=\sum_{j=1}^{n+2}\frac{G_{j}}{G_{\Sigma}}V_{dd}y_{j}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(83)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where the total conductance <math alttext="G_{\Sigma}" class="ltx_Math" display="inline" id="A4.SS1.p2.m3" intent=":literal"><semantics><msub><mi>G</mi><mi mathvariant="normal">Σ</mi></msub><annotation encoding="application/x-tex">G_{\Sigma}</annotation></semantics></math> is,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E84">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="G_{\Sigma}=\sum_{j=1}^{n+2}G_{j}" class="ltx_Math" display="block" id="A4.E84.m1" intent=":literal"><semantics><mrow><msub><mi>G</mi><mi mathvariant="normal">Σ</mi></msub><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow></munderover><msub><mi>G</mi><mi>j</mi></msub></mrow></mrow><annotation encoding="application/x-tex">G_{\Sigma}=\sum_{j=1}^{n+2}G_{j}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(84)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A4.SS1.p3">
<p class="ltx_p">The RNG has a bias curve which takes the form,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E85">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{P}\left(x_{i}=1\right)=\sigma\left(\frac{V_{b}}{V_{s}}-\phi\right)" class="ltx_Math" display="block" id="A4.E85.m1" intent=":literal"><semantics><mrow><mrow><mi mathvariant="normal">ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mfrac><msub><mi>V</mi><mi>b</mi></msub><msub><mi>V</mi><mi>s</mi></msub></mfrac><mo>−</mo><mi>ϕ</mi></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbb{P}\left(x_{i}=1\right)=\sigma\left(\frac{V_{b}}{V_{s}}-\phi\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(85)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">inserting Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.E83" title="Equation 83 ‣ D.1 Biasing circuit ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">83</span></a>) and expanding the term inside the sigmoid,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E86">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{V_{b}}{V_{s}}-\phi=\sum_{j=1}^{n}\frac{G_{j}}{G_{\Sigma}}\frac{V_{dd}}{V_{s}}\left(x_{j}\oplus s_{j}\right)+\left[\frac{G_{n+1}}{G_{\Sigma}}\frac{V_{dd}}{V_{s}}-\phi\right]" class="ltx_Math" display="block" id="A4.E86.m1" intent=":literal"><semantics><mrow><mrow><mfrac><msub><mi>V</mi><mi>b</mi></msub><msub><mi>V</mi><mi>s</mi></msub></mfrac><mo>−</mo><mi>ϕ</mi></mrow><mo rspace="0.111em">=</mo><mrow><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><msub><mi>G</mi><mi>j</mi></msub><msub><mi>G</mi><mi mathvariant="normal">Σ</mi></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><mfrac><msub><mi>V</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><msub><mi>V</mi><mi>s</mi></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>⊕</mo><msub><mi>s</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mo>[</mo><mrow><mrow><mfrac><msub><mi>G</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><msub><mi>G</mi><mi mathvariant="normal">Σ</mi></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><mfrac><msub><mi>V</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><msub><mi>V</mi><mi>s</mi></msub></mfrac></mrow><mo>−</mo><mi>ϕ</mi></mrow><mo>]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\frac{V_{b}}{V_{s}}-\phi=\sum_{j=1}^{n}\frac{G_{j}}{G_{\Sigma}}\frac{V_{dd}}{V_{s}}\left(x_{j}\oplus s_{j}\right)+\left[\frac{G_{n+1}}{G_{\Sigma}}\frac{V_{dd}}{V_{s}}-\phi\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(86)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">by comparison to the Boltzmann machine conditional, we can see that the first term implements the model weights (which can be positive or negative given an appropriate setting of the sign bit <math alttext="s_{j}" class="ltx_Math" display="inline" id="A4.SS1.p3.m1" intent=":literal"><semantics><msub><mi>s</mi><mi>j</mi></msub><annotation encoding="application/x-tex">s_{j}</annotation></semantics></math>), and the second term implements a bias.</p>
</div>
<div class="ltx_para" id="A4.SS1.p4">
<p class="ltx_p">The static power drawn by this circuit can be written in the form,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E87">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P^{\infty}=\frac{C}{\tau_{bias}}V_{dd}^{2}(1-\gamma)\gamma" class="ltx_Math" display="block" id="A4.E87.m1" intent=":literal"><semantics><mrow><msup><mi>P</mi><mi mathvariant="normal">∞</mi></msup><mo>=</mo><mrow><mfrac><mi>C</mi><msub><mi>τ</mi><mrow><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>V</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>γ</mi></mrow></mrow><annotation encoding="application/x-tex">P^{\infty}=\frac{C}{\tau_{bias}}V_{dd}^{2}(1-\gamma)\gamma</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(87)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="0\leq\gamma\leq 1" class="ltx_Math" display="inline" id="A4.SS1.p4.m1" intent=":literal"><semantics><mrow><mn>0</mn><mo>≤</mo><mi>γ</mi><mo>≤</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0\leq\gamma\leq 1</annotation></semantics></math> is the input-dependent constant,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E88">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\gamma=\sum_{j=1}^{n+2}\frac{G_{j}}{G_{\Sigma}}y_{j}" class="ltx_Math" display="block" id="A4.E88.m1" intent=":literal"><semantics><mrow><mi>γ</mi><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow></munderover><mrow><mfrac><msub><mi>G</mi><mi>j</mi></msub><msub><mi>G</mi><mi mathvariant="normal">Σ</mi></msub></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>y</mi><mi>j</mi></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\gamma=\sum_{j=1}^{n+2}\frac{G_{j}}{G_{\Sigma}}y_{j}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(88)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A4.SS1.p5">
<p class="ltx_p">This fixed point must be held while the noise generator relaxes, which means that the energetic cost of the biasing circuit is approximately,</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A11.EGx2">
<tbody id="A4.E89"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell" colspan="2"><math alttext="\displaystyle\begin{split}E_{bias}\approx&amp;P^{\infty}\tau_{rng}\\
=&amp;\&gt;C\frac{\tau_{rng}}{\tau_{bias}}V_{dd}^{2}(1-\gamma)\gamma\end{split}" class="ltx_Math" display="inline" id="A4.E89.m1" intent=":literal"><semantics><mtable columnspacing="0pt" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><mrow><msub><mi>E</mi><mrow><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></msub><mo>≈</mo><mi></mi></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><msup><mi>P</mi><mi mathvariant="normal">∞</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>τ</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>g</mi></mrow></msub></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><mo>=</mo></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi>C</mi><mo lspace="0em" rspace="0em">​</mo><mstyle displaystyle="true"><mfrac><msub><mi>τ</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>g</mi></mrow></msub><msub><mi>τ</mi><mrow><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></msub></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>V</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>γ</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>γ</mi></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\displaystyle\begin{split}E_{bias}\approx&amp;P^{\infty}\tau_{rng}\\
=&amp;\&gt;C\frac{\tau_{rng}}{\tau_{bias}}V_{dd}^{2}(1-\gamma)\gamma\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(89)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This is maximized for <math alttext="\gamma=\frac{1}{2}" class="ltx_Math" display="inline" id="A4.SS1.p5.m1" intent=":literal"><semantics><mrow><mi>γ</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\gamma=\frac{1}{2}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A4.SS1.p6">
<p class="ltx_p">To avoid slowing down the sampling machine, <math alttext="\frac{\tau_{rng}}{\tau_{bias}}\gg 1" class="ltx_Math" display="inline" id="A4.SS1.p6.m1" intent=":literal"><semantics><mrow><mfrac><msub><mi>τ</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>g</mi></mrow></msub><msub><mi>τ</mi><mrow><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></msub></mfrac><mo>≫</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\frac{\tau_{rng}}{\tau_{bias}}\gg 1</annotation></semantics></math>. As such, ignoring the energy spent charging the capacitor <math alttext="\sim\frac{1}{2}CV_{b}^{2}" class="ltx_Math" display="inline" id="A4.SS1.p6.m2" intent=":literal"><semantics><mrow><mi></mi><mo>∼</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>C</mi><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>V</mi><mi>b</mi><mn>2</mn></msubsup></mrow></mrow><annotation encoding="application/x-tex">\sim\frac{1}{2}CV_{b}^{2}</annotation></semantics></math> will not significantly affect the results, and the approximation made in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.E89" title="Equation 89 ‣ D.1 Biasing circuit ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">89</span></a>) should be accurate. The energy consumed by the bias circuit is primarily due to static power dissipation.</p>
</div>
</section>
<section class="ltx_subsection" id="A4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Local communication</h3>
<div class="ltx_para" id="A4.SS2.p1">
<p class="ltx_p">Another significant source of energy consumption is the communication of state information between neighboring cells. In most electronic devices, signals are communicated by charging and discharging wires. Charging a wire requires the energy input,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E90">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E_{\text{charge}}=\frac{1}{2}C_{\text{wire}}V_{\text{sig}}^{2}" class="ltx_Math" display="block" id="A4.E90.m1" intent=":literal"><semantics><mrow><msub><mi>E</mi><mtext>charge</mtext></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>C</mi><mtext>wire</mtext></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>V</mi><mtext>sig</mtext><mn>2</mn></msubsup></mrow></mrow><annotation encoding="application/x-tex">E_{\text{charge}}=\frac{1}{2}C_{\text{wire}}V_{\text{sig}}^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(90)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="C_{\text{wire}}" class="ltx_Math" display="inline" id="A4.SS2.p1.m1" intent=":literal"><semantics><msub><mi>C</mi><mtext>wire</mtext></msub><annotation encoding="application/x-tex">C_{\text{wire}}</annotation></semantics></math> is the capacitance associated with the wire, which grows with its length, and <math alttext="V_{\text{sig}}" class="ltx_Math" display="inline" id="A4.SS2.p1.m2" intent=":literal"><semantics><msub><mi>V</mi><mtext>sig</mtext></msub><annotation encoding="application/x-tex">V_{\text{sig}}</annotation></semantics></math> is the signaling voltage level.</p>
</div>
<figure class="ltx_figure" id="A4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="363" id="A4.F11.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><span class="ltx_text ltx_font_bold">Parameters for the energy model (a)</span> The parasitic capacitance associated with the output node of the biasing circuit for various numbers of neighbors. These capacitances were estimated using the PDK and a layout for a real transistor implementation of the biasing circuit. <span class="ltx_text ltx_font_bold">(b)</span> The capacitance associated with routing wires of various lengths and geometry in our process, extracted using the PDK. <span class="ltx_text ltx_font_bold">(c)</span> The energy required for a cell to signal to all of its neighbors as a function of signaling voltage for various connectivity patterns. This energy was calculated using the routing capacitance data from (b).</figcaption>
</figure>
<div class="ltx_para" id="A4.SS2.p2">
<p class="ltx_p">Given the connectivity patterns shown in table  <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3.T1" title="Table 1 ‣ C.2 Implementation of the marginal energy function ‣ Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">1</span></a>, it is possible to estimate the total capacitance <math alttext="C_{n}" class="ltx_Math" display="inline" id="A4.SS2.p2.m1" intent=":literal"><semantics><msub><mi>C</mi><mi>n</mi></msub><annotation encoding="application/x-tex">C_{n}</annotation></semantics></math> associated with the wire connecting a node to all of its neighbors,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E91">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="C_{n}=4\eta\ell\sum_{i}\sqrt{a_{i}^{2}+b_{i}^{2}}" class="ltx_Math" display="block" id="A4.E91.m1" intent=":literal"><semantics><mrow><msub><mi>C</mi><mi>n</mi></msub><mo>=</mo><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">ℓ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder><msqrt><mrow><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>b</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></mrow></mrow><annotation encoding="application/x-tex">C_{n}=4\eta\ell\sum_{i}\sqrt{a_{i}^{2}+b_{i}^{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(91)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\ell\approx 6\mu m" class="ltx_Math" display="inline" id="A4.SS2.p2.m2" intent=":literal"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo>≈</mo><mrow><mn>6</mn><mo lspace="0em" rspace="0em">​</mo><mi>μ</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi></mrow></mrow><annotation encoding="application/x-tex">\ell\approx 6\mu m</annotation></semantics></math> is the sampling cell side length, and <math alttext="\eta\approx 350\text{aF}/\mu m" class="ltx_Math" display="inline" id="A4.SS2.p2.m3" intent=":literal"><semantics><mrow><mi>η</mi><mo>≈</mo><mrow><mrow><mrow><mn>350</mn><mo lspace="0em" rspace="0em">​</mo><mtext>aF</mtext></mrow><mo>/</mo><mi>μ</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi>m</mi></mrow></mrow><annotation encoding="application/x-tex">\eta\approx 350\text{aF}/\mu m</annotation></semantics></math> is the wire capacitance per unit length in our process, see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.F11" title="Figure 11 ‣ D.2 Local communication ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">11</span></a> (b). <math alttext="a_{i}" class="ltx_Math" display="inline" id="A4.SS2.p2.m4" intent=":literal"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding="application/x-tex">a_{i}</annotation></semantics></math> and <math alttext="b_{i}" class="ltx_Math" display="inline" id="A4.SS2.p2.m5" intent=":literal"><semantics><msub><mi>b</mi><mi>i</mi></msub><annotation encoding="application/x-tex">b_{i}</annotation></semantics></math> are the <math alttext="x" class="ltx_Math" display="inline" id="A4.SS2.p2.m6" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and <math alttext="y" class="ltx_Math" display="inline" id="A4.SS2.p2.m7" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> components of the <math alttext="i^{th}" class="ltx_Math" display="inline" id="A4.SS2.p2.m8" intent=":literal"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>h</mi></mrow></msup><annotation encoding="application/x-tex">i^{th}</annotation></semantics></math> connection rule, as described in section  <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3.SS2" title="C.2 Implementation of the marginal energy function ‣ Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">C.C.2</span></a>.</p>
</div>
<div class="ltx_para" id="A4.SS2.p3">
<p class="ltx_p">The charging energy Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.E90" title="Equation 90 ‣ D.2 Local communication ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">90</span></a>) is plotted as a function of signaling voltage for various connectivity patterns in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.F11" title="Figure 11 ‣ D.2 Local communication ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">11</span></a> (b).</p>
</div>
</section>
<section class="ltx_subsection" id="A4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.3 </span>Global communication</h3>
<div class="ltx_para" id="A4.SS3.p1">
<p class="ltx_p">Several systems on the chip require signals to be transmitted from some central location out to the individual sampling cells. This communication involves sending signals over long wires with a large capacitance, which is energetically expensive. Here, the cost of this global communication will be taken into consideration.</p>
</div>
<section class="ltx_subsubsection" id="A4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">D.3.1 </span>Clocking</h4>
<div class="ltx_para" id="A4.SS3.SSS1.p1">
<p class="ltx_p">Although it is possible in principle to implement Gibbs sampling completely asynchronously, in practice, it is more efficient to implement standard chromatic Gibbs sampling with a global clock. A global clock requires a signal to be distributed from a central clock circuit to every sampling cell on the chip. This signal distribution is typically accomplished using a clock tree, a branching circuit designed to minimize timing inconsistencies between disparate circuit elements.</p>
</div>
<div class="ltx_para" id="A4.SS3.SSS1.p2">
<p class="ltx_p">To simplify the analysis, we will consider a simple clock distribution scheme in which the clock is distributed by lines that run the entire length of each row in the grid. The total length of the wires used for clock distribution in this scheme is,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E92">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{clock}=N\&gt;L" class="ltx_Math" display="block" id="A4.E92.m1" intent=":literal"><semantics><mrow><msub><mi>L</mi><mrow><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub><mo>=</mo><mrow><mi>N</mi><mo lspace="0.220em" rspace="0em">​</mo><mi>L</mi></mrow></mrow><annotation encoding="application/x-tex">L_{clock}=N\&gt;L</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(92)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="N" class="ltx_Math" display="inline" id="A4.SS3.SSS1.p2.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> is the number of rows in the grid, and <math alttext="L" class="ltx_Math" display="inline" id="A4.SS3.SSS1.p2.m2" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> is the length of a row. Given this length, the energetic cost of a clock pulse can be calculated using Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.E90" title="Equation 90 ‣ D.2 Local communication ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">90</span></a>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="A4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">D.3.2 </span>Initialization and readout</h4>
<div class="ltx_para" id="A4.SS3.SSS2.p1">
<p class="ltx_p">A sampling program begins by initializing every sampling cell to a specific state and ends by reading out the state of a subset of the cells for use off-chip. Both of these operations require bits to be sent over a long wire of length <math alttext="L" class="ltx_Math" display="inline" id="A4.SS3.SSS2.p1.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> from the chip’s boundaries to a sampling cell in the bulk.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.4 </span>Analysis of a complete sampling program</h3>
<div class="ltx_para" id="A4.SS4.p1">
<p class="ltx_p">Given the above analysis of the various subsystems, it is straightforward to construct a model of the energy consumption of a complete denoising model. Running each layer of the denoising model requires initialization of all <math alttext="N" class="ltx_Math" display="inline" id="A4.SS4.p1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> nodes, chromatic Gibbs sampling for <math alttext="K" class="ltx_Math" display="inline" id="A4.SS4.p1.m2" intent=":literal"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> iterations, and finally, readout of the <math alttext="N_{\text{data}}" class="ltx_Math" display="inline" id="A4.SS4.p1.m3" intent=":literal"><semantics><msub><mi>N</mi><mtext>data</mtext></msub><annotation encoding="application/x-tex">N_{\text{data}}</annotation></semantics></math> data nodes,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E93">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E=T\left(E_{\text{samp}}+E_{\text{init}}+E_{\text{read}}\right)" class="ltx_Math" display="block" id="A4.E93.m1" intent=":literal"><semantics><mrow><mi>E</mi><mo>=</mo><mrow><mi>T</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>E</mi><mtext>samp</mtext></msub><mo>+</mo><msub><mi>E</mi><mtext>init</mtext></msub><mo>+</mo><msub><mi>E</mi><mtext>read</mtext></msub></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">E=T\left(E_{\text{samp}}+E_{\text{init}}+E_{\text{read}}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(93)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A4.SS4.p2">
<p class="ltx_p"><math alttext="E_{\text{samp}}" class="ltx_Math" display="inline" id="A4.SS4.p2.m1" intent=":literal"><semantics><msub><mi>E</mi><mtext>samp</mtext></msub><annotation encoding="application/x-tex">E_{\text{samp}}</annotation></semantics></math> is the cost associated with the sampling iterations for each layer,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E94">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E_{\text{samp}}=KN\left(E_{\text{rng}}+E_{\text{bias}}+E_{\text{clock}}+E_{\text{nb}}\right)" class="ltx_Math" display="block" id="A4.E94.m1" intent=":literal"><semantics><mrow><msub><mi>E</mi><mtext>samp</mtext></msub><mo>=</mo><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>E</mi><mtext>rng</mtext></msub><mo>+</mo><msub><mi>E</mi><mtext>bias</mtext></msub><mo>+</mo><msub><mi>E</mi><mtext>clock</mtext></msub><mo>+</mo><msub><mi>E</mi><mtext>nb</mtext></msub></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">E_{\text{samp}}=KN\left(E_{\text{rng}}+E_{\text{bias}}+E_{\text{clock}}+E_{\text{nb}}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(94)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="E_{\text{clock}}" class="ltx_Math" display="inline" id="A4.SS4.p2.m2" intent=":literal"><semantics><msub><mi>E</mi><mtext>clock</mtext></msub><annotation encoding="application/x-tex">E_{\text{clock}}</annotation></semantics></math> and <math alttext="E_{\text{nb}}" class="ltx_Math" display="inline" id="A4.SS4.p2.m3" intent=":literal"><semantics><msub><mi>E</mi><mtext>nb</mtext></msub><annotation encoding="application/x-tex">E_{\text{nb}}</annotation></semantics></math> are the per-cell costs associated with clock distribution and neighbor communication, respectively.</p>
</div>
<div class="ltx_para" id="A4.SS4.p3">
<p class="ltx_p"><math alttext="E_{\text{init}}" class="ltx_Math" display="inline" id="A4.SS4.p3.m1" intent=":literal"><semantics><msub><mi>E</mi><mtext>init</mtext></msub><annotation encoding="application/x-tex">E_{\text{init}}</annotation></semantics></math> is the cost of initializing all the cells at the beginning of the program,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E95">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E_{\text{init}}=N\frac{1}{2}\eta LV_{sig}^{2}" class="ltx_Math" display="block" id="A4.E95.m1" intent=":literal"><semantics><mrow><msub><mi>E</mi><mtext>init</mtext></msub><mo>=</mo><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>V</mi><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>g</mi></mrow><mn>2</mn></msubsup></mrow></mrow><annotation encoding="application/x-tex">E_{\text{init}}=N\frac{1}{2}\eta LV_{sig}^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(95)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and <math alttext="E_{\text{read}}" class="ltx_Math" display="inline" id="A4.SS4.p3.m2" intent=":literal"><semantics><msub><mi>E</mi><mtext>read</mtext></msub><annotation encoding="application/x-tex">E_{\text{read}}</annotation></semantics></math> is the cost of reading out the data cells at the end,</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E96">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E_{\text{read}}=N_{\text{data}}\frac{1}{2}\eta LV_{sig}^{2}" class="ltx_Math" display="block" id="A4.E96.m1" intent=":literal"><semantics><mrow><msub><mi>E</mi><mtext>read</mtext></msub><mo>=</mo><mrow><msub><mi>N</mi><mtext>data</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mi>η</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>V</mi><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>g</mi></mrow><mn>2</mn></msubsup></mrow></mrow><annotation encoding="application/x-tex">E_{\text{read}}=N_{\text{data}}\frac{1}{2}\eta LV_{sig}^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(96)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A4.SS4.p4">
<p class="ltx_p">This model was used to estimate the energy consumption of the denoising model depicted in Fig. 1 of the article. The mixing behavior for each layer in this denoising model is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.F12" title="Figure 12 ‣ D.4 Analysis of a complete sampling program ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">12</span></a> (a). All of the layers mix in tens of iterations, with the first layer decaying the most slowly. For the sake of energy calculations, we used <math alttext="K=250" class="ltx_Math" display="inline" id="A4.SS4.p4.m1" intent=":literal"><semantics><mrow><mi>K</mi><mo>=</mo><mn>250</mn></mrow><annotation encoding="application/x-tex">K=250</annotation></semantics></math> for all layers to be conservative. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.F13" title="Figure 13 ‣ D.4 Analysis of a complete sampling program ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">13</span></a> shows that for our trained denoising models, sampling for more than <math alttext="K\approx 250" class="ltx_Math" display="inline" id="A4.SS4.p4.m2" intent=":literal"><semantics><mrow><mi>K</mi><mo>≈</mo><mn>250</mn></mrow><annotation encoding="application/x-tex">K\approx 250</annotation></semantics></math> steps brings almost no additional benefit, which supports our use of this number for energy calculation. This grid used for each EBM in this model consisted of <math alttext="N=4900" class="ltx_Math" display="inline" id="A4.SS4.p4.m3" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>4900</mn></mrow><annotation encoding="application/x-tex">N=4900</annotation></semantics></math> nodes that were connected using a <math alttext="G_{12}" class="ltx_Math" display="inline" id="A4.SS4.p4.m4" intent=":literal"><semantics><msub><mi>G</mi><mn>12</mn></msub><annotation encoding="application/x-tex">G_{12}</annotation></semantics></math> pattern. <math alttext="N_{\text{data}}=834" class="ltx_Math" display="inline" id="A4.SS4.p4.m5" intent=":literal"><semantics><mrow><msub><mi>N</mi><mtext>data</mtext></msub><mo>=</mo><mn>834</mn></mrow><annotation encoding="application/x-tex">N_{\text{data}}=834</annotation></semantics></math> of the nodes were assigned to data, and the rest were latent.</p>
</div>
<div class="ltx_para" id="A4.SS4.p5">
<p class="ltx_p">Given realistic choices for the rest of the free parameters of the model, the energetic cost of this denoising model is estimated to be around <math alttext="1.6\&gt;T\text{nJ}" class="ltx_Math" display="inline" id="A4.SS4.p5.m1" intent=":literal"><semantics><mrow><mn>1.6</mn><mo lspace="0.220em" rspace="0em">​</mo><mi>T</mi><mo lspace="0em" rspace="0em">​</mo><mtext>nJ</mtext></mrow><annotation encoding="application/x-tex">1.6\&gt;T\text{nJ}</annotation></semantics></math>. This is almost entirely dominated by <math alttext="E_{\text{samp}}" class="ltx_Math" display="inline" id="A4.SS4.p5.m2" intent=":literal"><semantics><msub><mi>E</mi><mtext>samp</mtext></msub><annotation encoding="application/x-tex">E_{\text{samp}}</annotation></semantics></math>, with <math alttext="E_{\text{init}}+E_{\text{read}}\approx 0.01\&gt;T\text{nJ}" class="ltx_Math" display="inline" id="A4.SS4.p5.m3" intent=":literal"><semantics><mrow><mrow><msub><mi>E</mi><mtext>init</mtext></msub><mo>+</mo><msub><mi>E</mi><mtext>read</mtext></msub></mrow><mo>≈</mo><mrow><mn>0.01</mn><mo lspace="0.220em" rspace="0em">​</mo><mi>T</mi><mo lspace="0em" rspace="0em">​</mo><mtext>nJ</mtext></mrow></mrow><annotation encoding="application/x-tex">E_{\text{init}}+E_{\text{read}}\approx 0.01\&gt;T\text{nJ}</annotation></semantics></math>. A breakdown of the various contributions to <math alttext="E_{\text{samp}}" class="ltx_Math" display="inline" id="A4.SS4.p5.m4" intent=":literal"><semantics><msub><mi>E</mi><mtext>samp</mtext></msub><annotation encoding="application/x-tex">E_{\text{samp}}</annotation></semantics></math>, along with more details about the used model parameters, is given in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.F12" title="Figure 12 ‣ D.4 Analysis of a complete sampling program ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">12</span></a> (b).</p>
</div>
<figure class="ltx_figure" id="A4.F12"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="327" id="A4.F12.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>(a) <span class="ltx_text ltx_font_bold">Autocorrelation curve of a denoising model composed of Boltzmann machines.</span> Each line represents the autocorrelation of one of the Boltzmann machines that make up a fully trained denoising model. 
<br class="ltx_break"/>(b) <span class="ltx_text ltx_font_bold">Breakdown of the energetic cost of running a sampling cell.</span> Here, we take <math alttext="\tau_{rng}/\tau_{bias}=15" class="ltx_Math" display="inline" id="A4.F12.m7" intent=":literal"><semantics><mrow><mrow><msub><mi>τ</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>g</mi></mrow></msub><mo>/</mo><msub><mi>τ</mi><mrow><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></msub></mrow><mo>=</mo><mn>15</mn></mrow><annotation encoding="application/x-tex">\tau_{rng}/\tau_{bias}=15</annotation></semantics></math> and <math alttext="\gamma=1/2" class="ltx_Math" display="inline" id="A4.F12.m8" intent=":literal"><semantics><mrow><mi>γ</mi><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow><annotation encoding="application/x-tex">\gamma=1/2</annotation></semantics></math>. We also assume that signaling to neighbors is conducted at a voltage of <math alttext="4V_{T}" class="ltx_Math" display="inline" id="A4.F12.m9" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><msub><mi>V</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">4V_{T}</annotation></semantics></math> (where <math alttext="V_{T}" class="ltx_Math" display="inline" id="A4.F12.m10" intent=":literal"><semantics><msub><mi>V</mi><mi>T</mi></msub><annotation encoding="application/x-tex">V_{T}</annotation></semantics></math> is the thermal voltage <math alttext="k_{B}T/e" class="ltx_Math" display="inline" id="A4.F12.m11" intent=":literal"><semantics><mrow><mrow><msub><mi>k</mi><mi>B</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow><mo>/</mo><mi>e</mi></mrow><annotation encoding="application/x-tex">k_{B}T/e</annotation></semantics></math>) and the clocking and read/write operations are conducted at a signal level of <math alttext="5V_{T}" class="ltx_Math" display="inline" id="A4.F12.m12" intent=":literal"><semantics><mrow><mn>5</mn><mo lspace="0em" rspace="0em">​</mo><msub><mi>V</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">5V_{T}</annotation></semantics></math>.</figcaption>
</figure>
<figure class="ltx_figure" id="A4.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="A4.F13.g1" src="x13.png" width="498"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>The quality of output images generated by our denoising models stops improving when we sample for more than <math alttext="K\approx 250" class="ltx_Math" display="inline" id="A4.F13.m2" intent=":literal"><semantics><mrow><mi>K</mi><mo>≈</mo><mn>250</mn></mrow><annotation encoding="application/x-tex">K\approx 250</annotation></semantics></math> steps.</figcaption>
</figure>
<div class="ltx_para" id="A4.SS4.p6">
<p class="ltx_p">This exact procedure was used to estimate the energy consumption of the MEBMs in Fig. 1 in the article. In this case, <math alttext="T=1" class="ltx_Math" display="inline" id="A4.SS4.p6.m1" intent=":literal"><semantics><mrow><mi>T</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">T=1</annotation></semantics></math> and <math alttext="K" class="ltx_Math" display="inline" id="A4.SS4.p6.m2" intent=":literal"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> were estimated from the autocorrelation data for each layer; see section  <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A11" title="Appendix K MEBM experiments ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">K</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="A4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.5 </span>Level of realism</h3>
<div class="ltx_para" id="A4.SS5.p1">
<p class="ltx_p">The model presented here captures all of the central functional units of a hardware Boltzmann machine sampler. However, the analysis was performed at a high level, and the model almost certainly underestimates the actual energy consumption of a complete device. In practice, when comparing the results of this type of calculation to a detailed analysis of a complete device design, we generally find agreement within an order of magnitude. Given that the gap between conventional methods and our novel hardware architecture is at least several orders of magnitude, this low-resolution analysis is useful, as it supports the claims made in this article without getting into every implementation detail.</p>
</div>
<div class="ltx_para" id="A4.SS5.p2">
<p class="ltx_p">Some of the discrepancies between the high-level and detailed model can be attributed to overheads associated with real circuits. A real implementation of the biasing circuit discussed in section  <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS1" title="D.1 Biasing circuit ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">D.D.1</span></a> is more complicated than the theoretical model because tunable resistors do not exist. Communications with neighboring cells over long wires require driver circuits, which consume additional energy beyond what is spent charging the line. Despite this, real circuits are bound by the same fundamental physics as the simplified models presented here. As such, the simplified models tend to estimate energy consumption within a factor of two or three of real-life values.</p>
</div>
<div class="ltx_para" id="A4.SS5.p3">
<p class="ltx_p">A real device also has additional supporting circuitry compared to our stripped-down model. In the remainder of this section, we will discuss some examples of such supporting circuitry and argue that their contributions to energy consumption at the system level ought not to be significant.</p>
</div>
<section class="ltx_subsubsection" id="A4.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">D.5.1 </span>Programming the weights and biases</h4>
<div class="ltx_para" id="A4.SS5.SSS1.p1">
<p class="ltx_p">Section <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS1" title="D.1 Biasing circuit ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">D.D.1</span></a> discusses a simple circuit that uses resistors to implement the multiply-accumulate required by the conditional update rule. Key to this is being able to tune the conductance of the resistors to implement specific sets of weights and biases (see Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.E86" title="Equation 86 ‣ D.1 Biasing circuit ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">86</span></a>)).</p>
</div>
<div class="ltx_para" id="A4.SS5.SSS1.p2">
<p class="ltx_p">Practically, implementing this tunability requires that the model parameters be stored in memory somewhere on the chip. Writing to and maintaining these memories costs energy.</p>
</div>
<div class="ltx_para" id="A4.SS5.SSS1.p3">
<p class="ltx_p">Writing to the memories uses much more energy than maintaining the state. However, if writes are infrequent (program the device once and then run many sampling programs on it before writing again), then the overall cost of the memory is dominated by maintenance. Luckily, most conventional memories are specifically designed to consume as little energy as possible when not being accessed. As such, in practice, the cost of memory maintenance is small compared to the other costs associated with the sampling cells and does not significantly change the outcome shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.F12" title="Figure 12 ‣ D.4 Analysis of a complete sampling program ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A4.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">D.5.2 </span>Off-chip communication</h4>
<div class="ltx_para" id="A4.SS5.SSS2.p1">
<p class="ltx_p">External devices have to communicate with our chip for it to be useful. The cost of this communication depends strongly on the tightness of integration between the two systems and is impossible to reason about at an abstract level. As such, the analysis of communication here (as in Section <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS3.SSS2" title="D.3.2 Initialization and readout ‣ D.3 Global communication ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">D.3.2</span></a>) was limited to the cost of getting bits out to the edge of our chip, which is a lower bound on the actual cost.</p>
</div>
<div class="ltx_para" id="A4.SS5.SSS2.p2">
<p class="ltx_p">However, we have found that a more detailed analysis, which includes the cost of communication between two chips mediated by a PCB, does not significantly change the results at the system level. The fundamental reason for this is that sampling programs for complex models run for many iterations before mixing and sending the results back to the outside world. This is reflected in the discrepancy between <math alttext="E_{\text{samp}}" class="ltx_Math" display="inline" id="A4.SS5.SSS2.p2.m1" intent=":literal"><semantics><msub><mi>E</mi><mtext>samp</mtext></msub><annotation encoding="application/x-tex">E_{\text{samp}}</annotation></semantics></math> and <math alttext="E_{\text{init}}+E_{\text{read}}" class="ltx_Math" display="inline" id="A4.SS5.SSS2.p2.m2" intent=":literal"><semantics><mrow><msub><mi>E</mi><mtext>init</mtext></msub><mo>+</mo><msub><mi>E</mi><mtext>read</mtext></msub></mrow><annotation encoding="application/x-tex">E_{\text{init}}+E_{\text{read}}</annotation></semantics></math> found in section <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A4.SS4" title="D.4 Analysis of a complete sampling program ‣ Appendix D Energetic analysis of the hardware architecture ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">D.D.4</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A4.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">D.5.3 </span>Supporting circuitry</h4>
<div class="ltx_para" id="A4.SS5.SSS3.p1">
<p class="ltx_p">Any real chip has digital and analog supporting circuitry that provides basic functionality, such as clocking and communication, allowing the rest of the chip to function correctly. The fraction of the energy budget spent on this supporting circuitry generally depends on its size compared to the core computer. Due to the heterogeneity of our architecture, it is possible to share most of the supporting circuitry among many sampling cells, which dramatically reduces the per-cell cost. As such, the energy cost of the supporting circuitry is not significant at the system level.</p>
</div>
</section>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Energy analysis of GPUs</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p">All experiments shown in Fig. 1 in the article were conducted on NVIDIA A100 GPUs. The empirical estimates of energy were conducted by drawing a batch of samples from the model and measuring the GPU energy consumption and time via Zeus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib9" title="Zeus: Understanding and optimizing {GPU} energy consumption of {DNN} training">14</a>]</cite>. The theoretical energy estimates were derived by taking the number of model FLOPS (via JAX and PyTorch’s internal estimators) and plugging them into the NVIDIA GPU specifications (19.5 TFLOPS for Float32 and 400W). The empirical measurements are compared to theoretical estimates for the VAE in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A5.T2" title="Table 2 ‣ Appendix E Energy analysis of GPUs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">2</span></a>, and the empirical measurements show good alignment with the theoretical.</p>
</div>
<figure class="ltx_table" id="A5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_ll ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">FID</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">Empirical Efficiency</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;">Theoretical Efficiency</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_r ltx_border_tt">30.5</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math alttext="6.1\times 10^{-5}" class="ltx_Math" display="inline" id="A5.T2.m1" intent=":literal"><semantics><mrow><mn>6.1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">6.1\times 10^{-5}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><math alttext="2.3\times 10^{-5}" class="ltx_Math" display="inline" id="A5.T2.m2" intent=":literal"><semantics><mrow><mn>2.3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2.3\times 10^{-5}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_r ltx_border_t">27.4</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math alttext="1.5\times 10^{-4}" class="ltx_Math" display="inline" id="A5.T2.m3" intent=":literal"><semantics><mrow><mn>1.5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1.5\times 10^{-4}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><math alttext="0.4\times 10^{-4}" class="ltx_Math" display="inline" id="A5.T2.m4" intent=":literal"><semantics><mrow><mn>0.4</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">0.4\times 10^{-4}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_ll ltx_border_r ltx_border_t">17.9</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math alttext="2.5\times 10^{-3}" class="ltx_Math" display="inline" id="A5.T2.m5" intent=":literal"><semantics><mrow><mn>2.5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2.5\times 10^{-3}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t"><math alttext="1.7\times 10^{-3}" class="ltx_Math" display="inline" id="A5.T2.m6" intent=":literal"><semantics><mrow><mn>1.7</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1.7\times 10^{-3}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparing theoretical vs empirical energy consumption for a VAE on a GPU. Energy efficiencies are reported in units of joules per sample.</figcaption>
</figure>
<div class="ltx_para" id="A5.p2">
<p class="ltx_p">The models were derived from available implementations and are based on ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib11" title="Deep residual learning for image recognition">3</a>]</cite> and UNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib12" title="U-net: Convolutional networks for biomedical image segmentation">11</a>]</cite> style architectures. Their FID performance is consistent with published literature values <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib8" title="Diagnosing and Enhancing VAE Models">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib7" title="Pythae: Unifying generative autoencoders in python-a benchmarking use case">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib10" title="Sparse Data Generation Using Diffusion Models">9</a>]</cite>. The goal is not to achieve state of the art performance, but to represent the relative scales of energy consumption of the algorithms.</p>
</div>
<div class="ltx_para" id="A5.p3">
<p class="ltx_p">The reader may be surprised to see that the diffusion model is substantially less energy-efficient than the VAE given the relative dominance in image generation. However, two points should be kept in mind. First, while VAE remains a semi-competitive model for these smaller datasets, this quickly breaks down. On larger datasets, a FID performance gap usually exists between diffusion models and VAEs. Second, these diffusion models (based on the original DDPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib4" title="Denoising diffusion probabilistic models">4</a>]</cite>) have performance that can depend on the number of diffusion time steps. So, not only is the UNet model often larger than a VAE decoder, but it also must be run dozens to thousands of times in order to generate a single sample (thus resulting in multiple orders of magnitude more energy required). Modern improvements, such as distillation <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">liu2023instaflow</span>]</cite>, may move the diffusion model energy efficiency closer to the VAE’s.</p>
</div>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Total correlation penalty</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p">In the main text (see Eq. 17), we explain how we utilize a total correlation penalty to encourage the latent variable EBMs employed in our model to mix rapidly. Here, we will discuss a few details of this regularizer and the method we use to control its strength adaptively.</p>
</div>
<section class="ltx_subsection" id="A6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.1 </span>Gradients of the total correlation penalty</h3>
<div class="ltx_para" id="A6.SS1.p1">
<p class="ltx_p">The total correlation penalty is a convenient choice in this context because its gradients can be computed using the same samples used to estimate the gradient of the usual loss used in training, <math alttext="\nabla_{\theta}\mathcal{L}_{DN}" class="ltx_Math" display="inline" id="A6.SS1.p1.m1" intent=":literal"><semantics><mrow><msub><mo>∇</mo><mi>θ</mi></msub><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\nabla_{\theta}\mathcal{L}_{DN}</annotation></semantics></math>. Namely, treating the factorized distribution as a constant with respect to the gradient,</p>
<table class="ltx_equation ltx_eqn_table" id="A6.E97">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\nabla_{\theta}\mathcal{L}^{TC}_{t}=\mathbb{E}_{Q(x^{t-1})}\left[\mathbb{E}_{d(s^{t-1}|x^{t})}\left[\nabla_{\theta}\mathcal{E}^{\theta}_{t-1}\right]-\mathbb{E}_{P_{\theta}(s^{t-1}|x^{t}))}\left[\nabla_{\theta}\mathcal{E}^{\theta}_{t-1}\right]\right]" class="ltx_math_unparsed" display="block" id="A6.E97.m1" intent=":literal"><semantics><mrow><mrow><msub><mo>∇</mo><mi>θ</mi></msub><msubsup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>t</mi><mrow><mi>T</mi><mo lspace="0em" rspace="0em">​</mo><mi>C</mi></mrow></msubsup></mrow><mo>=</mo><mrow><msub><mi mathvariant="normal">𝔼</mi><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><msub><mi mathvariant="normal">𝔼</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup></mrow><mo>]</mo></mrow></mrow><mo>−</mo><mrow><msub><mi mathvariant="normal">𝔼</mi><mrow><msub><mi>P</mi><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><msup><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msup><mi>x</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup></mrow><mo>]</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\nabla_{\theta}\mathcal{L}^{TC}_{t}=\mathbb{E}_{Q(x^{t-1})}\left[\mathbb{E}_{d(s^{t-1}|x^{t})}\left[\nabla_{\theta}\mathcal{E}^{\theta}_{t-1}\right]-\mathbb{E}_{P_{\theta}(s^{t-1}|x^{t}))}\left[\nabla_{\theta}\mathcal{E}^{\theta}_{t-1}\right]\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(97)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where,</p>
<table class="ltx_equation ltx_eqn_table" id="A6.E98">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d(s^{t-1}|x^{t})=\prod_{i=1}^{M}P_{\theta}(s^{t-1}_{i}|x^{t})" class="ltx_Math" display="block" id="A6.E98.m1" intent=":literal"><semantics><mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>s</mi><mi>i</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">d(s^{t-1}|x^{t})=\prod_{i=1}^{M}P_{\theta}(s^{t-1}_{i}|x^{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(98)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A6.SS1.p2">
<p class="ltx_p">The second term in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A6.E97" title="Equation 97 ‣ F.1 Gradients of the total correlation penalty ‣ Appendix F Total correlation penalty ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">97</span></a>) also appears in the estimator for <math alttext="\nabla_{\theta}\mathcal{L}_{DN}" class="ltx_Math" display="inline" id="A6.SS1.p2.m1" intent=":literal"><semantics><mrow><msub><mo>∇</mo><mi>θ</mi></msub><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\nabla_{\theta}\mathcal{L}_{DN}</annotation></semantics></math>. The first term can be simplified when <math alttext="\mathcal{E}^{\theta}_{t-1}" class="ltx_Math" display="inline" id="A6.SS1.p2.m2" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><annotation encoding="application/x-tex">\mathcal{E}^{\theta}_{t-1}</annotation></semantics></math> has particular symmetries. For example, if <math alttext="\mathcal{E}^{\theta}_{t-1}" class="ltx_Math" display="inline" id="A6.SS1.p2.m3" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup><annotation encoding="application/x-tex">\mathcal{E}^{\theta}_{t-1}</annotation></semantics></math> is a Boltzmann machine energy function (see main text Eq. 10),</p>
<table class="ltx_equation ltx_eqn_table" id="A6.E99">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}_{d(s^{t-1}|x^{t})}\left[\frac{d}{dh_{i}}\mathcal{E}^{\theta}_{t-1}\right]=-\beta\&gt;\mathbb{E}_{P_{\theta}(s_{i}|x_{t})}\left[s_{i}\right]" class="ltx_Math" display="block" id="A6.E99.m1" intent=":literal"><semantics><mrow><mrow><msub><mi mathvariant="normal">𝔼</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>h</mi><mi>i</mi></msub></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><mi>β</mi><mo lspace="0.220em" rspace="0em">​</mo><msub><mi mathvariant="normal">𝔼</mi><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo fence="false">|</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><msub><mi>s</mi><mi>i</mi></msub><mo>]</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}_{d(s^{t-1}|x^{t})}\left[\frac{d}{dh_{i}}\mathcal{E}^{\theta}_{t-1}\right]=-\beta\&gt;\mathbb{E}_{P_{\theta}(s_{i}|x_{t})}\left[s_{i}\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(99)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="A6.E100">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}_{d(s^{t-1}|x^{t})}\left[\frac{d}{dJ_{ij}}\mathcal{E}^{\theta}_{t-1}\right]=-\beta\&gt;\mathbb{E}_{P_{\theta}(s_{i}|x_{t})}\left[s_{i}\right]\&gt;\mathbb{E}_{P_{\theta}(s_{j}|x_{t})}\left[s_{j}\right]" class="ltx_Math" display="block" id="A6.E100.m1" intent=":literal"><semantics><mrow><mrow><msub><mi mathvariant="normal">𝔼</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false">|</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>J</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mi>θ</mi></msubsup></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><mi>β</mi><mo lspace="0.220em" rspace="0em">​</mo><msub><mi mathvariant="normal">𝔼</mi><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo fence="false">|</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><msub><mi>s</mi><mi>i</mi></msub><mo>]</mo></mrow><mo lspace="0.220em" rspace="0em">​</mo><msub><mi mathvariant="normal">𝔼</mi><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>s</mi><mi>j</mi></msub><mo fence="false">|</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><msub><mi>s</mi><mi>j</mi></msub><mo>]</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}_{d(s^{t-1}|x^{t})}\left[\frac{d}{dJ_{ij}}\mathcal{E}^{\theta}_{t-1}\right]=-\beta\&gt;\mathbb{E}_{P_{\theta}(s_{i}|x_{t})}\left[s_{i}\right]\&gt;\mathbb{E}_{P_{\theta}(s_{j}|x_{t})}\left[s_{j}\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(100)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Each of these terms is easy to compute given the samples used to estimate <math alttext="\nabla_{\theta}\mathcal{L}_{DN}" class="ltx_Math" display="inline" id="A6.SS1.p2.m4" intent=":literal"><semantics><mrow><msub><mo>∇</mo><mi>θ</mi></msub><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\nabla_{\theta}\mathcal{L}_{DN}</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="A6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.2 </span>Low dimensional embedding of the data</h3>
<div class="ltx_para" id="A6.SS2.p1">
<p class="ltx_p">For all experiments in this article, the embedding function <math alttext="f" class="ltx_Math" display="inline" id="A6.SS2.p1.m1" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> used in autocorrelation calculations was the encoding neural network used in FID computation. Using the encoder was an arbitrary choice, and we could have just as easily used a much simpler function. For example, we found that random linear projections <math alttext="y[j]=Ax[j]" class="ltx_Math" display="inline" id="A6.SS2.p1.m2" intent=":literal"><semantics><mrow><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">y[j]=Ax[j]</annotation></semantics></math> worked just as well as the neural network for autocorrelation calculations.</p>
</div>
</section>
<section class="ltx_subsection" id="A6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.3 </span>Control of the penalty strength</h3>
<div class="ltx_para" id="A6.SS3.p1">
<p class="ltx_p">The optimal strength of the correlation penalty <math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="A6.SS3.p1.m1" intent=":literal"><semantics><msub><mi>λ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\lambda_{t}</annotation></semantics></math> may vary depending on the specific denoising step <math alttext="t" class="ltx_Math" display="inline" id="A6.SS3.p1.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> (models for less noisy data near <math alttext="t=0" class="ltx_Math" display="inline" id="A6.SS3.p1.m3" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t=0</annotation></semantics></math> may require stronger regularization) and may even change during training for a single-step model. Manually tuning <math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="A6.SS3.p1.m4" intent=":literal"><semantics><msub><mi>λ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\lambda_{t}</annotation></semantics></math> for each of the step-models would be prohibitively expensive.</p>
</div>
<div class="ltx_para" id="A6.SS3.p2">
<p class="ltx_p">To address this, we employ an Adaptive Correlation Penalty (ACP) scheme that dynamically adjusts <math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="A6.SS3.p2.m1" intent=":literal"><semantics><msub><mi>λ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\lambda_{t}</annotation></semantics></math> based on an estimate of the model’s current mixing time. We use the autocorrelation of the Gibbs sampling chain, <math alttext="r_{yy}^{t}" class="ltx_Math" display="inline" id="A6.SS3.p2.m2" intent=":literal"><semantics><msubsup><mi>r</mi><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mi>y</mi></mrow><mi>t</mi></msubsup><annotation encoding="application/x-tex">r_{yy}^{t}</annotation></semantics></math>, as a proxy for mixing, as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A8" title="Appendix H The autocorrelation function and mixing time ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">H</span></a> and the main text, Eq. 18.</p>
</div>
<div class="ltx_para" id="A6.SS3.p3">
<p class="ltx_p">Our ACP algorithm monitors the autocorrelation at a lag <math alttext="K" class="ltx_Math" display="inline" id="A6.SS3.p3.m1" intent=":literal"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> equal to the number of Gibbs steps used in the estimation of <math alttext="\nabla_{\theta}\mathcal{L}_{DN}" class="ltx_Math" display="inline" id="A6.SS3.p3.m2" intent=":literal"><semantics><mrow><msub><mo>∇</mo><mi>θ</mi></msub><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\nabla_{\theta}\mathcal{L}_{DN}</annotation></semantics></math>. The goal is to adjust <math alttext="\gamma_{\text{CP}}" class="ltx_Math" display="inline" id="A6.SS3.p3.m3" intent=":literal"><semantics><msub><mi>γ</mi><mtext>CP</mtext></msub><annotation encoding="application/x-tex">\gamma_{\text{CP}}</annotation></semantics></math> to keep this autocorrelation below a predefined target threshold <math alttext="\varepsilon_{\text{ACP}}" class="ltx_Math" display="inline" id="A6.SS3.p3.m4" intent=":literal"><semantics><msub><mi>ε</mi><mtext>ACP</mtext></msub><annotation encoding="application/x-tex">\varepsilon_{\text{ACP}}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A6.SS3.p4">
<p class="ltx_p">A simple layerwise procedure is used for this control. The inputs to the algorithm are the initial values of <math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="A6.SS3.p4.m1" intent=":literal"><semantics><msub><mi>λ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\lambda_{t}</annotation></semantics></math>, a target autocorrelation threshold <math alttext="\varepsilon_{\text{ACP}}" class="ltx_Math" display="inline" id="A6.SS3.p4.m2" intent=":literal"><semantics><msub><mi>ε</mi><mtext>ACP</mtext></msub><annotation encoding="application/x-tex">\varepsilon_{\text{ACP}}</annotation></semantics></math> (e.g., <math alttext="0.03" class="ltx_Math" display="inline" id="A6.SS3.p4.m3" intent=":literal"><semantics><mn>0.03</mn><annotation encoding="application/x-tex">0.03</annotation></semantics></math>), an update factor <math alttext="\delta_{\text{ACP}}" class="ltx_Math" display="inline" id="A6.SS3.p4.m4" intent=":literal"><semantics><msub><mi>δ</mi><mtext>ACP</mtext></msub><annotation encoding="application/x-tex">\delta_{\text{ACP}}</annotation></semantics></math> (e.g., <math alttext="0.2" class="ltx_Math" display="inline" id="A6.SS3.p4.m5" intent=":literal"><semantics><mn>0.2</mn><annotation encoding="application/x-tex">0.2</annotation></semantics></math>) and a lower limit <math alttext="\lambda_{t}^{\text{min}}" class="ltx_Math" display="inline" id="A6.SS3.p4.m6" intent=":literal"><semantics><msubsup><mi>λ</mi><mi>t</mi><mtext>min</mtext></msubsup><annotation encoding="application/x-tex">\lambda_{t}^{\text{min}}</annotation></semantics></math> (e.g., <math alttext="0.0001" class="ltx_Math" display="inline" id="A6.SS3.p4.m7" intent=":literal"><semantics><mn>0.0001</mn><annotation encoding="application/x-tex">0.0001</annotation></semantics></math>).</p>
</div>
<div class="ltx_para" id="A6.SS3.p5">
<p class="ltx_p">At the end of each training epoch <math alttext="m" class="ltx_Math" display="inline" id="A6.SS3.p5.m1" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>:</p>
<ol class="ltx_enumerate" id="A6.I1">
<li class="ltx_item" id="A6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A6.I1.i1.p1">
<p class="ltx_p">Estimate the current autocorrelation <math alttext="a_{m}^{t}=r_{yy}^{t}[K]" class="ltx_Math" display="inline" id="A6.I1.i1.p1.m1" intent=":literal"><semantics><mrow><msubsup><mi>a</mi><mi>m</mi><mi>t</mi></msubsup><mo>=</mo><mrow><msubsup><mi>r</mi><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mi>y</mi></mrow><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">a_{m}^{t}=r_{yy}^{t}[K]</annotation></semantics></math>. This estimate can be done by running a longer Gibbs chain periodically and calculating the empirical autocorrelation from the samples.</p>
</div>
</li>
<li class="ltx_item" id="A6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A6.I1.i2.p1">
<p class="ltx_p">Set <math alttext="\lambda_{t}^{\prime}=max(\lambda_{t}^{\text{min}},\lambda_{t}^{(m)})" class="ltx_Math" display="inline" id="A6.I1.i2.p1.m1" intent=":literal"><semantics><mrow><msubsup><mi>λ</mi><mi>t</mi><mo>′</mo></msubsup><mo>=</mo><mrow><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>λ</mi><mi>t</mi><mtext>min</mtext></msubsup><mo>,</mo><msubsup><mi>λ</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\lambda_{t}^{\prime}=max(\lambda_{t}^{\text{min}},\lambda_{t}^{(m)})</annotation></semantics></math> to avoid getting stuck at 0.</p>
</div>
</li>
<li class="ltx_item" id="A6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A6.I1.i3.p1">
<p class="ltx_p">Update <math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="A6.I1.i3.p1.m1" intent=":literal"><semantics><msub><mi>λ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\lambda_{t}</annotation></semantics></math> for the next epoch (<math alttext="m+1" class="ltx_Math" display="inline" id="A6.I1.i3.p1.m2" intent=":literal"><semantics><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">m+1</annotation></semantics></math>) based on <math alttext="a_{m}^{t}" class="ltx_Math" display="inline" id="A6.I1.i3.p1.m3" intent=":literal"><semantics><msubsup><mi>a</mi><mi>m</mi><mi>t</mi></msubsup><annotation encoding="application/x-tex">a_{m}^{t}</annotation></semantics></math> and the previous value <math alttext="a_{m-1}^{t}" class="ltx_Math" display="inline" id="A6.I1.i3.p1.m4" intent=":literal"><semantics><msubsup><mi>a</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow><mi>t</mi></msubsup><annotation encoding="application/x-tex">a_{m-1}^{t}</annotation></semantics></math> (if <math alttext="m&gt;0" class="ltx_Math" display="inline" id="A6.I1.i3.p1.m5" intent=":literal"><semantics><mrow><mi>m</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">m&gt;0</annotation></semantics></math>):</p>
<ul class="ltx_itemize" id="A6.I1.i3.I1">
<li class="ltx_item" id="A6.I1.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A6.I1.i3.I1.i1.p1">
<p class="ltx_p">If <math alttext="a_{m}^{t}&lt;\varepsilon_{\text{ACP}}" class="ltx_Math" display="inline" id="A6.I1.i3.I1.i1.p1.m1" intent=":literal"><semantics><mrow><msubsup><mi>a</mi><mi>m</mi><mi>t</mi></msubsup><mo>&lt;</mo><msub><mi>ε</mi><mtext>ACP</mtext></msub></mrow><annotation encoding="application/x-tex">a_{m}^{t}&lt;\varepsilon_{\text{ACP}}</annotation></semantics></math>: The chain mixes sufficiently fast; reduce the penalty slightly.</p>
<table class="ltx_equation ltx_eqn_table" id="A6.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\lambda_{t}^{(m+1)}\leftarrow(1-\delta_{\text{ACP}})\lambda_{t}^{\prime}" class="ltx_Math" display="block" id="A6.Ex1.m1" intent=":literal"><semantics><mrow><msubsup><mi>λ</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">←</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>δ</mi><mtext>ACP</mtext></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>λ</mi><mi>t</mi><mo>′</mo></msubsup></mrow></mrow><annotation encoding="application/x-tex">\lambda_{t}^{(m+1)}\leftarrow(1-\delta_{\text{ACP}})\lambda_{t}^{\prime}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="A6.I1.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A6.I1.i3.I1.i2.p1">
<p class="ltx_p">Else if <math alttext="a_{m}^{t}\geq\varepsilon_{\text{ACP}}" class="ltx_Math" display="inline" id="A6.I1.i3.I1.i2.p1.m1" intent=":literal"><semantics><mrow><msubsup><mi>a</mi><mi>m</mi><mi>t</mi></msubsup><mo>≥</mo><msub><mi>ε</mi><mtext>ACP</mtext></msub></mrow><annotation encoding="application/x-tex">a_{m}^{t}\geq\varepsilon_{\text{ACP}}</annotation></semantics></math> and <math alttext="a_{m}^{t}\leq a_{m-1}^{t}" class="ltx_Math" display="inline" id="A6.I1.i3.I1.i2.p1.m2" intent=":literal"><semantics><mrow><msubsup><mi>a</mi><mi>m</mi><mi>t</mi></msubsup><mo>≤</mo><msubsup><mi>a</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">a_{m}^{t}\leq a_{m-1}^{t}</annotation></semantics></math> (or <math alttext="m=0" class="ltx_Math" display="inline" id="A6.I1.i3.I1.i2.p1.m3" intent=":literal"><semantics><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">m=0</annotation></semantics></math>): Mixing is slow but not worsening (or baseline); keep the penalty strength.</p>
<table class="ltx_equation ltx_eqn_table" id="A6.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\lambda_{t}^{(m+1)}\leftarrow\lambda_{t}^{\prime}" class="ltx_Math" display="block" id="A6.Ex2.m1" intent=":literal"><semantics><mrow><msubsup><mi>λ</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">←</mo><msubsup><mi>λ</mi><mi>t</mi><mo>′</mo></msubsup></mrow><annotation encoding="application/x-tex">\lambda_{t}^{(m+1)}\leftarrow\lambda_{t}^{\prime}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="A6.I1.i3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A6.I1.i3.I1.i3.p1">
<p class="ltx_p">Else (<math alttext="a_{m}^{t}&gt;\varepsilon_{\text{ACP}}" class="ltx_Math" display="inline" id="A6.I1.i3.I1.i3.p1.m1" intent=":literal"><semantics><mrow><msubsup><mi>a</mi><mi>m</mi><mi>t</mi></msubsup><mo>&gt;</mo><msub><mi>ε</mi><mtext>ACP</mtext></msub></mrow><annotation encoding="application/x-tex">a_{m}^{t}&gt;\varepsilon_{\text{ACP}}</annotation></semantics></math> and <math alttext="a_{m}^{t}&gt;a_{m-1}^{t}" class="ltx_Math" display="inline" id="A6.I1.i3.I1.i3.p1.m2" intent=":literal"><semantics><mrow><msubsup><mi>a</mi><mi>m</mi><mi>t</mi></msubsup><mo>&gt;</mo><msubsup><mi>a</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">a_{m}^{t}&gt;a_{m-1}^{t}</annotation></semantics></math>): Mixing is slow and worsening; increase the penalty.</p>
<table class="ltx_equation ltx_eqn_table" id="A6.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\lambda_{t}^{(m+1)}\leftarrow(1+\delta_{\text{ACP}})\lambda_{t}^{\prime}" class="ltx_Math" display="block" id="A6.Ex3.m1" intent=":literal"><semantics><mrow><msubsup><mi>λ</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">←</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msub><mi>δ</mi><mtext>ACP</mtext></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>λ</mi><mi>t</mi><mo>′</mo></msubsup></mrow></mrow><annotation encoding="application/x-tex">\lambda_{t}^{(m+1)}\leftarrow(1+\delta_{\text{ACP}})\lambda_{t}^{\prime}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="A6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A6.I1.i4.p1">
<p class="ltx_p">If the proposed value <math alttext="\lambda_{t}^{(m+1)}&lt;\lambda_{t}^{\text{min}}" class="ltx_Math" display="inline" id="A6.I1.i4.p1.m1" intent=":literal"><semantics><mrow><msubsup><mi>λ</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo>&lt;</mo><msubsup><mi>λ</mi><mi>t</mi><mtext>min</mtext></msubsup></mrow><annotation encoding="application/x-tex">\lambda_{t}^{(m+1)}&lt;\lambda_{t}^{\text{min}}</annotation></semantics></math>, then set <math alttext="\lambda_{t}^{(m+1)}\leftarrow 0" class="ltx_Math" display="inline" id="A6.I1.i4.p1.m2" intent=":literal"><semantics><mrow><msubsup><mi>λ</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">←</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda_{t}^{(m+1)}\leftarrow 0</annotation></semantics></math>.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="A6.SS3.p6">
<p class="ltx_p">Our experiments indicate that this simple feedback mechanism works effectively.
While <math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="A6.SS3.p6.m1" intent=":literal"><semantics><msub><mi>λ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\lambda_{t}</annotation></semantics></math> and the autocorrelation <math alttext="a_{m}^{t}" class="ltx_Math" display="inline" id="A6.SS3.p6.m2" intent=":literal"><semantics><msubsup><mi>a</mi><mi>m</mi><mi>t</mi></msubsup><annotation encoding="application/x-tex">a_{m}^{t}</annotation></semantics></math> might exhibit some damped oscillations for several epochs before stabilizing this automated procedure is vastly more efficient than performing manual hyperparameter searches for <math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="A6.SS3.p6.m3" intent=":literal"><semantics><msub><mi>λ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\lambda_{t}</annotation></semantics></math> for each of the <math alttext="T" class="ltx_Math" display="inline" id="A6.SS3.p6.m4" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> models.</p>
</div>
<div class="ltx_para" id="A6.SS3.p7">
<p class="ltx_p">Training is relatively insensitive to the exact choice of <math alttext="\varepsilon_{\text{ACP}}" class="ltx_Math" display="inline" id="A6.SS3.p7.m1" intent=":literal"><semantics><msub><mi>ε</mi><mtext>ACP</mtext></msub><annotation encoding="application/x-tex">\varepsilon_{\text{ACP}}</annotation></semantics></math> within a reasonable range (e.g., <math alttext="[0.02,0.1]" class="ltx_Math" display="inline" id="A6.SS3.p7.m2" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><mn>0.02</mn><mo>,</mo><mn>0.1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0.02,0.1]</annotation></semantics></math>) and <math alttext="\delta_{\text{ACP}}" class="ltx_Math" display="inline" id="A6.SS3.p7.m3" intent=":literal"><semantics><msub><mi>δ</mi><mtext>ACP</mtext></msub><annotation encoding="application/x-tex">\delta_{\text{ACP}}</annotation></semantics></math> (e.g., <math alttext="[0.1,0.3]" class="ltx_Math" display="inline" id="A6.SS3.p7.m4" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><mn>0.1</mn><mo>,</mo><mn>0.3</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0.1,0.3]</annotation></semantics></math>). Assuming that over the course of training the <math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="A6.SS3.p7.m5" intent=":literal"><semantics><msub><mi>λ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\lambda_{t}</annotation></semantics></math> parameter settles around some value <math alttext="\lambda_{t}^{*}" class="ltx_Math" display="inline" id="A6.SS3.p7.m6" intent=":literal"><semantics><msubsup><mi>λ</mi><mi>t</mi><mo>∗</mo></msubsup><annotation encoding="application/x-tex">\lambda_{t}^{*}</annotation></semantics></math>, one should aim for the lower bound parameter <math alttext="\lambda_{t}^{\text{min}}" class="ltx_Math" display="inline" id="A6.SS3.p7.m7" intent=":literal"><semantics><msubsup><mi>λ</mi><mi>t</mi><mtext>min</mtext></msubsup><annotation encoding="application/x-tex">\lambda_{t}^{\text{min}}</annotation></semantics></math> to be smaller than <math alttext="\frac{1}{2}\lambda_{t}^{*}" class="ltx_Math" display="inline" id="A6.SS3.p7.m8" intent=":literal"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>λ</mi><mi>t</mi><mo>∗</mo></msubsup></mrow><annotation encoding="application/x-tex">\frac{1}{2}\lambda_{t}^{*}</annotation></semantics></math>, while making sure that the ramp-up time <math alttext="\frac{\log(\lambda_{t}^{*})-\log(\lambda_{t}^{\text{min}})}{\log(1+\delta_{\text{ACP}})}" class="ltx_Math" display="inline" id="A6.SS3.p7.m9" intent=":literal"><semantics><mfrac><mrow><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>λ</mi><mi>t</mi><mo>∗</mo></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>λ</mi><mi>t</mi><mtext>min</mtext></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msub><mi>δ</mi><mtext>ACP</mtext></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><annotation encoding="application/x-tex">\frac{\log(\lambda_{t}^{*})-\log(\lambda_{t}^{\text{min}})}{\log(1+\delta_{\text{ACP}})}</annotation></semantics></math> remains small. Settings of <math alttext="\lambda_{t}^{\text{min}}" class="ltx_Math" display="inline" id="A6.SS3.p7.m10" intent=":literal"><semantics><msubsup><mi>λ</mi><mi>t</mi><mtext>min</mtext></msubsup><annotation encoding="application/x-tex">\lambda_{t}^{\text{min}}</annotation></semantics></math> in the range <math alttext="[0.001,0.00001]" class="ltx_Math" display="inline" id="A6.SS3.p7.m11" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><mn>0.001</mn><mo>,</mo><mn>0.00001</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0.001,0.00001]</annotation></semantics></math> all produced largely the same result, the only difference being that values on the lower end of that range led to a larger amplitude in oscillations of <math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="A6.SS3.p7.m12" intent=":literal"><semantics><msub><mi>λ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\lambda_{t}</annotation></semantics></math> and <math alttext="a_{m}^{t}" class="ltx_Math" display="inline" id="A6.SS3.p7.m13" intent=":literal"><semantics><msubsup><mi>a</mi><mi>m</mi><mi>t</mi></msubsup><annotation encoding="application/x-tex">a_{m}^{t}</annotation></semantics></math>, but training eventually settled for all values. An example of some ACP dynamics is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A6.F14" title="Figure 14 ‣ F.3 Control of the penalty strength ‣ Appendix F Total correlation penalty ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">14</span></a>:</p>
</div>
<div class="ltx_para" id="A6.SS3.p8">
<p class="ltx_p">Training on Fashion-MNIST with the typical experimental setup, we observed nearly the same performance (a FID of <math alttext="28\pm 1" class="ltx_Math" display="inline" id="A6.SS3.p8.m1" intent=":literal"><semantics><mrow><mn>28</mn><mo>±</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">28\pm 1</annotation></semantics></math>) for all choices of <math alttext="\varepsilon_{\text{ACP}}" class="ltx_Math" display="inline" id="A6.SS3.p8.m2" intent=":literal"><semantics><msub><mi>ε</mi><mtext>ACP</mtext></msub><annotation encoding="application/x-tex">\varepsilon_{\text{ACP}}</annotation></semantics></math>, <math alttext="\delta_{\text{ACP}}" class="ltx_Math" display="inline" id="A6.SS3.p8.m3" intent=":literal"><semantics><msub><mi>δ</mi><mtext>ACP</mtext></msub><annotation encoding="application/x-tex">\delta_{\text{ACP}}</annotation></semantics></math> and <math alttext="\lambda_{t}^{\text{min}}" class="ltx_Math" display="inline" id="A6.SS3.p8.m4" intent=":literal"><semantics><msubsup><mi>λ</mi><mi>t</mi><mtext>min</mtext></msubsup><annotation encoding="application/x-tex">\lambda_{t}^{\text{min}}</annotation></semantics></math> in the ranges written above, so long as we trained for at least 100 epochs (with specific settings the training took longer to converge).</p>
</div>
<figure class="ltx_figure" id="A6.F14"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="475" id="A6.F14.g1" src="x14.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14: </span><span class="ltx_text ltx_font_bold">The active correlation penalty</span> The dynamics of <math alttext="r_{yy}^{t}" class="ltx_Math" display="inline" id="A6.F14.m6" intent=":literal"><semantics><msubsup><mi>r</mi><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mi>y</mi></mrow><mi>t</mi></msubsup><annotation encoding="application/x-tex">r_{yy}^{t}</annotation></semantics></math> and <math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="A6.F14.m7" intent=":literal"><semantics><msub><mi>λ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\lambda_{t}</annotation></semantics></math> over a training run. Large values of <math alttext="r_{yy}^{t}" class="ltx_Math" display="inline" id="A6.F14.m8" intent=":literal"><semantics><msubsup><mi>r</mi><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mi>y</mi></mrow><mi>t</mi></msubsup><annotation encoding="application/x-tex">r_{yy}^{t}</annotation></semantics></math> lead to increasingly large values of <math alttext="\lambda_{t}" class="ltx_Math" display="inline" id="A6.F14.m9" intent=":literal"><semantics><msub><mi>λ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\lambda_{t}</annotation></semantics></math>, which cause <math alttext="r_{yy}^{t}" class="ltx_Math" display="inline" id="A6.F14.m10" intent=":literal"><semantics><msubsup><mi>r</mi><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mi>y</mi></mrow><mi>t</mi></msubsup><annotation encoding="application/x-tex">r_{yy}^{t}</annotation></semantics></math> to decrease. The system reaches a stable configuration by the end of training.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Embedding integers into Boltzmann machines</h2>
<div class="ltx_para" id="A7.p1">
<p class="ltx_p">In some of our experiments, we needed to embed continuous data into binary variables. We chose to do this by representing a <math alttext="k" class="ltx_Math" display="inline" id="A7.p1.m1" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-state categorical variable <math alttext="X_{i}" class="ltx_Math" display="inline" id="A7.p1.m2" intent=":literal"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding="application/x-tex">X_{i}</annotation></semantics></math> using the sum <math alttext="k" class="ltx_Math" display="inline" id="A7.p1.m3" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> binary variables <math alttext="Z_{i}^{k}" class="ltx_Math" display="inline" id="A7.p1.m4" intent=":literal"><semantics><msubsup><mi>Z</mi><mi>i</mi><mi>k</mi></msubsup><annotation encoding="application/x-tex">Z_{i}^{k}</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="A7.E101">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="X_{i}=\sum_{k=1}^{K_{i}}Z_{i}^{(k)}" class="ltx_Math" display="block" id="A7.E101.m1" intent=":literal"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>K</mi><mi>i</mi></msub></munderover><msubsup><mi>Z</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow><annotation encoding="application/x-tex">X_{i}=\sum_{k=1}^{K_{i}}Z_{i}^{(k)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(101)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="Z_{i}^{(k)}\in\{0,1\}" class="ltx_Math" display="inline" id="A7.p1.m5" intent=":literal"><semantics><mrow><msubsup><mi>Z</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">Z_{i}^{(k)}\in\{0,1\}</annotation></semantics></math>. These binary variables can be trivially converted into spin variables that are <math alttext="\{-1,1\}" class="ltx_Math" display="inline" id="A7.p1.m6" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{-1,1\}</annotation></semantics></math> valued using a linear change of variables.</p>
</div>
<div class="ltx_para" id="A7.p2">
<p class="ltx_p">Energy functions that involve quadratic interactions between these categorical variables can be reduced to Boltzmann machines with local patches of all-to-all connectivity. For example, consider the energy function,</p>
<table class="ltx_equation ltx_eqn_table" id="A7.E102">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E(x;\theta)=-\sum_{i\neq j}w_{ij}X_{i}X_{j}-\sum_{i=1}^{d}b_{i}X_{i}" class="ltx_Math" display="block" id="A7.E102.m1" intent=":literal"><semantics><mrow><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>−</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>≠</mo><mi>j</mi></mrow></munder><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>X</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>X</mi><mi>j</mi></msub></mrow></mrow></mrow><mo rspace="0.055em">−</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>b</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>X</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">E(x;\theta)=-\sum_{i\neq j}w_{ij}X_{i}X_{j}-\sum_{i=1}^{d}b_{i}X_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(102)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">inserting Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A7.E101" title="Equation 101 ‣ Appendix G Embedding integers into Boltzmann machines ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">101</span></a>), we can rewrite this in terms of quadratic interactions between the underlying spins <math alttext="Z_{i}^{(k)}" class="ltx_Math" display="inline" id="A7.p2.m1" intent=":literal"><semantics><msubsup><mi>Z</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">Z_{i}^{(k)}</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="A7.E103">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E(z;\theta)=-\sum_{i\neq j}w_{ij}\left(\sum_{k=1}^{K_{i}}Z_{i}^{(k)}\right)\left(\sum_{l=1}^{K_{j}}Z_{j}^{(l)}\right)-\sum_{i}b_{i}\left(\sum_{k=1}^{K_{i}}Z_{i}^{(k)}\right)" class="ltx_Math" display="block" id="A7.E103.m1" intent=":literal"><semantics><mrow><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo>;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>−</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>≠</mo><mi>j</mi></mrow></munder><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><munderover><mo lspace="0em" movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>K</mi><mi>i</mi></msub></munderover><msubsup><mi>Z</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><munderover><mo lspace="0em" movablelimits="false">∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>K</mi><mi>j</mi></msub></munderover><msubsup><mi>Z</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo rspace="0.055em">−</mo><mrow><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder><mrow><msub><mi>b</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><munderover><mo lspace="0em" movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>K</mi><mi>i</mi></msub></munderover><msubsup><mi>Z</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">E(z;\theta)=-\sum_{i\neq j}w_{ij}\left(\sum_{k=1}^{K_{i}}Z_{i}^{(k)}\right)\left(\sum_{l=1}^{K_{j}}Z_{j}^{(l)}\right)-\sum_{i}b_{i}\left(\sum_{k=1}^{K_{i}}Z_{i}^{(k)}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(103)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which is a standard Boltzmann machine energy function that can be run on our hardware, just like any other.</p>
</div>
</section>
<section class="ltx_appendix" id="A8">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>The autocorrelation function and mixing time</h2>
<div class="ltx_para" id="A8.p1">
<p class="ltx_p">This section gives a brief derivation of how the Mixing time of a Markov chain can be estimated using its autocorrelation. Proofs of the properties noted here can be found in most standard textbooks on Markov chains, such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib23" title="Markov Chains and Mixing Times">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="A8.p2">
<p class="ltx_p">Suppose <math alttext="X" class="ltx_Math" display="inline" id="A8.p2.m1" intent=":literal"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> is a discrete-time Markov chain on a finite state space <math alttext="\{1,\ldots,d\}" class="ltx_Math" display="inline" id="A8.p2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>d</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{1,\ldots,d\}</annotation></semantics></math>. Suppose its transition kernel is time-homogeneous and given by a matrix <math alttext="P=\left(p_{xy}\right)_{1\leq x,y\leq d}" class="ltx_Math" display="inline" id="A8.p2.m3" intent=":literal"><semantics><mrow><mi>P</mi><mo>=</mo><msub><mrow><mo>(</mo><msub><mi>p</mi><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mi>y</mi></mrow></msub><mo>)</mo></mrow><mrow><mrow><mn>1</mn><mo>≤</mo><mi>x</mi></mrow><mo>,</mo><mrow><mi>y</mi><mo>≤</mo><mi>d</mi></mrow></mrow></msub></mrow><annotation encoding="application/x-tex">P=\left(p_{xy}\right)_{1\leq x,y\leq d}</annotation></semantics></math> with entries <math alttext="p_{xy}=\mathbb{P}\left(X_{t+1}=y|X_{t}=x\right)" class="ltx_Math" display="inline" id="A8.p2.m4" intent=":literal"><semantics><mrow><msub><mi>p</mi><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mi>y</mi></mrow></msub><mo>=</mo><mrow><mi mathvariant="normal">ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>X</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mi>y</mi><mo fence="false">|</mo><msub><mi>X</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>x</mi></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">p_{xy}=\mathbb{P}\left(X_{t+1}=y|X_{t}=x\right)</annotation></semantics></math>. Furthermore, assume the Markov chain is:</p>
<ul class="ltx_itemize" id="A8.I1">
<li class="ltx_item" id="A8.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A8.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">irreducible</span>, i.e. it is possible to get from any starting node to any other node in a finite number of steps, and</p>
</div>
</li>
<li class="ltx_item" id="A8.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A8.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">aperiodic</span>, i.e. for any starting position <math alttext="x\in\{1,\ldots,d\}" class="ltx_Math" display="inline" id="A8.I1.i2.p1.m1" intent=":literal"><semantics><mrow><mi>x</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>d</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">x\in\{1,\ldots,d\}</annotation></semantics></math>, there exists some <math alttext="T\in\mathbb{N}" class="ltx_Math" display="inline" id="A8.I1.i2.p1.m2" intent=":literal"><semantics><mrow><mi>T</mi><mo>∈</mo><mi mathvariant="normal">ℕ</mi></mrow><annotation encoding="application/x-tex">T\in\mathbb{N}</annotation></semantics></math> such that for all <math alttext="t\geq T" class="ltx_Math" display="inline" id="A8.I1.i2.p1.m3" intent=":literal"><semantics><mrow><mi>t</mi><mo>≥</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">t\geq T</annotation></semantics></math>, <math alttext="\mathbb{P}\left(X_{t}=x|X_{0}=x\right)&gt;0" class="ltx_Math" display="inline" id="A8.I1.i2.p1.m4" intent=":literal"><semantics><mrow><mrow><mi mathvariant="normal">ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>X</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>x</mi><mo fence="false">|</mo><msub><mi>X</mi><mn>0</mn></msub></mrow><mo>=</mo><mi>x</mi></mrow><mo>)</mo></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathbb{P}\left(X_{t}=x|X_{0}=x\right)&gt;0</annotation></semantics></math>.</p>
</div>
</li>
</ul>
<p class="ltx_p">Since this Markov chain is defined on a finite state space and is irreducible, there exists a unique stationary distribution <math alttext="\pi=\pi P" class="ltx_Math" display="inline" id="A8.p2.m5" intent=":literal"><semantics><mrow><mi>π</mi><mo>=</mo><mrow><mi>π</mi><mo lspace="0em" rspace="0em">​</mo><mi>P</mi></mrow></mrow><annotation encoding="application/x-tex">\pi=\pi P</annotation></semantics></math>. Furthermore, as a consequence of aperiodicity, for any starting distribution <math alttext="\psi_{0}" class="ltx_Math" display="inline" id="A8.p2.m6" intent=":literal"><semantics><msub><mi>ψ</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\psi_{0}</annotation></semantics></math>, the distribution <math alttext="\psi_{t}" class="ltx_Math" display="inline" id="A8.p2.m7" intent=":literal"><semantics><msub><mi>ψ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\psi_{t}</annotation></semantics></math> of the Markov chain at time <math alttext="t" class="ltx_Math" display="inline" id="A8.p2.m8" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> converges to <math alttext="\pi" class="ltx_Math" display="inline" id="A8.p2.m9" intent=":literal"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> as <math alttext="t\rightarrow\infty" class="ltx_Math" display="inline" id="A8.p2.m10" intent=":literal"><semantics><mrow><mi>t</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">t\rightarrow\infty</annotation></semantics></math>, that is</p>
<table class="ltx_equation ltx_eqn_table" id="A8.Ex4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\psi_{t}=\psi_{0}P^{t}\rightarrow\pi\;\text{ as }\;t\rightarrow\infty." class="ltx_Math" display="block" id="A8.Ex4.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>ψ</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>ψ</mi><mn>0</mn></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>P</mi><mi>t</mi></msup></mrow><mo stretchy="false">→</mo><mrow><mi>π</mi><mo lspace="0.280em" rspace="0em">​</mo><mtext> as </mtext><mo lspace="0.280em" rspace="0em">​</mo><mi>t</mi></mrow><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\psi_{t}=\psi_{0}P^{t}\rightarrow\pi\;\text{ as }\;t\rightarrow\infty.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A8.p3">
<p class="ltx_p">The transition matrix can be diagonalized as <math alttext="P=U^{-1}\Sigma\,U" class="ltx_Math" display="inline" id="A8.p3.m1" intent=":literal"><semantics><mrow><mi>P</mi><mo>=</mo><mrow><msup><mi>U</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">Σ</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>U</mi></mrow></mrow><annotation encoding="application/x-tex">P=U^{-1}\Sigma\,U</annotation></semantics></math>, where <math alttext="\Sigma" class="ltx_Math" display="inline" id="A8.p3.m2" intent=":literal"><semantics><mi mathvariant="normal">Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math> is a diagonal matrix and without loss of generality, we can assume that its entries are ordered <math alttext="1=\sigma_{1}&gt;\sigma_{2}\geq\ldots\geq\sigma_{d}\geq 0" class="ltx_Math" display="inline" id="A8.p3.m3" intent=":literal"><semantics><mrow><mn>1</mn><mo>=</mo><msub><mi>σ</mi><mn>1</mn></msub><mo>&gt;</mo><msub><mi>σ</mi><mn>2</mn></msub><mo>≥</mo><mi mathvariant="normal">…</mi><mo>≥</mo><msub><mi>σ</mi><mi>d</mi></msub><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">1=\sigma_{1}&gt;\sigma_{2}\geq\ldots\geq\sigma_{d}\geq 0</annotation></semantics></math>. Then the <math alttext="t" class="ltx_Math" display="inline" id="A8.p3.m4" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>th power of <math alttext="P" class="ltx_Math" display="inline" id="A8.p3.m5" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> can be written as <math alttext="P^{t}=U^{-1}\Sigma^{t}U" class="ltx_Math" display="inline" id="A8.p3.m6" intent=":literal"><semantics><mrow><msup><mi>P</mi><mi>t</mi></msup><mo>=</mo><mrow><msup><mi>U</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi mathvariant="normal">Σ</mi><mi>t</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>U</mi></mrow></mrow><annotation encoding="application/x-tex">P^{t}=U^{-1}\Sigma^{t}U</annotation></semantics></math>. We write <math alttext="U(i,x)" class="ltx_Math" display="inline" id="A8.p3.m7" intent=":literal"><semantics><mrow><mi>U</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">U(i,x)</annotation></semantics></math> for the entry in the <math alttext="i" class="ltx_Math" display="inline" id="A8.p3.m8" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th row and <math alttext="x" class="ltx_Math" display="inline" id="A8.p3.m9" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>th column of <math alttext="U" class="ltx_Math" display="inline" id="A8.p3.m10" intent=":literal"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math> (and likewise for <math alttext="U^{-1}" class="ltx_Math" display="inline" id="A8.p3.m11" intent=":literal"><semantics><msup><mi>U</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">U^{-1}</annotation></semantics></math>). The first left eigenvector of <math alttext="P" class="ltx_Math" display="inline" id="A8.p3.m12" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> (and hence the first row of <math alttext="U" class="ltx_Math" display="inline" id="A8.p3.m13" intent=":literal"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math>) is the stationary distribution <math alttext="\pi=\pi P=U(1,\cdot)" class="ltx_Math" display="inline" id="A8.p3.m14" intent=":literal"><semantics><mrow><mi>π</mi><mo>=</mo><mrow><mi>π</mi><mo lspace="0em" rspace="0em">​</mo><mi>P</mi></mrow><mo>=</mo><mrow><mi>U</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\pi=\pi P=U(1,\cdot)</annotation></semantics></math>. The first right eigenvector of <math alttext="P" class="ltx_Math" display="inline" id="A8.p3.m15" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> is a column of ones, that is <math alttext="U^{-1}(\cdot,1)=\mathbf{1}" class="ltx_Math" display="inline" id="A8.p3.m16" intent=":literal"><semantics><mrow><mrow><msup><mi>U</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>𝟏</mn></mrow><annotation encoding="application/x-tex">U^{-1}(\cdot,1)=\mathbf{1}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A8.p4">
<p class="ltx_p">Let <math alttext="f:\{1,\ldots,d\}\rightarrow\mathbb{R}" class="ltx_Math" display="inline" id="A8.p4.m1" intent=":literal"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>d</mi><mo stretchy="false">}</mo></mrow><mo stretchy="false">→</mo><mi mathvariant="normal">ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">f:\{1,\ldots,d\}\rightarrow\mathbb{R}</annotation></semantics></math> be any function and write,</p>
<table class="ltx_equation ltx_eqn_table" id="A8.E104">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mu^{f}_{0}\equiv\mathbb{E}_{Y\sim\psi_{0}}\left[f(Y)\right]=\mathbb{E}\left[f(X_{0})\right]" class="ltx_Math" display="block" id="A8.E104.m1" intent=":literal"><semantics><mrow><msubsup><mi>μ</mi><mn>0</mn><mi>f</mi></msubsup><mo>≡</mo><mrow><msub><mi mathvariant="normal">𝔼</mi><mrow><mi>Y</mi><mo>∼</mo><msub><mi>ψ</mi><mn>0</mn></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mi mathvariant="normal">𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mu^{f}_{0}\equiv\mathbb{E}_{Y\sim\psi_{0}}\left[f(Y)\right]=\mathbb{E}\left[f(X_{0})\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(104)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\psi_{0}" class="ltx_Math" display="inline" id="A8.p4.m2" intent=":literal"><semantics><msub><mi>ψ</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\psi_{0}</annotation></semantics></math> is the initial distribution of the Markov chain <math alttext="X" class="ltx_Math" display="inline" id="A8.p4.m3" intent=":literal"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> and,</p>
<table class="ltx_equation ltx_eqn_table" id="A8.E105">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mu^{f}_{\infty}\equiv\mathbb{E}_{Y\sim\pi}\left[f(Y)\right]=\lim_{t\rightarrow\infty}\mathbb{E}\left[f(X_{t})\right]" class="ltx_Math" display="block" id="A8.E105.m1" intent=":literal"><semantics><mrow><msubsup><mi>μ</mi><mi mathvariant="normal">∞</mi><mi>f</mi></msubsup><mo>≡</mo><mrow><msub><mi mathvariant="normal">𝔼</mi><mrow><mi>Y</mi><mo>∼</mo><mi>π</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo rspace="0.1389em">=</mo><mrow><munder><mo lspace="0.1389em" movablelimits="false" rspace="0.167em">lim</mo><mrow><mi>t</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow></munder><mrow><mi mathvariant="normal">𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mu^{f}_{\infty}\equiv\mathbb{E}_{Y\sim\pi}\left[f(Y)\right]=\lim_{t\rightarrow\infty}\mathbb{E}\left[f(X_{t})\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(105)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A8.p5">
<p class="ltx_p">Then, write <math alttext="\delta_{k}" class="ltx_Math" display="inline" id="A8.p5.m1" intent=":literal"><semantics><msub><mi>δ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\delta_{k}</annotation></semantics></math> for the column vector with a one at <math alttext="k" class="ltx_Math" display="inline" id="A8.p5.m2" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>th entry and zeros elsewhere. We can then compute,</p>
<table class="ltx_equation ltx_eqn_table" id="A8.E106">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}\mathbb{E}\left[f(X_{0})f(X_{t})\right]&amp;=\sum_{x_{0}=1}^{d}f(x_{0})\mathbb{P}\left[X_{0}=x_{0}\right]\mathbb{E}\left[f(X_{t})|X_{0}=x_{0}\right]=\sum_{x_{0}=1}^{d}\sum_{x=1}^{d}f(x_{0})f(x)\,\psi_{0}(x_{0})\,(\delta^{T}_{x_{0}}P^{t})(x)\\[5.69054pt]
&amp;=\sum_{x_{0}=1}^{d}\sum_{x=1}^{d}f(x_{0})f(x)\psi_{0}(x_{0})\,\sum_{j=1}^{d}U^{-1}(x_{0},j)\,\sigma_{j}^{t}\,U(j,x)\\[5.69054pt]
&amp;=\left(\sum_{x_{0}=1}^{d}\sum_{x=1}^{d}f(x_{0})f(x)\psi_{0}(x_{0})U^{-1}(x_{0},1)\,\sigma_{1}^{t}\,U(1,x)\right)+\\
&amp;\quad+\sum_{x_{0}=1}^{d}\sum_{x=1}^{d}f(x_{0})f(x)\psi_{0}(x_{0})\,\sum_{j=2}^{d}U^{-1}(x_{0},j)\,\sigma_{j}^{t}\,U(j,x)\\[5.69054pt]
&amp;=\left(\sum_{x_{0}=1}^{d}\sum_{x=1}^{d}f(x_{0})f(x)\psi_{0}(x_{0})\pi(x)\right)+\sum_{j=2}^{d}\sigma_{j}^{t}\sum_{x_{0}=1}^{d}\sum_{x=1}^{d}f(x_{0})f(x)\psi_{0}(x_{0})\,U^{-1}(x_{0},j)\,U(j,x)\\[5.69054pt]
&amp;=\mu^{f}_{0}\mu^{f}_{\infty}+\sum_{j=2}^{d}\sigma_{j}^{t}c_{j},\end{split}" class="ltx_Math" display="block" id="A8.E106.m1" intent=":literal"><semantics><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><mrow><mi mathvariant="normal">𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><msub><mi>X</mi><mn>0</mn></msub><mo>=</mo><msub><mi>x</mi><mn>0</mn></msub></mrow><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo fence="false">|</mo><msub><mi>X</mi><mn>0</mn></msub></mrow><mo>=</mo><msub><mi>x</mi><mn>0</mn></msub></mrow><mo>]</mo></mrow></mrow></mrow><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.170em" rspace="0em">​</mo><msub><mi>ψ</mi><mn>0</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0.170em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>δ</mi><msub><mi>x</mi><mn>0</mn></msub><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>P</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>ψ</mi><mn>0</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0.337em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><msup><mi>U</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.170em" rspace="0em">​</mo><msubsup><mi>σ</mi><mi>j</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>U</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>=</mo><mrow><mrow><mo>(</mo><mrow><munderover><mo lspace="0em" movablelimits="false" rspace="0em">∑</mo><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>ψ</mi><mn>0</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>U</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo lspace="0.170em" rspace="0em">​</mo><msubsup><mi>σ</mi><mn>1</mn><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>U</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow><mo>+</mo></mrow></mrow></mtd></mtr><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mo>+</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>ψ</mi><mn>0</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0.337em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><mi>d</mi></munderover><mrow><msup><mi>U</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.170em" rspace="0em">​</mo><msubsup><mi>σ</mi><mi>j</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>U</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>=</mo><mrow><mrow><mo>(</mo><mrow><munderover><mo lspace="0em" movablelimits="false" rspace="0em">∑</mo><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>ψ</mi><mn>0</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>π</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow><mo rspace="0.055em">+</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><mi>d</mi></munderover><mrow><msubsup><mi>σ</mi><mi>j</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>ψ</mi><mn>0</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0.170em" rspace="0em">​</mo><msup><mi>U</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.170em" rspace="0em">​</mo><mi>U</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><msubsup><mi>μ</mi><mn>0</mn><mi>f</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>μ</mi><mi mathvariant="normal">∞</mi><mi>f</mi></msubsup></mrow><mo rspace="0.055em">+</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><mi>d</mi></munderover><mrow><msubsup><mi>σ</mi><mi>j</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>c</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{split}\mathbb{E}\left[f(X_{0})f(X_{t})\right]&amp;=\sum_{x_{0}=1}^{d}f(x_{0})\mathbb{P}\left[X_{0}=x_{0}\right]\mathbb{E}\left[f(X_{t})|X_{0}=x_{0}\right]=\sum_{x_{0}=1}^{d}\sum_{x=1}^{d}f(x_{0})f(x)\,\psi_{0}(x_{0})\,(\delta^{T}_{x_{0}}P^{t})(x)\\[5.69054pt]
&amp;=\sum_{x_{0}=1}^{d}\sum_{x=1}^{d}f(x_{0})f(x)\psi_{0}(x_{0})\,\sum_{j=1}^{d}U^{-1}(x_{0},j)\,\sigma_{j}^{t}\,U(j,x)\\[5.69054pt]
&amp;=\left(\sum_{x_{0}=1}^{d}\sum_{x=1}^{d}f(x_{0})f(x)\psi_{0}(x_{0})U^{-1}(x_{0},1)\,\sigma_{1}^{t}\,U(1,x)\right)+\\
&amp;\quad+\sum_{x_{0}=1}^{d}\sum_{x=1}^{d}f(x_{0})f(x)\psi_{0}(x_{0})\,\sum_{j=2}^{d}U^{-1}(x_{0},j)\,\sigma_{j}^{t}\,U(j,x)\\[5.69054pt]
&amp;=\left(\sum_{x_{0}=1}^{d}\sum_{x=1}^{d}f(x_{0})f(x)\psi_{0}(x_{0})\pi(x)\right)+\sum_{j=2}^{d}\sigma_{j}^{t}\sum_{x_{0}=1}^{d}\sum_{x=1}^{d}f(x_{0})f(x)\psi_{0}(x_{0})\,U^{-1}(x_{0},j)\,U(j,x)\\[5.69054pt]
&amp;=\mu^{f}_{0}\mu^{f}_{\infty}+\sum_{j=2}^{d}\sigma_{j}^{t}c_{j},\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(106)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="c_{j}" class="ltx_Math" display="inline" id="A8.p5.m3" intent=":literal"><semantics><msub><mi>c</mi><mi>j</mi></msub><annotation encoding="application/x-tex">c_{j}</annotation></semantics></math> are constants independent of <math alttext="t" class="ltx_Math" display="inline" id="A8.p5.m4" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>. Hence, if we can plot the quantity <math alttext="\operatorname*{\mathbb{E}}{f(X_{0})f(X_{t})}-\mu^{f}_{0}\mu^{f}_{\infty}" class="ltx_Math" display="inline" id="A8.p5.m5" intent=":literal"><semantics><mrow><mrow><mrow><mo rspace="0.167em">𝔼</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msubsup><mi>μ</mi><mn>0</mn><mi>f</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>μ</mi><mi mathvariant="normal">∞</mi><mi>f</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">\operatorname*{\mathbb{E}}{f(X_{0})f(X_{t})}-\mu^{f}_{0}\mu^{f}_{\infty}</annotation></semantics></math> for very large <math alttext="t" class="ltx_Math" display="inline" id="A8.p5.m6" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, the contributions of the smaller eigenvalues become negligible relative to the contribution of <math alttext="\sigma_{2}" class="ltx_Math" display="inline" id="A8.p5.m7" intent=":literal"><semantics><msub><mi>σ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\sigma_{2}</annotation></semantics></math>, allowing us to estimate the value of <math alttext="\sigma_{2}" class="ltx_Math" display="inline" id="A8.p5.m8" intent=":literal"><semantics><msub><mi>σ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\sigma_{2}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A8.p6">
<p class="ltx_p">Why does knowing <math alttext="\sigma_{2}" class="ltx_Math" display="inline" id="A8.p6.m1" intent=":literal"><semantics><msub><mi>σ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\sigma_{2}</annotation></semantics></math> help us compute the mixing time? Recall that the mixing time <math alttext="\tau" class="ltx_Math" display="inline" id="A8.p6.m2" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math> is defined as</p>
<table class="ltx_equation ltx_eqn_table" id="A8.Ex5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tau(\varepsilon)=\min\left\{t\geq 0:\max_{\psi_{0}}\|\psi_{0}P^{t}-\pi\|_{\text{TV}}\leq\varepsilon\right\}," class="ltx_Math" display="block" id="A8.Ex5.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>τ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>ε</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo>{</mo><mrow><mrow><mi>t</mi><mo>≥</mo><mn>0</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><munder><mi>max</mi><msub><mi>ψ</mi><mn>0</mn></msub></munder><mo>⁡</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><mrow><msub><mi>ψ</mi><mn>0</mn></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>P</mi><mi>t</mi></msup></mrow><mo>−</mo><mi>π</mi></mrow><mo stretchy="false">‖</mo></mrow><mtext>TV</mtext></msub></mrow><mo>≤</mo><mi>ε</mi></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\tau(\varepsilon)=\min\left\{t\geq 0:\max_{\psi_{0}}\|\psi_{0}P^{t}-\pi\|_{\text{TV}}\leq\varepsilon\right\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><math alttext="\|\mu-\nu\|_{\text{TV}}=\frac{1}{2}\sum_{y\in\mathcal{X}}|\mu(y)-\nu(y)|" class="ltx_Math" display="inline" id="A8.p6.m3" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>μ</mi><mo>−</mo><mi>ν</mi></mrow><mo stretchy="false">‖</mo></mrow><mtext>TV</mtext></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><msub><mo rspace="0em">∑</mo><mrow><mi>y</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒳</mi></mrow></msub><mrow><mo stretchy="false">|</mo><mrow><mrow><mi>μ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>ν</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">|</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\|\mu-\nu\|_{\text{TV}}=\frac{1}{2}\sum_{y\in\mathcal{X}}|\mu(y)-\nu(y)|</annotation></semantics></math> denotes the total variation distance, and <math alttext="\varepsilon&gt;0" class="ltx_Math" display="inline" id="A8.p6.m4" intent=":literal"><semantics><mrow><mi>ε</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\varepsilon&gt;0</annotation></semantics></math> is a prescribed tolerance.</p>
</div>
<div class="ltx_para" id="A8.p7">
<p class="ltx_p">We can rewrite the total variation distance in this definition as</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A11.EGx3">
<tbody id="A8.Ex6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\|\psi_{0}P^{t}-\pi\|_{\text{TV}}" class="ltx_Math" display="inline" id="A8.Ex6.m1" intent=":literal"><semantics><msub><mrow><mo stretchy="false">‖</mo><mrow><mrow><msub><mi>ψ</mi><mn>0</mn></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>P</mi><mi>t</mi></msup></mrow><mo>−</mo><mi>π</mi></mrow><mo stretchy="false">‖</mo></mrow><mtext>TV</mtext></msub><annotation encoding="application/x-tex">\displaystyle\|\psi_{0}P^{t}-\pi\|_{\text{TV}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{2}\sum_{x=1}^{d}\left|\pi(x)-\sum_{j=1}^{d}\big(\psi_{0}\cdot U^{-1}(\cdot,j)\big)\,\sigma_{j}^{t}\,U(j,x)\right|" class="ltx_Math" display="inline" id="A8.Ex6.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><mrow><mo>|</mo><mrow><mrow><mi>π</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><mrow><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mrow><mrow><msub><mi>ψ</mi><mn>0</mn></msub><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mi>U</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow><mo lspace="0.170em" rspace="0em">​</mo><msubsup><mi>σ</mi><mi>j</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>U</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>|</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{2}\sum_{x=1}^{d}\left|\pi(x)-\sum_{j=1}^{d}\big(\psi_{0}\cdot U^{-1}(\cdot,j)\big)\,\sigma_{j}^{t}\,U(j,x)\right|</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\text{( }\psi_{0}\cdot U^{-1}(\cdot,1)=1\text{ and }U(1,\cdot)=\pi\text{ )}" class="ltx_Math" display="inline" id="A8.Ex6.m3" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mtext>( </mtext><mo lspace="0em" rspace="0em">​</mo><msub><mi>ψ</mi><mn>0</mn></msub></mrow><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mi>U</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mtext> and </mtext><mo lspace="0em" rspace="0em">​</mo><mi>U</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>π</mi><mo lspace="0em" rspace="0em">​</mo><mtext> )</mtext></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\text{( }\psi_{0}\cdot U^{-1}(\cdot,1)=1\text{ and }U(1,\cdot)=\pi\text{ )}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A8.Ex7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{2}\sum_{x=1}^{d}\left|\pi(x)-\pi(x)+\sum_{j=2}^{d}\big(\psi_{0}\cdot U^{-1}(\cdot,j)\big)\,\sigma_{j}^{t}\,U(j,x)\right|" class="ltx_Math" display="inline" id="A8.Ex7.m1" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><mrow><mo>|</mo><mrow><mrow><mrow><mi>π</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>π</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><mi>d</mi></munderover></mstyle><mrow><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mrow><mrow><msub><mi>ψ</mi><mn>0</mn></msub><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mi>U</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow><mo lspace="0.170em" rspace="0em">​</mo><msubsup><mi>σ</mi><mi>j</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>U</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>|</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{1}{2}\sum_{x=1}^{d}\left|\pi(x)-\pi(x)+\sum_{j=2}^{d}\big(\psi_{0}\cdot U^{-1}(\cdot,j)\big)\,\sigma_{j}^{t}\,U(j,x)\right|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright" colspan="2"></td>
</tr></tbody>
<tbody id="A8.Ex8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\leq\frac{1}{2}\sum_{x=1}^{d}\sum_{j=2}^{d}\big|\psi_{0}\cdot U^{-1}(\cdot,j)\big|\,|\sigma_{j}^{t}|\,\big|U(j,x)\big|" class="ltx_Math" display="inline" id="A8.Ex8.m1" intent=":literal"><semantics><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><mi>d</mi></munderover></mstyle><mrow><mrow><mo maxsize="1.200em" minsize="1.200em" stretchy="true">|</mo><mrow><mrow><msub><mi>ψ</mi><mn>0</mn></msub><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mi>U</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="1.200em" minsize="1.200em" stretchy="true">|</mo></mrow><mo lspace="0.170em" rspace="0em">​</mo><mrow><mo stretchy="false">|</mo><msubsup><mi>σ</mi><mi>j</mi><mi>t</mi></msubsup><mo stretchy="false">|</mo></mrow><mo lspace="0.170em" rspace="0em">​</mo><mrow><mo maxsize="1.200em" minsize="1.200em" stretchy="true">|</mo><mrow><mi>U</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="1.200em" minsize="1.200em" stretchy="true">|</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\leq\frac{1}{2}\sum_{x=1}^{d}\sum_{j=2}^{d}\big|\psi_{0}\cdot U^{-1}(\cdot,j)\big|\,|\sigma_{j}^{t}|\,\big|U(j,x)\big|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright" colspan="2"></td>
</tr></tbody>
<tbody id="A8.Ex9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sum_{j=2}^{d}\sigma_{j}^{t}\,a_{j}\leq\sigma^{t}_{2}\sum_{j=2}^{d}a_{j}" class="ltx_Math" display="inline" id="A8.Ex9.m1" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><mi>d</mi></munderover></mstyle><mrow><msubsup><mi>σ</mi><mi>j</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>a</mi><mi>j</mi></msub></mrow></mrow><mo>≤</mo><mrow><msubsup><mi>σ</mi><mn>2</mn><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><mi>d</mi></munderover></mstyle><msub><mi>a</mi><mi>j</mi></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\sum_{j=2}^{d}\sigma_{j}^{t}\,a_{j}\leq\sigma^{t}_{2}\sum_{j=2}^{d}a_{j}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright" colspan="2"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="a_{j}" class="ltx_Math" display="inline" id="A8.p7.m1" intent=":literal"><semantics><msub><mi>a</mi><mi>j</mi></msub><annotation encoding="application/x-tex">a_{j}</annotation></semantics></math> are some non-negative constants. Therefore, we can establish the following upper bound:</p>
<table class="ltx_equation ltx_eqn_table" id="A8.Ex10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tau(\varepsilon)\leq\frac{\log(\varepsilon)-\log\left(\sum_{j=2}^{d}a_{j}\right)}{\log(\sigma_{2})}." class="ltx_Math" display="block" id="A8.Ex10.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>τ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>ε</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mfrac><mrow><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>ε</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><msubsup><mo lspace="0em">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><mi>d</mi></msubsup><msub><mi>a</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>σ</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\tau(\varepsilon)\leq\frac{\log(\varepsilon)-\log\left(\sum_{j=2}^{d}a_{j}\right)}{\log(\sigma_{2})}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The smaller the value of <math alttext="\sigma_{2}" class="ltx_Math" display="inline" id="A8.p7.m2" intent=":literal"><semantics><msub><mi>σ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\sigma_{2}</annotation></semantics></math>, the faster the mixing time.</p>
</div>
</section>
<section class="ltx_appendix" id="A9">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Deterministic embeddings for DTMs</h2>
<div class="ltx_para" id="A9.p1">
<p class="ltx_p">In Section V of the paper, we mention hybrid thermodynamic models, the purpose of which is to combine the flexibility of classical neural networks (NNs) with the efficiency of probabilistic computers. For example, in the context of image generation, a small convolutional neural network can be used to map color images into a format compatible with a binary DTM. To properly take advantage of the DTM’s energy efficiency, the classical model should be at least 2 or 3 orders of magnitude smaller (e.g., in terms of parameter count or number of operations per sample) than the DTM.</p>
</div>
<div class="ltx_para" id="A9.p2">
<p class="ltx_p">There are various options for the type of classical model one can use for the embedding, e.g., invertible models such as GLOW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib16" title="Glow: Generative Flow with Invertible 1x1 Convolutions">5</a>]</cite> or Normalizing Flows <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib17" title="Normalizing Flows for Probabilistic Modeling and Inference">10</a>]</cite>, as well as simpler solutions, such as an Autoencoder.</p>
</div>
<div class="ltx_para" id="A9.p3">
<p class="ltx_p">For our proof-of-concept for hybrid models, we used a combination of an Autoencoder and a GAN <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">Goodfellow2014GAN</span>]</cite>.</p>
<ul class="ltx_itemize" id="A9.I1">
<li class="ltx_item" id="A9.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A9.I1.i1.p1">
<p class="ltx_p">First, we train a convolutional Autoencoder (encoder plus decoder) that maps images into a binary latent space (achieved through a combination of a sigmoid activation, a binarization penalty, and a straight-through gradient).</p>
</div>
</li>
<li class="ltx_item" id="A9.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A9.I1.i2.p1">
<p class="ltx_p">Second, we train a DTM on latent embeddings of the training images. At inference time, the samples generated by the DTM are passed through the decoder to produce images.</p>
</div>
</li>
<li class="ltx_item" id="A9.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A9.I1.i3.p1">
<p class="ltx_p">Thirdly, we use a GAN-like approach to fine-tune the decoder to utilize the outputs of the Boltzmann machine maximally. Specifically, the Boltzmann machine outputs are used as the noise source, which is fed into the decoder (now taking the role of the generator in the GAN architecture), and finally, a critic is trained to guide the decoder towards generating higher-quality images.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A9.p4">
<p class="ltx_p">Our hybrid model achieved a FID score of <math alttext="\sim 60" class="ltx_Math" display="inline" id="A9.p4.m1" intent=":literal"><semantics><mrow><mi></mi><mo>∼</mo><mn>60</mn></mrow><annotation encoding="application/x-tex">\sim 60</annotation></semantics></math> on CIFAR10. Our DTM had 8 million parameters, the decoder had 65k, and the encoder and critic were both below 500k parameters. At inference time, only the DTM and the decoder are used. To achieve a similar performance with a classical GAN, the decoder/generator requires about 500000 parameters.</p>
</div>
</section>
<section class="ltx_appendix" id="A10">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix J </span>Some details on our RNG</h2>
<div class="ltx_para" id="A10.p1">
<p class="ltx_p">Our RNG is a digitizing comparator fed by a source of Gaussian noise. The noise source is implemented using the circuitry and principles described in <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">our_gyrator</span>]</cite>. The comparator is a standard design that operates in subthreshold to minimize energy consumption. The mean of the Gaussian noise is shifted before it is sent into the comparator to implement the bias control. A schematic of our RNG is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A10.F15" title="Figure 15 ‣ Appendix J Some details on our RNG ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">15</span></a> (a).</p>
</div>
<div class="ltx_para" id="A10.p2">
<p class="ltx_p">Another example of an output voltage signal from our RNG is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A10.F15" title="Figure 15 ‣ Appendix J Some details on our RNG ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">15</span></a> (b). The signal randomly wanders between high and low-voltage states. Suppose this signal is repeatedly observed, waiting for at least the correlation time of the circuit between observations. In that case, one will approximately draw samples from a Bernoulli distribution with a bias parameter that depends on the circuit’s control voltage.</p>
</div>
<div class="ltx_para" id="A10.p3">
<p class="ltx_p">Our RNG was part of the same test chip used to carry out the experiments in <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">our_gyrator</span>]</cite>. The output of the RNG was fed into an amplification chain that buffered it and allowed its signal to be observed using an external oscilloscope. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A10.F15" title="Figure 15 ‣ Appendix J Some details on our RNG ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">15</span></a> (c) shows an image of our packaged test chip, along with a view of our RNG through a <math alttext="100\times" class="ltx_math_unparsed" display="inline" id="A10.p3.m1" intent=":literal"><semantics><mrow><mn>100</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">100\times</annotation></semantics></math> microscope objective.</p>
</div>
<figure class="ltx_figure" id="A10.F15"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="276" id="A10.F15.g1" src="x15.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 15: </span><span class="ltx_text ltx_font_bold">Our RNG</span>. <span class="ltx_text ltx_font_bold">(a)</span> A high-level schematic of our RNG design. <span class="ltx_text ltx_font_bold">(b)</span> Stochastic voltage signal from our RNG. The high level represents one, and the low level represents 0. The signal wanders randomly between high and low levels, with the amount of time it spends in each level controlled by the bias voltage. <span class="ltx_text ltx_font_bold">(c)</span> An image of our packaged test chip (with the top of the package removed) assembled onto a PCB. We also show an optical microscope image of several RNG circuits on our test chip. Each circuit occupies an approximately <math alttext="3\times 3\mu m" class="ltx_Math" display="inline" id="A10.F15.m2" intent=":literal"><semantics><mrow><mrow><mn>3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>3</mn></mrow><mo lspace="0em" rspace="0em">​</mo><mi>μ</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">3\times 3\mu m</annotation></semantics></math> area on the chip.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A11">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix K </span>MEBM experiments</h2>
<div class="ltx_para" id="A11.p1">
<p class="ltx_p">Our experiments on MEBMs were conducted in the typical way <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#biba.bib15" title="Scalable connectivity for ising machines: dense to sparse">12</a>]</cite>. We employed the same Boltzmann machine architecture as we typically use for the DTM layers, specifically <math alttext="L=70" class="ltx_Math" display="inline" id="A11.p1.m1" intent=":literal"><semantics><mrow><mi>L</mi><mo>=</mo><mn>70</mn></mrow><annotation encoding="application/x-tex">L=70</annotation></semantics></math> with <math alttext="G_{12}" class="ltx_Math" display="inline" id="A11.p1.m2" intent=":literal"><semantics><msub><mi>G</mi><mn>12</mn></msub><annotation encoding="application/x-tex">G_{12}</annotation></semantics></math> connectivity. Random nodes were chosen to represent the data, and the rest were left as latent variables (as discussed in section <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A3" title="Appendix C A hardware architecture for denoising ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">C</span></a>).</p>
</div>
<div class="ltx_para" id="A11.p2">
<p class="ltx_p">Generating the data presented in Figs. 1 and 2 in the main text required controlling the mixing time of a trained Boltzmann machine. To achieve this, we added a fixed correlation penalty (Eq. 17 in the main text) and varied the strength to control the allowed complexity of the energy landscape.</p>
</div>
<div class="ltx_para" id="A11.p3">
<p class="ltx_p">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A11.F16" title="Figure 16 ‣ Appendix K MEBM experiments ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">16</span></a> (a) shows an example of the raw autocorrelation curves produced by sampling from Boltzmann machines trained with different correlation penalty strengths. The slowest exponential decay rate (<math alttext="\sigma_{2}" class="ltx_Math" display="inline" id="A11.p3.m1" intent=":literal"><semantics><msub><mi>σ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\sigma_{2}</annotation></semantics></math>) could be estimated for most of the curves by fitting a line to the natural log of the autocorrelation curve at long times, see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A11.F16" title="Figure 16 ‣ Appendix K MEBM experiments ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">16</span></a> (b) The two curves with the smallest correlation penalty did not reduce to simple exponential decay during the measured lag values, which means the decay rate was too long to be extracted from our data.</p>
</div>
<figure class="ltx_figure" id="A11.F16"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="348" id="A11.F16.g1" src="x16.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 16: </span><span class="ltx_text ltx_font_bold">Boltzmann machine autocorrelation curves (a)</span> The raw autocorrelation data associated with Boltzmann machines trained using different values of the parameter <math alttext="\lambda" class="ltx_Math" display="inline" id="A11.F16.m2" intent=":literal"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>. <span class="ltx_text ltx_font_bold">(b)</span> The log of the long-time autocorrelation for some of the curves shown in (a). All curves, except for the blue and orange ones, eventually became linear.</figcaption>
</figure>
<div class="ltx_para" id="A11.p4">
<p class="ltx_p">The exponential decay rates extracted from Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A11.F16" title="Figure 16 ‣ Appendix K MEBM experiments ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">16</span></a> were used as the mixing times in Fig. 2 in the article. Calling this a "mixing time" is a slight abuse of nomenclature. However, we did not think it made enough of a difference to the article’s message to disambiguate (since it is an upper bound on the mixing time, as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A8" title="Appendix H The autocorrelation function and mixing time ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">H</span></a>).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography ltx_list_bu2" id="biba">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist" id="biba.L1">
<li class="ltx_bibitem ltx_bib_article" id="biba.bib7">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Chadebec, L. Vincent, and S. Allassonniere</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Pythae: Unifying generative autoencoders in python-a benchmarking use case</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Adv. Neural Inf. Process. Syst.</span> <span class="ltx_text ltx_bib_volume">35</span>, <span class="ltx_text ltx_bib_pages"> pp. 21575–21589</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.48550/arXiv.2206.14813" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2206.14813" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A5.p2" title="Appendix E Energy analysis of GPUs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Appendix E</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="biba.bib8">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Dai and D. Wipf</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Diagnosing and Enhancing VAE Models</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Learning Representations</span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://openreview.net/forum?id=B1e0X3C9tQ" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A5.p2" title="Appendix E Energy analysis of GPUs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Appendix E</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="biba.bib11">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. He, X. Zhang, S. Ren, and J. Sun</span><span class="ltx_text ltx_bib_year"> (2016)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Deep residual learning for image recognition</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 770–778</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1109/CVPR.2016.90" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1109/CVPR.2016.90" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A5.p2" title="Appendix E Energy analysis of GPUs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Appendix E</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="biba.bib4">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Ho, A. Jain, and P. Abbeel</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Denoising diffusion probabilistic models</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Adv. Neural Inf. Process. Syst.</span> <span class="ltx_text ltx_bib_volume">33</span>, <span class="ltx_text ltx_bib_pages"> pp. 6840–6851</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.48550/arXiv.2006.11239" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2006.11239" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS2.SSS1.p3" title="A.2.1 Continuous variables ‣ A.2 Reverse Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§A.2.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A5.p3" title="Appendix E Energy analysis of GPUs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Appendix E</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="biba.bib16">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. P. Kingma and P. Dhariwal</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Glow: Generative Flow with Invertible 1x1 Convolutions</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,  <span class="ltx_text ltx_bib_editor">S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">31</span>, <span class="ltx_text ltx_bib_pages"> pp. </span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://proceedings.neurips.cc/paper_files/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A9.p2" title="Appendix I Deterministic embeddings for DTMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Appendix I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="biba.bib23">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D.A. Levin, Y. Peres, and E.L. Wilmer</span><span class="ltx_text ltx_bib_year"> (2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Markov Chains and Mixing Times</em></span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">American Mathematical Soc.</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 9780821886274</span>,
<a class="ltx_ref ltx_bib_external" href="https://books.google.ca/books?id=6Cg5Nq5sSv4C" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A8.p1" title="Appendix H The autocorrelation function and mixing time ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Appendix H</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="biba.bib3">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Lou, C. Meng, and S. Ermon</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution</em></span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2310.16834</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2310.16834" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS2.SSS2.p1" title="A.2.2 Discrete variables ‣ A.2 Reverse Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§A.2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="biba.bib6">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. P. Murphy</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Probabilistic Machine Learning: Advanced Topics</em></span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">MIT Press</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="http://probml.github.io/book2" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A2.p5" title="Appendix B Hardware accelerators for EBMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Appendix B</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="biba.bib10">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Ostheimer, M. Nagda, M. Kloft, and S. Fellenz</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Sparse Data Generation Using Diffusion Models</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2502.02448</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.48550/arXiv.2502.02448" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.48550/arXiv.2502.02448" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A5.p2" title="Appendix E Energy analysis of GPUs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Appendix E</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="biba.bib17">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Normalizing Flows for Probabilistic Modeling and Inference</em></span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">J. Mach. Learn. Res.</span> <span class="ltx_text ltx_bib_volume">22</span> (<span class="ltx_text ltx_bib_number">57</span>), <span class="ltx_text ltx_bib_pages"> pp. 1–64</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="http://jmlr.org/papers/v22/19-1028.html" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A9.p2" title="Appendix I Deterministic embeddings for DTMs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Appendix I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="biba.bib12">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">O. Ronneberger, P. Fischer, and T. Brox</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">U-net: Convolutional networks for biomedical image segmentation</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 234–241</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1007/978-3-319-24574-4%5F28" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1007/978-3-319-24574-4_28" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A5.p2" title="Appendix E Energy analysis of GPUs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Appendix E</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="biba.bib15">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. M. H. Sajeeb, N. A. Aadit, S. Chowdhury, T. Wu, C. Smith, D. Chinmay, A. Raut, K. Y. Camsari, C. Delacour, and T. Srimani</span><span class="ltx_text ltx_bib_year"> (2025-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Scalable connectivity for ising machines: dense to sparse</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Phys. Rev. Appl.</span> <span class="ltx_text ltx_bib_volume">24</span>, <span class="ltx_text ltx_bib_pages"> pp. 014005</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1103/kx8m-5h3h" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://link.aps.org/doi/10.1103/kx8m-5h3h" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A11.p1" title="Appendix K MEBM experiments ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Appendix K</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="biba.bib5">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli</span><span class="ltx_text ltx_bib_year"> (2015)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Deep unsupervised learning using nonequilibrium thermodynamics</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International conference on machine learning</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 2256–2265</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.5555/3045118.3045322" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.5555/3045118.3045322" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A1.SS2.SSS1.p3" title="A.2.1 Continuous variables ‣ A.2 Reverse Processes ‣ Appendix A Denoising Diffusion Models ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">§A.2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="biba.bib9">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. You, J. Chung, and M. Chowdhury</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title"><em class="ltx_emph ltx_font_italic">Zeus: Understanding and optimizing <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib9.m1a" intent=":literal"><semantics><mo stretchy="false">{</mo><annotation encoding="application/x-tex">\{</annotation></semantics></math>GPU<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib9.m2a" intent=":literal"><semantics><mo stretchy="false">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math> energy consumption of <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib9.m3a" intent=":literal"><semantics><mo stretchy="false">{</mo><annotation encoding="application/x-tex">\{</annotation></semantics></math>DNN<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib9.m4a" intent=":literal"><semantics><mo stretchy="false">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math> training</em></span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 119–139</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.5555/3600006.3600016" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.5555/3600006.3600016" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2510.23972v1#A5.p1" title="Appendix E Energy analysis of GPUs ‣ An efficient probabilistic hardware architecture for diffusion-like models"><span class="ltx_text ltx_ref_tag">Appendix E</span></a>.
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct 28 01:04:10 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
